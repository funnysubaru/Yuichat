---
name: 网页爬虫重构计划
overview: 按照参考文档中的实现方法，使用 Selenium + unstructured 库完全重新实现网页爬虫功能，替换现有的 Playwright + BeautifulSoup 实现
todos:
  - id: update-dependencies
    content: 更新 requirements.txt，添加 selenium 和 unstructured 依赖
    status: completed
  - id: implement-helper-functions
    content: 实现辅助函数：is_valid_url, is_url_accessible, replace_special_char
    status: completed
    dependencies:
      - update-dependencies
  - id: implement-selenium-loader
    content: 实现 SeleniumChromeURLLoader 类，包括 _get_driver 和 load 方法
    status: completed
    dependencies:
      - implement-helper-functions
  - id: implement-loader-factory
    content: 实现 loader_factory 函数，支持 selenium/unstructured/playwright 三种加载器
    status: completed
    dependencies:
      - implement-selenium-loader
  - id: implement-process-web-content
    content: 实现 process_web_content 函数，使用 loader_factory 处理网页内容
    status: completed
    dependencies:
      - implement-loader-factory
  - id: update-async-interface
    content: 更新 crawl_urls 异步函数，保持与 workflow.py 的兼容性
    status: completed
    dependencies:
      - implement-process-web-content
  - id: update-version
    content: 更新 VERSION 和 CHANGELOG.md 记录版本 1.1.12
    status: completed
    dependencies:
      - update-async-interface
---

# 网页爬虫重构计划

## 概述

根据 `docs/网页爬虫代码分析.md` 文档，完全按照参考代码的实现方法重新实现网页爬虫功能。核心变更：从 Playwright + BeautifulSoup 切换到 Selenium + unstructured。

## 技术栈变更

- **当前实现**: Playwright + BeautifulSoup + lxml
- **目标实现**: Selenium + Chrome + unstructured.partition.html

## 实现步骤

### 1. 更新依赖包

更新 `backend_py/requirements.txt`：

- 添加 `selenium` 和 `webdriver-manager`（或直接使用 ChromeDriver）
- 添加 `unstructured[html] `或 `unstructured`
- 保留 `playwright` 和 `beautifulsoup4` 作为可选加载器（通过 loader_factory 支持）
- 添加 `requests`（如果尚未包含）

### 2. 重写 crawler.py

完全按照参考代码 `sample/chatmax-后端(1)/AutoLM/app/url/url.py` 的结构重写 `backend_py/crawler.py`：

#### 2.1 实现 SeleniumChromeURLLoader 类

- 继承 `BaseLoader`（langchain）
- 支持参数：`urls`, `continue_on_failure`, `headless`, `imageless`, `infobarless`, `executable_path`
- `_get_driver()` 方法：创建 Chrome WebDriver，配置选项（headless, no-sandbox, disable-images等）
- `load()` 方法：
  - 遍历 URL 列表
  - 使用 `driver.get(url)` 访问页面
  - **固定等待 3 秒**：`time.sleep(3)`
  - 获取 `driver.title` 和 `driver.page_source`
  - 使用 `unstructured.partition.html.partition_html()` 解析 HTML
  - 使用 `replace_special_char()` 清理每个元素
  - 创建 `Document` 对象（metadata 包含 source 和 title）
  - 错误处理：支持 `continue_on_failure`
  - 最后调用 `driver.quit()` 关闭浏览器

#### 2.2 实现辅助函数

- `is_valid_url(s: str)`: 检查 URL 是否非空且非空白
- `is_url_accessible(url)`: 
  - 使用 `requests.get(url, timeout=5)` 检查可访问性
  - 状态码 200 返回 True
  - 其他状态码也返回 True（记录日志）
  - 异常返回 False
- `replace_special_char(text)`: 
  - 使用正则表达式：`re.sub(r"([ ■◼•＊…— ●⚫]+|[·\.~•、—'}]{3,})", ' ', text)`
  - 清理特殊字符和重复标点

#### 2.3 实现 loader_factory 函数

支持三种加载器类型：

- `"selenium"`: 返回 `SeleniumChromeURLLoader(urls=urls)`
- `"unstructured"`: 返回 `UnstructuredURLLoader(urls=urls)`（langchain）
- `"playwright"`: 返回 `PlaywrightURLLoader(urls=urls, remove_selectors=["header", "footer"], continue_on_failure=False, headless=True)`

#### 2.4 实现 process_web_content 函数

- 接收 `decoded_url` 和 `omit_content` 参数
- 创建 URL 列表 `[decoded_url]`
- 使用 `loader_factory(urls, "selenium")` 创建加载器
- 调用 `loader.load()` 获取文档列表
- 提取第一个文档的内容、标题和字数统计
- 如果 `omit_content=True`，清空内容
- 再次调用 `replace_special_char()` 清理内容
- 返回 `(content, title, word_count)`

#### 2.5 保持异步接口兼容性

- 保留 `crawl_urls()` 异步函数，但内部调用同步的 Selenium 加载器
- 在异步函数中使用 `asyncio.to_thread()` 或线程池执行同步代码
- 保持与 `workflow.py` 的兼容性

### 3. 更新 workflow.py

确保 `crawl_url_node` 函数能够正确调用新的爬虫实现：

- 保持现有的异步调用方式
- 确保错误处理逻辑兼容

### 4. 版本记录

在 `CHANGELOG.md` 和 `VERSION` 文件中记录版本更新（1.1.12）：

- 重构网页爬虫：从 Playwright 切换到 Selenium + unstructured
- 实现与参考代码完全一致的爬虫逻辑
- 支持多种加载器（selenium, unstructured, playwright）

## 关键实现细节

### Chrome WebDriver 配置

```python
chrome_options = ChromeOptions()
if self.headless:
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
if self.imageless:
    chrome_options.add_argument("--blink-settings=imagesEnabled=false")
if self.infobarless:
    chrome_options.add_argument("--disable-infobars")
```

### 内容提取流程

1. `driver.get(url)` - 访问页面
2. `time.sleep(3)` - 固定等待 3 秒
3. `driver.title` - 获取标题
4. `driver.page_source` - 获取 HTML 源码
5. `partition_html(text=page_content)` - 解析 HTML
6. `replace_special_char(str(el).strip())` - 清理每个元素
7. `"\n\n".join(...)` - 合并文本

### 错误处理

- 单个 URL 失败时，如果 `continue_on_failure=True`，记录错误但继续处理其他 URL
- 如果 `continue_on_failure=False`，抛出异常

## 注意事项

1. **ChromeDriver 依赖**: 需要确保系统已安装 Chrome 浏览器和 ChromeDriver，或使用 `webdriver-manager` 自动管理
2. **同步 vs 异步**: Selenium 是同步库，需要在异步环境中使用线程池或 `asyncio.to_thread()` 执行
3. **资源清理**: 确保在 finally 块中调用 `driver.quit()` 关闭浏览器
4. **日志输出**: 仅在开发环境（`ENV=development`）输出日志
5. **向后兼容**: 保持现有的 API 接口不变，确保 `workflow.py` 和 `app.py` 无需修改

## 文件清单

- `backend_py/requirements.txt` - 更新依赖
- `backend_py/crawler.py` - 完全重写
- `CHANGELOG.md` - 添加版本记录
- `VERSION` - 更新版本号到 1.1.12