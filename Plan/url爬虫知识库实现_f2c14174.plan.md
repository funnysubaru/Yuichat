---
name: URL爬虫知识库实现
overview: 实现基于Playwright和Beautiful Soup的URL爬虫功能，通过LangGraph节点处理网页内容并生成知识库文档。支持JavaScript动态内容、批量URL处理，并集成到现有的文档处理流程中。
todos:
  - id: update-dependencies
    content: 更新requirements.txt，添加playwright、beautifulsoup4、lxml依赖
    status: completed
  - id: create-crawler-module
    content: 创建backend_py/crawler.py，实现Playwright爬虫和Beautiful Soup解析功能
    status: completed
    dependencies:
      - update-dependencies
  - id: update-workflow
    content: 在workflow.py中添加crawl_url_node节点，更新GraphState和路由逻辑
    status: completed
    dependencies:
      - create-crawler-module
  - id: add-api-endpoint
    content: 在app.py中添加/api/process-url端点，处理URL爬取请求
    status: completed
    dependencies:
      - update-workflow
  - id: update-frontend-service
    content: 在kbService.ts中添加uploadUrlsToKB函数，调用后端API
    status: completed
    dependencies:
      - add-api-endpoint
  - id: implement-frontend-logic
    content: 在KnowledgeBasePage.tsx中实现网站处理逻辑（替代TODO）
    status: completed
    dependencies:
      - update-frontend-service
  - id: update-env-config
    content: 更新env.example，添加爬虫相关配置项（超时、并发数等）
    status: completed
  - id: update-version
    content: 更新VERSION文件为1.1.11，并在CHANGELOG.md中添加版本记录
    status: completed
    dependencies:
      - implement-frontend-logic
---

# URL爬虫知识库实现计划

## 概述

实现通过URL爬取网页内容并生成知识库文档的功能。使用Playwright处理JavaScript动态内容，Beautiful Soup清洗HTML，通过LangGraph节点集成到现有工作流。

## 技术架构

```
前端 (React) → API调用 → Python后端 (FastAPI) → LangGraph工作流 → 向量存储
                                      ↓
                              Playwright爬虫 + Beautiful Soup解析
```

## 实现步骤

### 1. Python后端依赖更新

**文件**: `backend_py/requirements.txt`

添加依赖：

- `playwright` - 无头浏览器，处理JavaScript动态内容
- `beautifulsoup4` - HTML解析和清洗
- `lxml` - Beautiful Soup的XML/HTML解析器（可选，性能更好）

**安装说明**：

```bash
playwright install chromium  # 安装Chromium浏览器
```

### 2. 创建爬虫服务模块

**文件**: `backend_py/crawler.py` (新建)

实现核心爬虫功能：

- `crawl_url(url: str, options: dict) -> str`: 使用Playwright爬取单个URL
  - 支持JavaScript渲染等待
  - 处理Cookie和认证
  - 超时控制
  - 错误处理
- `parse_html(html: str, url: str) -> Document`: 使用Beautiful Soup解析HTML
  - 移除script、style标签
  - 提取正文内容
  - 保留标题、段落结构
  - 转换为LangChain Document格式

**关键实现点**：

- Playwright等待页面加载完成（`page.wait_for_load_state("networkidle")`）
- Beautiful Soup提取`<main>`, `<article>`, `<body>`主要内容区域
- 处理编码问题（UTF-8优先）

### 3. 更新LangGraph工作流

**文件**: `backend_py/workflow.py`

#### 3.1 修改GraphState

添加新字段支持URL输入：

- `urls: List[str]` - URL列表（可选）
- `url_content: str` - 爬取的网页内容（可选）

#### 3.2 创建爬虫节点

**新节点**: `crawl_url_node(state: GraphState)`

功能：

- 从`urls`获取URL列表
- 批量调用`crawler.crawl_url()`爬取所有URL
- 使用`crawler.parse_html()`解析HTML
- 将解析结果合并为Document列表
- 设置文档元数据（来源URL、标题、爬取时间等）

#### 3.3 更新路由逻辑

修改`should_process_file()`函数，支持三种输入模式：

- `file_path` → 文件处理流程
- `urls` → URL爬虫流程
- 仅`messages` → 直接聊天流程

#### 3.4 更新工作流边

```
entry → [条件路由]
  ├─ 有file_path → process_file_node → split_text_node
  ├─ 有urls → crawl_url_node → split_text_node
  └─ 仅messages → chat_node
```

### 4. 创建API端点

**文件**: `backend_py/app.py`

**新端点**: `POST /api/process-url`

请求格式：

```json
{
  "urls": ["https://example.com/page1", "https://example.com/page2"],
  "collection_name": "kb_xxx"
}
```

功能：

- 验证URL格式
- 调用LangGraph工作流处理URL
- 返回处理状态和结果

**错误处理**：

- URL格式错误（400）
- 爬取超时（504）
- 网页无法访问（502）
- 处理失败（500）

### 5. 前端服务层更新

**文件**: `src/services/kbService.ts`

**新函数**: `uploadUrlsToKB(kbId: string, urls: string[])`

功能：

- 获取知识库的`vector_collection`
- 调用`/api/process-url`端点
- 处理错误和成功状态
- 返回处理结果

### 6. 前端页面更新

**文件**: `src/pages/KnowledgeBasePage.tsx`

**更新**: `handleUpload()`函数中的网站处理逻辑（当前是TODO）

实现：

- 接收URL数组（单个或多个）
- 为每个URL创建文档记录（状态：processing）
- 调用`uploadUrlsToKB()`
- 更新文档状态（completed/failed）
- 刷新文档列表

**文档记录字段**：

- `filename`: URL（或提取的页面标题）
- `file_type`: "url"
- `storage_path`: 原始URL
- `status`: "processing" → "completed"/"failed"

### 7. 错误处理和日志

**通用改进**：

- 添加爬虫相关的日志记录
- 记录爬取时间、页面大小、错误类型
- 仅在开发环境输出详细日志（遵循1.1.3规范）

**错误分类**：

- 网络错误：连接超时、DNS解析失败
- 内容错误：页面为空、解析失败
- 认证错误：需要登录、需要验证码（记录但不阻断）

### 8. 性能优化

**批量处理策略**：

- 多个URL并行爬取（使用`asyncio.gather()`）
- 控制并发数量（避免过多请求）
- 失败重试机制（最多3次）

**缓存策略**（可选，未来优化）：

- 根据URL hash缓存已爬取内容
- 避免重复爬取相同URL

## 文件清单

### 新建文件

1. `backend_py/crawler.py` - 爬虫核心模块

### 修改文件

1. `backend_py/requirements.txt` - 添加依赖
2. `backend_py/workflow.py` - 添加爬虫节点，更新工作流
3. `backend_py/app.py` - 添加URL处理API端点
4. `src/services/kbService.ts` - 添加URL上传服务
5. `src/pages/KnowledgeBasePage.tsx` - 实现网站处理逻辑

### 配置文件

1. `backend_py/env.example` - 添加爬虫相关配置（超时时间、并发数等）

## 测试要点

1. **单URL爬取**: 测试基本功能
2. **多URL批量**: 测试并发处理
3. **JavaScript网站**: 测试动态内容渲染
4. **错误处理**: 测试无效URL、超时、无法访问
5. **文档生成**: 验证内容正确提取并生成向量

## 注意事项

1. **Playwright安装**: 需要单独安装浏览器二进制文件
2. **资源消耗**: 无头浏览器占用内存较大，需要监控
3. **反爬虫**: 某些网站可能有反爬措施，需要设置合理的请求头和延迟
4. **合规性**: 确保遵守目标网站的robots.txt和使用条款

## 版本记录

- **1.1.11**: 添加URL爬虫功能，支持Playwright和Beautiful Soup