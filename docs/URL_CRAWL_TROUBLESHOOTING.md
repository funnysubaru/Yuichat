# URLçˆ¬è™«é—®é¢˜è¯Šæ–­æŒ‡å—

## é—®é¢˜ç°è±¡

URLçˆ¬å–åçŠ¶æ€æ˜¾ç¤º"å­¦ä¹ æˆåŠŸ"ï¼Œä½†åœ¨æµ‹è¯•å¯¹è¯ä¸­æ— æ³•å›ç­”ç›¸å…³é—®é¢˜ï¼ŒAIå›ç­”"æä¾›çš„ä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯"ã€‚

## å¯èƒ½åŸå› åˆ†æ

### 1. å‘é‡å­˜å‚¨å¤±è´¥ï¼ˆæœ€å¯èƒ½ï¼‰

**ç—‡çŠ¶**ï¼š
- URLçˆ¬å–å®Œæˆï¼ŒçŠ¶æ€æ˜¾ç¤ºæˆåŠŸ
- ä½†å‘é‡åº“ä¸­æ²¡æœ‰æ•°æ®

**è¯Šæ–­æ–¹æ³•**ï¼š
```bash
cd backend_py
python test_url_crawl_diagnosis.py <collection_name>
```

**æ£€æŸ¥é¡¹**ï¼š
- å‘é‡é›†åˆæ˜¯å¦å­˜åœ¨
- æ–‡æ¡£æ•°é‡æ˜¯å¦ä¸º0
- æ˜¯å¦æœ‰é”™è¯¯æ–‡æ¡£

**è§£å†³æ–¹æ¡ˆ**ï¼š
- æ£€æŸ¥çˆ¬å–æ—¥å¿—ï¼Œç¡®è®¤æ–‡æ¡£æ˜¯å¦æ­£ç¡®ç”Ÿæˆ
- æ£€æŸ¥embeddingç”Ÿæˆæ˜¯å¦å¤±è´¥
- æ£€æŸ¥å‘é‡å­˜å‚¨æ˜¯å¦æœ‰é”™è¯¯

### 2. collection_name ä¸åŒ¹é…

**ç—‡çŠ¶**ï¼š
- çˆ¬å–æ—¶ä½¿ç”¨çš„collection_nameä¸æ£€ç´¢æ—¶ä½¿ç”¨çš„ä¸åŒ

**è¯Šæ–­æ–¹æ³•**ï¼š
1. æŸ¥çœ‹çˆ¬å–æ—¥å¿—ä¸­çš„collection_name
2. æŸ¥çœ‹æµ‹è¯•å¯¹è¯æ—¶ä½¿ç”¨çš„collection_name
3. å¯¹æ¯”ä¸¤è€…æ˜¯å¦ä¸€è‡´

**æ£€æŸ¥ç‚¹**ï¼š
- `/api/process-url` ä½¿ç”¨çš„collection_name
- `/api/chat` æŸ¥è¯¢åˆ°çš„collection_name
- ä¸¤è€…åº”è¯¥éƒ½æ˜¯é¡¹ç›®çš„ `vector_collection`

### 3. çˆ¬å–å†…å®¹ä¸ºç©ºæˆ–è§£æå¤±è´¥

**ç—‡çŠ¶**ï¼š
- çˆ¬å–æˆåŠŸä½†å†…å®¹ä¸ºç©º
- è§£æå¤±è´¥ä½†è¢«å½“ä½œæˆåŠŸå¤„ç†

**è¯Šæ–­æ–¹æ³•**ï¼š
```python
# åœ¨crawler.pyä¸­æ·»åŠ è°ƒè¯•è¾“å‡º
# æ£€æŸ¥è¿”å›çš„Documentçš„page_contentæ˜¯å¦ä¸ºç©º
```

**æ£€æŸ¥ç‚¹**ï¼š
- Document.page_content é•¿åº¦
- æ˜¯å¦æœ‰"è§£æå¤±è´¥"æˆ–"çˆ¬å–å¤±è´¥"çš„é”™è¯¯æ–‡æœ¬
- å…ƒæ•°æ®ä¸­çš„errorå­—æ®µ

### 4. æ£€ç´¢é€»è¾‘é—®é¢˜

**ç—‡çŠ¶**ï¼š
- æ•°æ®å·²å­˜å‚¨ä½†æ£€ç´¢ä¸åˆ°

**è¯Šæ–­æ–¹æ³•**ï¼š
- æ£€æŸ¥æ£€ç´¢æ—¶ä½¿ç”¨çš„collection_name
- æ£€æŸ¥æ£€ç´¢å‚æ•°ï¼ˆkå€¼ã€ç›¸ä¼¼åº¦é˜ˆå€¼ç­‰ï¼‰
- æŸ¥çœ‹æ£€ç´¢æ—¥å¿—è¾“å‡º

## è¯Šæ–­æ­¥éª¤

### æ­¥éª¤1: è¿è¡Œè¯Šæ–­è„šæœ¬

```bash
cd backend_py

# è·å–é¡¹ç›®çš„vector_collection
# å¯ä»¥åœ¨Supabaseæ•°æ®åº“ä¸­æŸ¥è¯¢ï¼š
# SELECT id, name, vector_collection FROM knowledge_bases;

# è¿è¡Œè¯Šæ–­è„šæœ¬
python test_url_crawl_diagnosis.py <vector_collection>
```

### æ­¥éª¤2: æ£€æŸ¥åç«¯æ—¥å¿—

å¯åŠ¨åç«¯æ—¶ï¼Œç¡®ä¿ `ENV=development`ï¼ŒæŸ¥çœ‹è¯¦ç»†æ—¥å¿—ï¼š

```bash
cd backend_py
export ENV=development
chainlit run app.py
```

æŸ¥çœ‹ä»¥ä¸‹å…³é”®æ—¥å¿—ï¼š
- `ğŸ•·ï¸ Crawling X URL(s)...` - çˆ¬å–å¼€å§‹
- `âœ… Successfully crawled X URL(s)` - çˆ¬å–å®Œæˆ
- `Split into X chunks` - æ–‡æœ¬åˆ‡ç‰‡
- `âœ… Stored X vectors in Chroma: <collection_name>` - å‘é‡å­˜å‚¨
- `ğŸ” æ£€ç´¢åˆ° X ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ` - æ£€ç´¢ç»“æœ

### æ­¥éª¤3: æ£€æŸ¥å‘é‡åº“

```python
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

collection_name = "your_collection_name"
vectorstore = Chroma(
    persist_directory=f"./chroma_db/{collection_name}",
    embedding_function=OpenAIEmbeddings()
)

# è·å–æ‰€æœ‰æ–‡æ¡£
collection = vectorstore._collection
all_docs = collection.get()

print(f"æ–‡æ¡£æ•°é‡: {len(all_docs.get('ids', []))}")
print(f"å‘é‡æ•°é‡: {len(all_docs.get('embeddings', []))}")

# æ£€æŸ¥URLæ¥æºçš„æ–‡æ¡£
url_docs = [
    (metadata.get('source'), text[:100])
    for metadata, text in zip(
        all_docs.get('metadatas', []),
        all_docs.get('documents', [])
    )
    if metadata.get('source', '').startswith(('http://', 'https://'))
]
print(f"URLæ¥æºæ–‡æ¡£: {len(url_docs)}")
for source, text in url_docs:
    print(f"  {source}: {text}...")
```

### æ­¥éª¤4: æµ‹è¯•æ£€ç´¢

```python
# æµ‹è¯•æ£€ç´¢
retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
query = "markdown"
docs = retriever.invoke(query)

print(f"æŸ¥è¯¢: {query}")
print(f"æ‰¾åˆ° {len(docs)} ä¸ªç›¸å…³æ–‡æ¡£:")
for i, doc in enumerate(docs):
    print(f"[{i+1}] æ¥æº: {doc.metadata.get('source')}")
    print(f"    å†…å®¹: {doc.page_content[:200]}...")
```

## å¸¸è§é—®é¢˜ä¿®å¤

### é—®é¢˜1: å‘é‡é›†åˆä¸ºç©º

**åŸå› **ï¼šçˆ¬å–å¤±è´¥æˆ–å­˜å‚¨å¤±è´¥

**ä¿®å¤**ï¼š
1. æ£€æŸ¥çˆ¬å–æ—¥å¿—ï¼Œç¡®è®¤æ˜¯å¦æˆåŠŸ
2. æ£€æŸ¥OpenAI APIå¯†é’¥æ˜¯å¦æ­£ç¡®
3. æ£€æŸ¥æ˜¯å¦æœ‰embeddingç”Ÿæˆçš„é”™è¯¯
4. é‡æ–°çˆ¬å–URL

### é—®é¢˜2: collection_nameä¸åŒ¹é…

**åŸå› **ï¼šURLçˆ¬å–å’Œæµ‹è¯•å¯¹è¯ä½¿ç”¨äº†ä¸åŒçš„collection

**ä¿®å¤**ï¼š
1. ç¡®è®¤URLçˆ¬å–æ—¶ä½¿ç”¨çš„collection_name
2. ç¡®è®¤æµ‹è¯•å¯¹è¯æ—¶æŸ¥è¯¢åˆ°çš„collection_name
3. ç¡®ä¿ä¸¤è€…ä¸€è‡´

### é—®é¢˜3: æ£€ç´¢ä¸åˆ°æ•°æ®

**åŸå› **ï¼š
- æŸ¥è¯¢è¯­ä¹‰ä¸å­˜å‚¨å†…å®¹ä¸åŒ¹é…
- ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡é«˜
- kå€¼è®¾ç½®è¿‡å°

**ä¿®å¤**ï¼š
1. å°è¯•ä¸åŒçš„æŸ¥è¯¢è¯
2. é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆå¦‚æœæœ‰ï¼‰
3. å¢åŠ kå€¼ï¼ˆæ£€ç´¢æ•°é‡ï¼‰

### é—®é¢˜4: å†…å®¹è§£æå¤±è´¥

**åŸå› **ï¼š
- ç½‘é¡µéœ€è¦ç™»å½•
- ç½‘é¡µæœ‰åçˆ¬æªæ–½
- JavaScriptæ¸²æŸ“å¤±è´¥

**ä¿®å¤**ï¼š
1. æ£€æŸ¥çˆ¬å–æ—¥å¿—ä¸­çš„é”™è¯¯ä¿¡æ¯
2. å°è¯•æ‰‹åŠ¨è®¿é—®URLç¡®è®¤å¯è®¿é—®æ€§
3. å¢åŠ ç­‰å¾…æ—¶é—´ï¼ˆWAIT_NETWORK_IDLEï¼‰
4. æ·»åŠ Cookieæ”¯æŒï¼ˆå¦‚æœéœ€è¦ï¼‰

## è°ƒè¯•æŠ€å·§

### 1. å¯ç”¨è¯¦ç»†æ—¥å¿—

```bash
export ENV=development
```

### 2. æ·»åŠ è°ƒè¯•è¾“å‡º

åœ¨å…³é”®ä½ç½®æ·»åŠ printè¯­å¥ï¼š
- `crawler.py` çš„ `parse_html` å‡½æ•°
- `workflow.py` çš„å„ä¸ªèŠ‚ç‚¹
- `app.py` çš„APIç«¯ç‚¹

### 3. æ‰‹åŠ¨æµ‹è¯•çˆ¬å–

```python
from crawler import crawl_urls

urls = ["https://markdown.lovejade.cn/"]
docs = await crawl_urls(urls)

for doc in docs:
    print(f"æ¥æº: {doc.metadata.get('source')}")
    print(f"å†…å®¹é•¿åº¦: {len(doc.page_content)}")
    print(f"å†…å®¹é¢„è§ˆ: {doc.page_content[:500]}")
```

## è”ç³»æ”¯æŒ

å¦‚æœä»¥ä¸Šæ–¹æ³•éƒ½æ— æ³•è§£å†³é—®é¢˜ï¼Œè¯·æä¾›ï¼š
1. è¯Šæ–­è„šæœ¬çš„è¾“å‡º
2. åç«¯å®Œæ•´æ—¥å¿—
3. URLåœ°å€
4. collection_name
5. é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
