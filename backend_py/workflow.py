import os
import asyncio
import re  # 1.1.13: å¯¼å…¥ re ç”¨äº collection_name éªŒè¯
import tempfile  # 1.2.43: ä¸´æ—¶æ–‡ä»¶å¤„ç†
import urllib.parse  # 1.2.43: URL è§£æ
import requests  # 1.2.43: HTTP è¯·æ±‚ä¸‹è½½æ–‡ä»¶
from typing import List, Dict, Any, TypedDict
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# 1.2.42: æ—§ç‰ˆå¯¼å…¥ï¼ˆæ³¨é‡Šä¿ç•™ï¼‰
# from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, UnstructuredExcelLoader
# 1.2.42: æ–°ç‰ˆå¯¼å…¥ - æ”¯æŒæ›´å¤šæ–‡ä»¶æ ¼å¼
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, UnstructuredExcelLoader
from pptx_loader import GeneralPPTXLoader  # 1.2.42: PPT/PPTX åŠ è½½å™¨
from txt_loader import TxtLoader  # 1.2.42: TXT æ–‡æœ¬åŠ è½½å™¨
from langchain_text_splitters import RecursiveCharacterTextSplitter
# 1.2.56: Chroma æ”¹ä¸ºå»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…åœ¨ä½¿ç”¨ pgvector æ—¶ä»éœ€å®‰è£… chromadb
# from langchain_community.vectorstores import Chroma
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.documents import Document  # 1.1.12: å¯¼å…¥ Document ç”¨äºé”™è¯¯å¤„ç†
from langgraph.graph import StateGraph, END
from dotenv import load_dotenv
# 1.1.11: å¯¼å…¥çˆ¬è™«æ¨¡å—
from crawler import crawl_urls

# 1.2.39: ä¼˜å…ˆåŠ è½½ .env.localï¼Œç„¶ååŠ è½½ .envï¼ˆå¦‚æœå­˜åœ¨ï¼‰
load_dotenv('.env.local')  # æœ¬åœ°å¼€å‘é…ç½®ä¼˜å…ˆ
load_dotenv()  # å›é€€åˆ° .env

# 1.1.3: ç¯å¢ƒé…ç½® - æ”¯æŒæœ¬åœ°/çº¿ä¸Šæ•°æ®åº“åˆ‡æ¢
USE_PGVECTOR = os.getenv("USE_PGVECTOR", "false").lower() == "true"
# 1.2.56: è°ƒè¯•è¾“å‡º - ç¡®è®¤é…ç½®æ˜¯å¦æ­£ç¡®åŠ è½½
print(f"ğŸ”§ USE_PGVECTOR ç¯å¢ƒå˜é‡: {os.getenv('USE_PGVECTOR')} -> {USE_PGVECTOR}")
# 1.1.17: é‡å‘½åä¸º PGVECTOR_DATABASE_URL é¿å…ä¸ Chainlit æ•°æ®æŒä¹…åŒ–å†²çª
DATABASE_URL = os.getenv("PGVECTOR_DATABASE_URL") or os.getenv("DATABASE_URL")

# 1.2.12: æ–‡æ¡£ç‰‡æ®µæ•°é‡é…ç½® - æ”¯æŒå¯é…ç½®çš„æ–‡æ¡£ç‰‡æ®µæ•°é‡é™åˆ¶
# MAX_CHUNKS: æœ€ç»ˆä½¿ç”¨çš„æ–‡æ¡£ç‰‡æ®µæ•°é‡ï¼ˆé»˜è®¤4ï¼‰
MAX_CHUNKS = int(os.getenv("MAX_CHUNKS", "4"))
# RETRIEVE_K: æ£€ç´¢æ—¶è·å–çš„æ–‡æ¡£æ•°é‡ï¼Œåº”è¯¥æ¯” MAX_CHUNKS å¤§ä»¥ä¾¿è¿‡æ»¤é”™è¯¯æ–‡æ¡£ï¼ˆé»˜è®¤8ï¼‰
RETRIEVE_K = int(os.getenv("RETRIEVE_K", "8"))

# 1.1.3: å¦‚æœå¯ç”¨ pgvectorï¼Œå¯¼å…¥ vecs åº“
# 1.2.56: Chroma æ”¹ä¸ºæ¡ä»¶å¯¼å…¥ï¼Œé¿å…åœ¨ä½¿ç”¨ pgvector æ—¶ä»éœ€å®‰è£… chromadb
Chroma = None  # å»¶è¿Ÿå¯¼å…¥å ä½ç¬¦
if USE_PGVECTOR:
    try:
        import vecs
        from vecs import Collection
        print("âœ… ä½¿ç”¨ Supabase pgvector ä½œä¸ºå‘é‡æ•°æ®åº“")
    except ImportError:
        print("âš ï¸ vecs åº“æœªå®‰è£…ï¼Œå›é€€åˆ° Chroma")
        USE_PGVECTOR = False
        from langchain_community.vectorstores import Chroma
else:
    from langchain_community.vectorstores import Chroma
    print("âœ… ä½¿ç”¨ Chroma ä½œä¸ºæœ¬åœ°å‘é‡æ•°æ®åº“")

# å®šä¹‰çŠ¶æ€
# 1.1.11: æ·»åŠ URLçˆ¬è™«ç›¸å…³å­—æ®µ
# 1.2.52: æ·»åŠ  language å­—æ®µï¼Œæ”¯æŒå¤šè¯­è¨€å›å¤
# 1.3.11: æ·»åŠ  citations å­—æ®µï¼Œæ”¯æŒå¼•ç”¨æ¥æºå±•ç¤º
class GraphState(TypedDict):
    file_path: str
    urls: List[str]  # 1.1.11: URLåˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
    docs: List[Any]
    splits: List[Any]
    collection_name: str
    messages: List[BaseMessage]
    context: str
    answer: str
    language: str  # 1.2.52: è¯­è¨€è®¾ç½®ï¼ˆzh/en/jaï¼‰
    citations: List[Dict[str, Any]]  # 1.3.11: å¼•ç”¨æ¥æºåˆ—è¡¨

# 1.2.43: ä» URL ä¸‹è½½æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
def download_file_from_url(url: str) -> str:
    """
    1.2.43: ä» URL ä¸‹è½½æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•

    Args:
        url: æ–‡ä»¶çš„ URL åœ°å€

    Returns:
        str: ä¸‹è½½åçš„æœ¬åœ°æ–‡ä»¶è·¯å¾„
    """
    if os.getenv("ENV") == "development":
        print(f"ğŸ“¥ å¼€å§‹ä¸‹è½½æ–‡ä»¶: {url}")

    # è§£æ URL è·å–æ–‡ä»¶å
    parsed_url = urllib.parse.urlparse(url)
    path_parts = parsed_url.path.split('/')
    # è·å–åŸå§‹æ–‡ä»¶åï¼ˆæœ€åä¸€éƒ¨åˆ†ï¼‰
    original_filename = path_parts[-1] if path_parts[-1] else 'downloaded_file'
    # URL è§£ç æ–‡ä»¶å
    original_filename = urllib.parse.unquote(original_filename)

    # è·å–æ–‡ä»¶æ‰©å±•å
    file_ext = original_filename.split('.')[-1].lower() if '.' in original_filename else ''

    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    temp_dir = tempfile.mkdtemp(prefix='yuichat_')
    local_path = os.path.join(temp_dir, original_filename)

    try:
        # ä¸‹è½½æ–‡ä»¶
        response = requests.get(url, timeout=60, stream=True)
        response.raise_for_status()

        # å†™å…¥æœ¬åœ°æ–‡ä»¶
        with open(local_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)

        if os.getenv("ENV") == "development":
            file_size = os.path.getsize(local_path)
            print(f"âœ… æ–‡ä»¶ä¸‹è½½å®Œæˆ: {local_path} ({file_size} å­—èŠ‚)")

        return local_path

    except Exception as e:
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        import shutil
        shutil.rmtree(temp_dir, ignore_errors=True)
        raise ValueError(f"æ–‡ä»¶ä¸‹è½½å¤±è´¥: {str(e)}")


# 1.1.0: æ–‡ä»¶å¤„ç†èŠ‚ç‚¹
# 1.2.42: æ‰©å±•æ”¯æŒ PPTX å’Œ TXT æ–‡ä»¶æ ¼å¼
# 1.2.43: æ”¯æŒä» URL ä¸‹è½½æ–‡ä»¶
def process_file_node(state: GraphState):
    file_path = state.get('file_path')
    if not file_path:
        print("No file path provided, skipping file processing.")
        return {"docs": state.get('docs', [])}

    print(f"Processing file: {file_path}")

    # 1.2.43: æ£€æŸ¥æ˜¯å¦æ˜¯ URLï¼Œå¦‚æœæ˜¯åˆ™å…ˆä¸‹è½½åˆ°æœ¬åœ°
    local_file_path = file_path
    temp_dir_to_cleanup = None

    if file_path.startswith(('http://', 'https://')):
        if os.getenv("ENV") == "development":
            print(f"ğŸŒ æ£€æµ‹åˆ° URLï¼Œå¼€å§‹ä¸‹è½½æ–‡ä»¶...")
        local_file_path = download_file_from_url(file_path)
        # è®°å½•ä¸´æ—¶ç›®å½•ä»¥ä¾¿åç»­æ¸…ç†
        temp_dir_to_cleanup = os.path.dirname(local_file_path)

    try:
        docs = []
        # 1.2.42: è·å–æ–‡ä»¶æ‰©å±•åï¼ˆå°å†™ï¼‰
        file_ext = local_file_path.lower().split('.')[-1] if '.' in local_file_path else ''

        # 1.2.42: æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åŠ è½½å™¨
        if file_ext == 'pdf':
            loader = PyPDFLoader(local_file_path)
        elif file_ext in ['docx', 'doc']:
            loader = Docx2txtLoader(local_file_path)
        elif file_ext in ['xlsx', 'xls']:
            # 1.1.0: Excel å»ºè®®è½¬æ¢ä¸º CSV æˆ–ä½¿ç”¨ä¸“é—¨å¤„ç†ï¼Œè¿™é‡Œæš‚æ—¶ä½¿ç”¨é€šç”¨åŠ è½½å™¨
            loader = UnstructuredExcelLoader(local_file_path)
        elif file_ext in ['pptx', 'ppt']:
            # 1.2.42: PPT/PPTX æ–‡ä»¶ - ä½¿ç”¨è‡ªå®šä¹‰åŠ è½½å™¨
            # æ³¨æ„ï¼š.ppt æ ¼å¼éœ€è¦å…ˆè½¬æ¢ä¸º .pptxï¼ˆpython-pptx åªæ”¯æŒ .pptxï¼‰
            if file_ext == 'ppt':
                if os.getenv("ENV") == "development":
                    print("âš ï¸ .ppt æ ¼å¼ä¸å—æ”¯æŒï¼Œè¯·è½¬æ¢ä¸º .pptx æ ¼å¼")
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: .pptï¼Œè¯·è½¬æ¢ä¸º .pptx æ ¼å¼åé‡æ–°ä¸Šä¼ ")
            loader = GeneralPPTXLoader(local_file_path, enable_ocr=False)
            if os.getenv("ENV") == "development":
                print(f"ğŸ“Š ä½¿ç”¨ GeneralPPTXLoader å¤„ç† PPTX æ–‡ä»¶")
        elif file_ext == 'txt':
            # 1.2.42: TXT æ–‡æœ¬æ–‡ä»¶ - ä½¿ç”¨è‡ªå®šä¹‰åŠ è½½å™¨ï¼ˆæ”¯æŒç¼–ç æ£€æµ‹ï¼‰
            loader = TxtLoader(local_file_path)
            if os.getenv("ENV") == "development":
                print(f"ğŸ“„ ä½¿ç”¨ TxtLoader å¤„ç† TXT æ–‡ä»¶")
        else:
            # 1.2.42: ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {local_file_path}ã€‚æ”¯æŒçš„æ ¼å¼: pdf, docx, xlsx, pptx, txt")

        docs = loader.load()

        # 1.2.42: æ‰“å°åŠ è½½ç»“æœ
        if os.getenv("ENV") == "development":
            print(f"âœ… æ–‡ä»¶åŠ è½½å®Œæˆï¼Œç”Ÿæˆ {len(docs)} ä¸ªæ–‡æ¡£")
            for i, doc in enumerate(docs):
                content_preview = doc.page_content[:100].replace('\n', ' ') if doc.page_content else ''
                print(f"  æ–‡æ¡£ {i+1}: {len(doc.page_content)} å­—ç¬¦, é¢„è§ˆ: {content_preview}...")

        return {"docs": docs}

    finally:
        # 1.2.43: æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_dir_to_cleanup and os.path.exists(temp_dir_to_cleanup):
            import shutil
            shutil.rmtree(temp_dir_to_cleanup, ignore_errors=True)
            if os.getenv("ENV") == "development":
                print(f"ğŸ—‘ï¸ å·²æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_dir_to_cleanup}")

# 1.1.11: URLçˆ¬è™«èŠ‚ç‚¹
def crawl_url_node(state: GraphState):
    """1.1.11: çˆ¬å–URLå¹¶è§£æä¸ºDocument"""
    urls = state.get('urls', [])
    if not urls:
        if os.getenv("ENV") == "development":
            print("No URLs provided, skipping URL crawling.")
        return {"docs": state.get('docs', [])}
    
    if os.getenv("ENV") == "development":
        print(f"ğŸ•·ï¸ Crawling {len(urls)} URL(s)...")
    
    try:
        # 1.1.11: åœ¨åŒæ­¥å‡½æ•°ä¸­è°ƒç”¨å¼‚æ­¥å‡½æ•°
        # ä¼˜åŒ–äº‹ä»¶å¾ªç¯å¤„ç†ï¼Œç¡®ä¿çº¿ç¨‹å®‰å…¨
        import concurrent.futures
        import threading
        
        def run_async_in_thread():
            """åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œå¼‚æ­¥ä»£ç ï¼Œåˆ›å»ºç‹¬ç«‹çš„äº‹ä»¶å¾ªç¯"""
            # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
            new_loop = asyncio.new_event_loop()
            asyncio.set_event_loop(new_loop)
            try:
                return new_loop.run_until_complete(crawl_urls(urls))
            finally:
                # ç¡®ä¿å…³é—­æ‰€æœ‰å¼‚æ­¥èµ„æº
                try:
                    # å–æ¶ˆæ‰€æœ‰å¾…å¤„ç†çš„ä»»åŠ¡
                    pending = asyncio.all_tasks(new_loop)
                    for task in pending:
                        task.cancel()
                    if pending:
                        new_loop.run_until_complete(asyncio.gather(*pending, return_exceptions=True))
                except:
                    pass
                finally:
                    new_loop.close()
        
        # æ€»æ˜¯åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œï¼Œé¿å…äº‹ä»¶å¾ªç¯å†²çª
        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(run_async_in_thread)
            docs = future.result(timeout=600)  # 10åˆ†é’Ÿè¶…æ—¶ï¼ˆURLçˆ¬å–å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´ï¼‰
        
        if os.getenv("ENV") == "development":
            print(f"âœ… Successfully crawled {len(docs)} URL(s)")
            # 1.1.12: æ‰“å°è¯¦ç»†ä¿¡æ¯ç”¨äºè¯Šæ–­ï¼ŒåŒºåˆ†æˆåŠŸå’Œå¤±è´¥çš„æ–‡æ¡£
            for i, doc in enumerate(docs):
                is_error = 'error' in doc.metadata or 'è§£æå¤±è´¥' in doc.page_content or 'çˆ¬å–å¤±è´¥' in doc.page_content
                status = "âŒ å¤±è´¥" if is_error else "âœ… æˆåŠŸ"
                print(f"  æ–‡æ¡£ {i+1}: {status} - æ¥æº={doc.metadata.get('source', 'unknown')}, å†…å®¹é•¿åº¦={len(doc.page_content)} å­—ç¬¦")
        
        # 1.1.12: æŒ‰ç…§ chatmax é€»è¾‘ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰è§£æå¤±è´¥çš„æ–‡æ¡£
        # å¦‚æœæœ‰å¤±è´¥çš„æ–‡æ¡£ï¼Œç¡®ä¿å®ƒä»¬åŒ…å«æ˜ç¡®çš„é”™è¯¯æ ‡è®°
        processed_docs = []
        for doc in docs:
            # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯æ–‡æ¡£
            is_error = (
                'error' in doc.metadata or 
                'çˆ¬å–å¤±è´¥' in doc.page_content or 
                'è§£æå¤±è´¥' in doc.page_content or
                doc.page_content.strip().startswith('çˆ¬å–å¤±è´¥') or
                doc.page_content.strip().startswith('è§£æå¤±è´¥')
            )
            
            if is_error:
                # ç¡®ä¿é”™è¯¯æ–‡æ¡£åŒ…å«æ˜ç¡®çš„é”™è¯¯æ ‡è®°
                if 'error' not in doc.metadata:
                    doc.metadata['error'] = 'è§£æå¤±è´¥'
                if not doc.page_content.strip().startswith('è§£æå¤±è´¥'):
                    doc.page_content = f"è§£æå¤±è´¥: {doc.page_content}"
            
            processed_docs.append(doc)
        
        return {"docs": processed_docs}
    except Exception as e:
        error_msg = f"URL crawling failed: {str(e)}"
        if os.getenv("ENV") == "development":
            print(f"âŒ {error_msg}")
        # 1.1.12: URLçˆ¬è™«å¤±è´¥æ—¶ä¸ä¸­æ–­æ•´ä¸ªæµç¨‹ï¼Œè¿”å›é”™è¯¯æ–‡æ¡£åˆ—è¡¨
        # è¿™æ ·è‡³å°‘å‰ç«¯èƒ½æ”¶åˆ°å“åº”ï¼Œè€Œä¸æ˜¯å®Œå…¨å¤±è´¥
        error_docs = [
            Document(
                page_content=f"URLçˆ¬å–å¤±è´¥: {str(e)}\nåŸå§‹URL: {url}",
                metadata={
                    "source": url,
                    "title": url,
                    "url": url,
                    "error": str(e)
                }
            )
            for url in urls
        ]
        return {"docs": error_docs}

# 1.1.0: æ–‡æœ¬åˆ‡ç‰‡èŠ‚ç‚¹
def split_text_node(state: GraphState):
    docs = state.get('docs', [])
    if not docs:
        if os.getenv("ENV") == "development":
            print("No documents to split, skipping.")
        return {"splits": state.get('splits', [])}
    
    # 1.1.12: æŒ‰ç…§ chatmax é€»è¾‘ï¼Œè¿‡æ»¤æ‰é”™è¯¯æ–‡æ¡£ï¼ˆä¸è¿›è¡Œåˆ‡ç‰‡å’Œå‘é‡åŒ–ï¼‰
    valid_docs = []
    error_docs = []
    for doc in docs:
        is_error = (
            'error' in doc.metadata or 
            'çˆ¬å–å¤±è´¥' in doc.page_content or 
            'è§£æå¤±è´¥' in doc.page_content or
            doc.page_content.strip().startswith('çˆ¬å–å¤±è´¥') or
            doc.page_content.strip().startswith('è§£æå¤±è´¥')
        )
        if is_error:
            error_docs.append(doc)
        else:
            valid_docs.append(doc)
    
    if error_docs and os.getenv("ENV") == "development":
        print(f"âš ï¸ è·³è¿‡ {len(error_docs)} ä¸ªé”™è¯¯æ–‡æ¡£çš„åˆ‡ç‰‡å’Œå‘é‡åŒ–")
        for doc in error_docs:
            print(f"  é”™è¯¯æ–‡æ¡£: {doc.metadata.get('source', 'unknown')} - {doc.metadata.get('error', 'è§£æå¤±è´¥')}")
    
    if not valid_docs:
        if os.getenv("ENV") == "development":
            print("âš ï¸ æ‰€æœ‰æ–‡æ¡£éƒ½æ˜¯é”™è¯¯æ–‡æ¡£ï¼Œè·³è¿‡åˆ‡ç‰‡")
        return {"splits": state.get('splits', [])}
        
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(valid_docs)
    
    if os.getenv("ENV") == "development":
        print(f"Split into {len(splits)} chunks")
        # 1.1.11: æ‰“å°åˆ‡ç‰‡æ¥æºä¿¡æ¯
        url_splits = sum(1 for s in splits if s.metadata.get('source', '').startswith(('http://', 'https://')))
        if url_splits > 0:
            print(f"  å…¶ä¸­ {url_splits} ä¸ªåˆ‡ç‰‡æ¥è‡ªURLçˆ¬å–")
    
    return {"splits": splits}

# 1.1.0: å‘é‡å­˜å‚¨èŠ‚ç‚¹
# 1.1.3: æ”¯æŒ Chroma å’Œ pgvector åˆ‡æ¢
# 1.1.13: åŠ å¼ºçŸ¥è¯†åº“éš”ç¦»ï¼ŒéªŒè¯ collection_name
def embed_and_store_node(state: GraphState):
    splits = state.get('splits', [])
    collection_name = state.get('collection_name', 'default_collection')
    
    # 1.1.15: éªŒè¯ collection_name æ ¼å¼ï¼Œç¡®ä¿çŸ¥è¯†åº“éš”ç¦»
    if not collection_name or not isinstance(collection_name, str) or not collection_name.strip():
        error_msg = f"collection_name must be a non-empty string, got: {type(collection_name).__name__}={collection_name}"
        if os.getenv("ENV") == "development":
            print(f"âŒ Embed and store node error: {error_msg}")
        raise ValueError(error_msg)
    
    # 1.1.15: ç¡®ä¿ collection_name ç¬¦åˆå‘½åè§„èŒƒï¼ˆé˜²æ­¢æ³¨å…¥æ”»å‡»ï¼‰
    collection_name = collection_name.strip()
    if not re.match(r'^[a-zA-Z0-9_-]+$', collection_name):
        error_msg = f"Invalid collection_name format: {collection_name}"
        if os.getenv("ENV") == "development":
            print(f"âŒ Embed and store node error: {error_msg}")
        raise ValueError(error_msg)
    
    if not splits:
        if os.getenv("ENV") == "development":  # 1.1.3: ä»…å¼€å‘ç¯å¢ƒè¾“å‡ºæ—¥å¿—
            print("No splits to store, skipping embedding.")
        return {"collection_name": collection_name}
    
    if USE_PGVECTOR and DATABASE_URL:
        # 1.1.3: ä½¿ç”¨ Supabase pgvectorï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰
        try:
            vx = vecs.create_client(DATABASE_URL)
            
            # è·å–æˆ–åˆ›å»º collection
            try:
                collection = vx.get_collection(name=collection_name)
            except:
                collection = vx.create_collection(
                    name=collection_name,
                    dimension=1536  # OpenAI embeddings ç»´åº¦
                )
            
            # ç”Ÿæˆå‘é‡å¹¶å­˜å‚¨
            embeddings_model = OpenAIEmbeddings()
            texts = [doc.page_content for doc in splits]
            metadatas = [doc.metadata for doc in splits]
            vectors = embeddings_model.embed_documents(texts)
            
            # 1.2.56: æ¸…ç†æ–‡æœ¬å’Œ metadata ä¸­çš„ç©ºå­—ç¬¦ï¼ˆ\u0000ï¼‰ï¼Œé˜²æ­¢ pgvector æ’å…¥å¤±è´¥
            def clean_null_chars(obj):
                """é€’å½’æ¸…ç†å¯¹è±¡ä¸­çš„ç©ºå­—ç¬¦"""
                if isinstance(obj, str):
                    return obj.replace('\x00', '').replace('\u0000', '')
                elif isinstance(obj, dict):
                    return {k: clean_null_chars(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [clean_null_chars(item) for item in obj]
                return obj
            
            texts = [clean_null_chars(text) for text in texts]
            metadatas = [clean_null_chars(metadata) for metadata in metadatas]
            
            # å‡†å¤‡æ•°æ®
            records = [
                (f"{collection_name}_{i}", vector, {"text": text, **metadata})
                for i, (vector, text, metadata) in enumerate(zip(vectors, texts, metadatas))
            ]
            
            # æ‰¹é‡æ’å…¥
            collection.upsert(records=records)
            
            if os.getenv("ENV") == "development":  # 1.1.3: ä»…å¼€å‘ç¯å¢ƒè¾“å‡ºæ—¥å¿—
                print(f"âœ… Stored {len(splits)} vectors in Supabase pgvector: {collection_name}")
                # 1.1.11: æ‰“å°å­˜å‚¨çš„æ–‡æ¡£æ¥æºç»Ÿè®¡
                url_vectors = sum(1 for m in metadatas if m.get('source', '').startswith(('http://', 'https://')))
                if url_vectors > 0:
                    print(f"  å…¶ä¸­ {url_vectors} ä¸ªå‘é‡æ¥è‡ªURLçˆ¬å–")
        except Exception as e:
            print(f"âŒ pgvector error: {e}, falling back to Chroma")
            # 1.2.56: å›é€€æ—¶éœ€è¦å…ˆå¯¼å…¥ Chroma
            from langchain_community.vectorstores import Chroma as ChromaFallback
            vectorstore = ChromaFallback.from_documents(
                documents=splits,
                embedding=OpenAIEmbeddings(),
                persist_directory=f"./chroma_db/{collection_name}"
            )
    else:
        # 1.1.0: ä½¿ç”¨ Chroma ä½œä¸ºæœ¬åœ°å‘é‡åº“ï¼ˆæœ¬åœ°å¼€å‘ï¼‰
        # 1.2.56: Chroma å·²åœ¨æ¡ä»¶å¯¼å…¥å—ä¸­å¯¼å…¥
        vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=OpenAIEmbeddings(),
            persist_directory=f"./chroma_db/{collection_name}"
        )
        if os.getenv("ENV") == "development":  # 1.1.3: ä»…å¼€å‘ç¯å¢ƒè¾“å‡ºæ—¥å¿—
            print(f"âœ… Stored {len(splits)} vectors in Chroma: {collection_name}")
            # 1.1.11: æ‰“å°å­˜å‚¨çš„æ–‡æ¡£æ¥æºç»Ÿè®¡
            url_vectors = sum(1 for doc in splits if doc.metadata.get('source', '').startswith(('http://', 'https://')))
            if url_vectors > 0:
                print(f"  å…¶ä¸­ {url_vectors} ä¸ªå‘é‡æ¥è‡ªURLçˆ¬å–")
    
    return {"collection_name": collection_name}

# 1.1.0: æ£€ç´¢ä¸é—®ç­”èŠ‚ç‚¹ (RAG)
# 1.1.3: æ”¯æŒ Chroma å’Œ pgvector åˆ‡æ¢
# 1.1.13: åŠ å¼ºçŸ¥è¯†åº“éš”ç¦»ï¼ŒéªŒè¯ collection_name
def chat_node(state: GraphState):
    messages = state.get('messages', [])
    if not messages:
        if os.getenv("ENV") == "development":  # 1.1.3: ä»…å¼€å‘ç¯å¢ƒè¾“å‡ºæ—¥å¿—
            print("No messages provided, skipping chat node.")
        return {"answer": "", "messages": []}
        
    collection_name = state.get('collection_name')
    
    # 1.1.15: éªŒè¯ collection_nameï¼Œç¡®ä¿çŸ¥è¯†åº“éš”ç¦»
    if not collection_name or not isinstance(collection_name, str) or not collection_name.strip():
        error_msg = f"collection_name must be a non-empty string, got: {type(collection_name).__name__}={collection_name}"
        if os.getenv("ENV") == "development":
            print(f"âŒ Chat node error: {error_msg}")
        raise ValueError(error_msg)
    
    # 1.1.15: ç¡®ä¿ collection_name ç¬¦åˆå‘½åè§„èŒƒï¼ˆé˜²æ­¢æ³¨å…¥æ”»å‡»ï¼‰
    collection_name = collection_name.strip()
    if not re.match(r'^[a-zA-Z0-9_-]+$', collection_name):
        error_msg = f"Invalid collection_name format: {collection_name}"
        if os.getenv("ENV") == "development":
            print(f"âŒ Chat node error: {error_msg}")
        raise ValueError(error_msg)
    
    # è·å–æœ€åä¸€æ¡æ¶ˆæ¯ï¼ˆç”¨æˆ·é—®é¢˜ï¼‰
    user_query = messages[-1].content
    
    # 1.3.11: åˆå§‹åŒ– citations æ•°ç»„
    citations = []
    
    # 1.1.3: æ ¹æ®é…ç½®é€‰æ‹©å‘é‡æ•°æ®åº“
    if USE_PGVECTOR and DATABASE_URL:
        # ä½¿ç”¨ Supabase pgvector
        try:
            vx = vecs.create_client(DATABASE_URL)
            collection = vx.get_collection(name=collection_name)
            
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            embeddings_model = OpenAIEmbeddings()
            query_vector = embeddings_model.embed_query(user_query)
            
            # 1.3.11: æ£€ç´¢ç›¸ä¼¼æ–‡æ¡£ï¼Œå¯ç”¨ include_value è·å–ç›¸ä¼¼åº¦åˆ†æ•°
            # 1.3.10: æ—§ç‰ˆæœ¬ä½¿ç”¨ include_value=False
            results = collection.query(
                data=query_vector,
                limit=MAX_CHUNKS,
                include_value=True,  # 1.3.11: å¯ç”¨ç›¸ä¼¼åº¦åˆ†æ•°
                include_metadata=True
            )
            
            # 1.3.11: æå–æ–‡æœ¬å¹¶æ”¶é›† citationsï¼ˆvecs è¿”å›æ ¼å¼: (id, score, metadata)ï¼‰
            # 1.3.10: æ—§ç‰ˆæœ¬è¿”å›æ ¼å¼: (id, metadata)
            valid_texts = []
            for record in results:
                # 1.3.11: æ ¹æ® include_value è°ƒæ•´è§£æé€»è¾‘
                if len(record) >= 3:
                    record_id = record[0]
                    score = record[1]
                    metadata = record[2] if record[2] else {}
                elif len(record) >= 2:
                    record_id = record[0]
                    score = None
                    metadata = record[1] if record[1] else {}
                else:
                    continue
                    
                text = metadata.get("text", "")
                source = metadata.get("source", metadata.get("url", ""))
                inner_metadata = metadata.get("metadata", {}) if isinstance(metadata, dict) else {}
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯æ–‡æ¡£
                is_error = (
                    'error' in inner_metadata or 
                    'çˆ¬å–å¤±è´¥' in text or 
                    'è§£æå¤±è´¥' in text or
                    text.strip().startswith('çˆ¬å–å¤±è´¥') or
                    text.strip().startswith('è§£æå¤±è´¥')
                )
                if not is_error and text.strip():
                    valid_texts.append(text)
                    # 1.3.11: æ”¶é›† citation ä¿¡æ¯ï¼ˆé™åˆ¶å†…å®¹é•¿åº¦ä¸º500å­—ç¬¦ï¼‰
                    citations.append({
                        "id": record_id,
                        "source": source,
                        "content": text[:500] if len(text) > 500 else text,
                        "score": float(score) if score is not None else None
                    })
            
            # 1.2.12: ä½¿ç”¨å¯é…ç½®çš„ç‰‡æ®µæ•°é‡é™åˆ¶
            context = "\n\n".join(valid_texts[:MAX_CHUNKS])
            # 1.3.11: é™åˆ¶ citations æ•°é‡ï¼Œåªè¿”å›ç›¸å…³åº¦æœ€é«˜çš„å‰5ä¸ª
            citations = citations[:5]
            
            # 1.1.11: æ£€æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦ä¸ºç©º
            if not context or not context.strip() or len(context.strip()) < 50:
                if os.getenv("ENV") == "development":
                    print("âš ï¸ è­¦å‘Š: pgvectoræ£€ç´¢åä¸Šä¸‹æ–‡ä¸ºç©ºæˆ–è¿‡çŸ­")
            
        except Exception as e:
            print(f"âŒ pgvector query error: {e}, falling back to Chroma")
            # 1.2.56: å›é€€æ—¶éœ€è¦å…ˆå¯¼å…¥ Chroma
            from langchain_community.vectorstores import Chroma as ChromaFallback
            vectorstore = ChromaFallback(
                persist_directory=f"./chroma_db/{collection_name}",
                embedding_function=OpenAIEmbeddings()
            )
            # 1.2.12: ä½¿ç”¨å¯é…ç½®çš„æ£€ç´¢æ•°é‡
            retriever = vectorstore.as_retriever(search_kwargs={"k": RETRIEVE_K})
            relevant_docs = retriever.invoke(user_query)
            
            # 1.1.11: è¿‡æ»¤æ‰é”™è¯¯æ–‡æ¡£
            valid_docs = []
            for idx, doc in enumerate(relevant_docs):
                is_error = (
                    'error' in doc.metadata or 
                    'çˆ¬å–å¤±è´¥' in doc.page_content or 
                    'è§£æå¤±è´¥' in doc.page_content or
                    doc.page_content.strip().startswith('çˆ¬å–å¤±è´¥') or
                    doc.page_content.strip().startswith('è§£æå¤±è´¥')
                )
                if not is_error:
                    valid_docs.append(doc)
                    # 1.3.11: æ”¶é›† citation ä¿¡æ¯ï¼ˆChroma å›é€€æ—¶ï¼‰
                    source = doc.metadata.get('source', doc.metadata.get('url', ''))
                    content = doc.page_content[:500] if len(doc.page_content) > 500 else doc.page_content
                    citations.append({
                        "id": f"chroma-fallback-{idx}",
                        "source": source,
                        "content": content,
                        "score": None  # Chroma retriever ä¸è¿”å›åˆ†æ•°
                    })
            
            # 1.2.12: ä½¿ç”¨å¯é…ç½®çš„ç‰‡æ®µæ•°é‡é™åˆ¶
            if valid_docs:
                relevant_docs = valid_docs[:MAX_CHUNKS]
            
            context = "\n\n".join([doc.page_content for doc in relevant_docs])
            # 1.3.11: é™åˆ¶ citations æ•°é‡
            citations = citations[:5]
            
            # 1.1.11: æ£€æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦ä¸ºç©º
            if not context or not context.strip() or len(context.strip()) < 50:
                if os.getenv("ENV") == "development":
                    print("âš ï¸ è­¦å‘Š: pgvectorå›é€€åˆ°Chromaåä¸Šä¸‹æ–‡ä¸ºç©ºæˆ–è¿‡çŸ­")
    else:
        # ä½¿ç”¨ Chromaï¼ˆæœ¬åœ°å¼€å‘ï¼‰
        # 1.2.56: Chroma å·²åœ¨æ¡ä»¶å¯¼å…¥å—ä¸­å¯¼å…¥
        vectorstore = Chroma(
            persist_directory=f"./chroma_db/{collection_name}",
            embedding_function=OpenAIEmbeddings()
        )
        # 1.2.12: ä½¿ç”¨å¯é…ç½®çš„æ£€ç´¢æ•°é‡
        retriever = vectorstore.as_retriever(search_kwargs={"k": RETRIEVE_K})
        relevant_docs = retriever.invoke(user_query)
        
        # 1.1.11: è¿‡æ»¤æ‰é”™è¯¯æ–‡æ¡£ï¼ˆåŒ…å«"çˆ¬å–å¤±è´¥"æˆ–"error"å­—æ®µçš„æ–‡æ¡£ï¼‰
        valid_docs = []
        for idx, doc in enumerate(relevant_docs):
            # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯æ–‡æ¡£
            is_error = (
                'error' in doc.metadata or 
                'çˆ¬å–å¤±è´¥' in doc.page_content or 
                'è§£æå¤±è´¥' in doc.page_content or
                doc.page_content.strip().startswith('çˆ¬å–å¤±è´¥') or
                doc.page_content.strip().startswith('è§£æå¤±è´¥')
            )
            if not is_error:
                valid_docs.append(doc)
                # 1.3.11: æ”¶é›† citation ä¿¡æ¯ï¼ˆChroma æœ¬åœ°æ¨¡å¼ï¼‰
                source = doc.metadata.get('source', doc.metadata.get('url', ''))
                content = doc.page_content[:500] if len(doc.page_content) > 500 else doc.page_content
                citations.append({
                    "id": f"chroma-{idx}",
                    "source": source,
                    "content": content,
                    "score": None  # Chroma retriever ä¸è¿”å›åˆ†æ•°
                })
        
        # 1.2.12: å¦‚æœè¿‡æ»¤åè¿˜æœ‰æ–‡æ¡£ï¼Œä½¿ç”¨è¿‡æ»¤åçš„ï¼›å¦åˆ™ä½¿ç”¨åŸå§‹çš„ï¼ˆè‡³å°‘è¿”å›ä¸€äº›å†…å®¹ï¼‰
        original_count = len(relevant_docs)
        if valid_docs:
            # 1.2.12: ä½¿ç”¨å¯é…ç½®çš„ç‰‡æ®µæ•°é‡é™åˆ¶
            relevant_docs = valid_docs[:MAX_CHUNKS]
            # 1.3.11: é™åˆ¶ citations æ•°é‡
            citations = citations[:5]
            if os.getenv("ENV") == "development":
                print(f"ğŸ” æ£€ç´¢åˆ° {len(valid_docs)} ä¸ªæœ‰æ•ˆæ–‡æ¡£ï¼ˆå·²è¿‡æ»¤ {original_count - len(valid_docs)} ä¸ªé”™è¯¯æ–‡æ¡£ï¼‰")
        else:
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ–‡æ¡£ï¼Œä½¿ç”¨åŸå§‹ç»“æœï¼ˆå¯èƒ½éƒ½æ˜¯é”™è¯¯æ–‡æ¡£ï¼‰
            # 1.2.12: ä½¿ç”¨å¯é…ç½®çš„ç‰‡æ®µæ•°é‡é™åˆ¶
            relevant_docs = relevant_docs[:MAX_CHUNKS]
            if os.getenv("ENV") == "development":
                print(f"âš ï¸ æ£€ç´¢åˆ°çš„æ–‡æ¡£å¯èƒ½åŒ…å«é”™è¯¯ï¼Œä½¿ç”¨åŸå§‹ç»“æœ")
        
        if os.getenv("ENV") == "development":
            print(f"  æœ€ç»ˆä½¿ç”¨ {len(relevant_docs)} ä¸ªæ–‡æ¡£ç‰‡æ®µ:")
            for i, doc in enumerate(relevant_docs):
                source = doc.metadata.get('source', 'unknown')
                print(f"  [{i+1}] æ¥æº: {source}")
        
        context = "\n\n".join([doc.page_content for doc in relevant_docs])
    
    # 1.2.52: è·å–è¯­è¨€è®¾ç½®ï¼Œé»˜è®¤ä¸ºä¸­æ–‡
    language = state.get('language', 'zh')
    if language not in ['zh', 'en', 'ja']:
        language = 'zh'
    
    # 1.2.52: å¤šè¯­è¨€ç©ºä¸Šä¸‹æ–‡æç¤º
    empty_context_messages = {
        'zh': "æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨çš„é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ã€‚è¯·å°è¯•ï¼š\n1. ä½¿ç”¨ä¸åŒçš„å…³é”®è¯æé—®\n2. ç¡®è®¤ç›¸å…³çŸ¥è¯†åº“æ–‡æ¡£å·²æ­£ç¡®ä¸Šä¼ å’Œç´¢å¼•\n3. æ£€æŸ¥æŸ¥è¯¢æ˜¯å¦æ­£ç¡®",
        'en': "Sorry, I couldn't find any relevant information in the knowledge base related to your question. Please try:\n1. Using different keywords\n2. Confirming the relevant documents have been uploaded and indexed\n3. Checking if your query is correct",
        'ja': "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã«ã”è³ªå•ã«é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ä»¥ä¸‹ã‚’ãŠè©¦ã—ãã ã•ã„ï¼š\n1. ç•°ãªã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è³ªå•ã™ã‚‹\n2. é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹\n3. ã‚¯ã‚¨ãƒªãŒæ­£ã—ã„ã‹ç¢ºèªã™ã‚‹"
    }
    
    # 1.1.11: å¦‚æœä¸Šä¸‹æ–‡ä¸ºç©ºæˆ–åªæœ‰é”™è¯¯ä¿¡æ¯ï¼Œç»™å‡ºæç¤º
    if not context or not context.strip() or len(context.strip()) < 50:
        if os.getenv("ENV") == "development":
            print("âš ï¸ è­¦å‘Š: ä¸Šä¸‹æ–‡ä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œå¯èƒ½æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
        # è¿”å›ä¸€ä¸ªå‹å¥½çš„æç¤ºï¼ˆ1.2.52: æ ¹æ®è¯­è¨€è¿”å›ï¼‰
        # 1.3.11: æ·»åŠ  citations å­—æ®µ
        return {
            "answer": empty_context_messages.get(language, empty_context_messages['zh']),
            "messages": messages,
            "context": "",
            "citations": []  # 1.3.11: ç©ºä¸Šä¸‹æ–‡æ—¶æ— å¼•ç”¨
        }
    
    # 1.2.52: å¤šè¯­è¨€ç³»ç»Ÿæç¤ºè¯
    system_prompts = {
        'zh': "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åº“åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®åœ°è¯´ä½ ä¸çŸ¥é“ã€‚è¯·ä½¿ç”¨ä¸­æ–‡å›å¤ã€‚\n\nä¸Šä¸‹æ–‡:\n{context}",
        'en': "You are a professional knowledge base assistant. Please answer the user's question based on the context provided below. If there is no relevant information in the context, please honestly say you don't know. Please respond in English.\n\nContext:\n{context}",
        'ja': "ã‚ãªãŸã¯ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã«æä¾›ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«é–¢é€£æƒ…å ±ãŒãªã„å ´åˆã¯ã€æ­£ç›´ã«ã‚ã‹ã‚‰ãªã„ã¨è¨€ã£ã¦ãã ã•ã„ã€‚æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚\n\nã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:\n{context}"
    }
    
    # ç”Ÿæˆå›ç­”
    # 1.2.52: ä½¿ç”¨å¤šè¯­è¨€ç³»ç»Ÿæç¤ºè¯
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompts.get(language, system_prompts['zh'])),
        MessagesPlaceholder(variable_name="messages"),
    ])
    
    llm = ChatOpenAI(model="gpt-4o", streaming=True)
    chain = prompt | llm
    
    response = chain.invoke({"context": context, "messages": messages})
    
    # 1.3.11: æ·»åŠ  citations å­—æ®µåˆ°è¿”å›å€¼
    return {
        "answer": response.content,
        "messages": messages + [response],
        "context": context,
        "citations": citations  # 1.3.11: å¼•ç”¨æ¥æºåˆ—è¡¨
    }

# 1.2.24: æµå¼ç‰ˆæœ¬çš„ chat_nodeï¼Œç”¨äºæ”¯æŒ SSE æµå¼è¾“å‡º
async def chat_node_stream(state: GraphState):
    """
    1.2.24: æµå¼ç‰ˆæœ¬çš„èŠå¤©èŠ‚ç‚¹ï¼Œæ”¯æŒå®æ—¶è¾“å‡º AI å›å¤
    ä¸ chat_node ç±»ä¼¼ï¼Œä½†ä½¿ç”¨ astream ç”Ÿæˆå™¨è¿”å›æ•°æ®å—
    """
    messages = state.get('messages', [])
    if not messages:
        if os.getenv("ENV") == "development":
            print("No messages provided, skipping chat node.")
        yield {"answer": "", "done": True}
        return
        
    collection_name = state.get('collection_name')
    
    # 1.2.24: éªŒè¯ collection_nameï¼Œç¡®ä¿çŸ¥è¯†åº“éš”ç¦»
    if not collection_name or not isinstance(collection_name, str) or not collection_name.strip():
        error_msg = f"collection_name must be a non-empty string, got: {type(collection_name).__name__}={collection_name}"
        if os.getenv("ENV") == "development":
            print(f"âŒ Chat node error: {error_msg}")
        raise ValueError(error_msg)
    
    # 1.2.24: ç¡®ä¿ collection_name ç¬¦åˆå‘½åè§„èŒƒï¼ˆé˜²æ­¢æ³¨å…¥æ”»å‡»ï¼‰
    collection_name = collection_name.strip()
    if not re.match(r'^[a-zA-Z0-9_-]+$', collection_name):
        error_msg = f"Invalid collection_name format: {collection_name}"
        if os.getenv("ENV") == "development":
            print(f"âŒ Chat node error: {error_msg}")
        raise ValueError(error_msg)
    
    # è·å–æœ€åä¸€æ¡æ¶ˆæ¯ï¼ˆç”¨æˆ·é—®é¢˜ï¼‰
    user_query = messages[-1].content
    
    # 1.3.11: åˆå§‹åŒ– citations æ•°ç»„
    citations = []
    
    # 1.2.24: å‘é‡æ£€ç´¢ï¼ˆä¸ chat_node ç›¸åŒçš„é€»è¾‘ï¼‰
    context = ""
    if USE_PGVECTOR and DATABASE_URL:
        # ä½¿ç”¨ Supabase pgvector
        try:
            vx = vecs.create_client(DATABASE_URL)
            collection = vx.get_collection(name=collection_name)
            
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            embeddings_model = OpenAIEmbeddings()
            query_vector = embeddings_model.embed_query(user_query)
            
            # 1.3.11: æ£€ç´¢ç›¸ä¼¼æ–‡æ¡£ï¼Œå¯ç”¨ include_value è·å–ç›¸ä¼¼åº¦åˆ†æ•°
            results = collection.query(
                data=query_vector,
                limit=MAX_CHUNKS,
                include_value=True,  # 1.3.11: å¯ç”¨ç›¸ä¼¼åº¦åˆ†æ•°
                include_metadata=True
            )
            
            # 1.3.11: æå–æ–‡æœ¬å¹¶æ”¶é›† citations
            valid_texts = []
            for record in results:
                # 1.3.11: æ ¹æ® include_value è°ƒæ•´è§£æé€»è¾‘
                if len(record) >= 3:
                    record_id = record[0]
                    score = record[1]
                    metadata = record[2] if record[2] else {}
                elif len(record) >= 2:
                    record_id = record[0]
                    score = None
                    metadata = record[1] if record[1] else {}
                else:
                    continue
                    
                text = metadata.get("text", "")
                source = metadata.get("source", metadata.get("url", ""))
                inner_metadata = metadata.get("metadata", {}) if isinstance(metadata, dict) else {}
                
                is_error = (
                    'error' in inner_metadata or 
                    'çˆ¬å–å¤±è´¥' in text or 
                    'è§£æå¤±è´¥' in text or
                    text.strip().startswith('çˆ¬å–å¤±è´¥') or
                    text.strip().startswith('è§£æå¤±è´¥')
                )
                if not is_error and text.strip():
                    valid_texts.append(text)
                    # 1.3.11: æ”¶é›† citation ä¿¡æ¯ï¼ˆé™åˆ¶å†…å®¹é•¿åº¦ä¸º500å­—ç¬¦ï¼‰
                    citations.append({
                        "id": record_id,
                        "source": source,
                        "content": text[:500] if len(text) > 500 else text,
                        "score": float(score) if score is not None else None
                    })
            
            context = "\n\n".join(valid_texts[:MAX_CHUNKS])
            # 1.3.11: é™åˆ¶ citations æ•°é‡
            citations = citations[:5]
            
            if not context or not context.strip() or len(context.strip()) < 50:
                if os.getenv("ENV") == "development":
                    print("âš ï¸ è­¦å‘Š: pgvectoræ£€ç´¢åä¸Šä¸‹æ–‡ä¸ºç©ºæˆ–è¿‡çŸ­")
            
        except Exception as e:
            print(f"âŒ pgvector query error: {e}, falling back to Chroma")
            # 1.2.56: å›é€€æ—¶éœ€è¦å…ˆå¯¼å…¥ Chroma
            from langchain_community.vectorstores import Chroma as ChromaFallback
            vectorstore = ChromaFallback(
                persist_directory=f"./chroma_db/{collection_name}",
                embedding_function=OpenAIEmbeddings()
            )
            retriever = vectorstore.as_retriever(search_kwargs={"k": RETRIEVE_K})
            relevant_docs = retriever.invoke(user_query)
            
            valid_docs = []
            for idx, doc in enumerate(relevant_docs):
                is_error = (
                    'error' in doc.metadata or 
                    'çˆ¬å–å¤±è´¥' in doc.page_content or 
                    'è§£æå¤±è´¥' in doc.page_content or
                    doc.page_content.strip().startswith('çˆ¬å–å¤±è´¥') or
                    doc.page_content.strip().startswith('è§£æå¤±è´¥')
                )
                if not is_error:
                    valid_docs.append(doc)
                    # 1.3.11: æ”¶é›† citation ä¿¡æ¯ï¼ˆChroma å›é€€æ—¶ï¼‰
                    source = doc.metadata.get('source', doc.metadata.get('url', ''))
                    content = doc.page_content[:500] if len(doc.page_content) > 500 else doc.page_content
                    citations.append({
                        "id": f"chroma-stream-fallback-{idx}",
                        "source": source,
                        "content": content,
                        "score": None
                    })
            
            if valid_docs:
                relevant_docs = valid_docs[:MAX_CHUNKS]
            
            context = "\n\n".join([doc.page_content for doc in relevant_docs])
            # 1.3.11: é™åˆ¶ citations æ•°é‡
            citations = citations[:5]
    else:
        # ä½¿ç”¨ Chromaï¼ˆæœ¬åœ°å¼€å‘ï¼‰
        # 1.2.56: Chroma å·²åœ¨æ¡ä»¶å¯¼å…¥å—ä¸­å¯¼å…¥
        vectorstore = Chroma(
            persist_directory=f"./chroma_db/{collection_name}",
            embedding_function=OpenAIEmbeddings()
        )
        retriever = vectorstore.as_retriever(search_kwargs={"k": RETRIEVE_K})
        relevant_docs = retriever.invoke(user_query)
        
        valid_docs = []
        for idx, doc in enumerate(relevant_docs):
            is_error = (
                'error' in doc.metadata or 
                'çˆ¬å–å¤±è´¥' in doc.page_content or 
                'è§£æå¤±è´¥' in doc.page_content or
                doc.page_content.strip().startswith('çˆ¬å–å¤±è´¥') or
                doc.page_content.strip().startswith('è§£æå¤±è´¥')
            )
            if not is_error:
                valid_docs.append(doc)
                # 1.3.11: æ”¶é›† citation ä¿¡æ¯ï¼ˆChroma æœ¬åœ°æ¨¡å¼ï¼‰
                source = doc.metadata.get('source', doc.metadata.get('url', ''))
                content = doc.page_content[:500] if len(doc.page_content) > 500 else doc.page_content
                citations.append({
                    "id": f"chroma-stream-{idx}",
                    "source": source,
                    "content": content,
                    "score": None
                })
        
        original_count = len(relevant_docs)
        if valid_docs:
            relevant_docs = valid_docs[:MAX_CHUNKS]
            # 1.3.11: é™åˆ¶ citations æ•°é‡
            citations = citations[:5]
            if os.getenv("ENV") == "development":
                print(f"ğŸ” æ£€ç´¢åˆ° {len(valid_docs)} ä¸ªæœ‰æ•ˆæ–‡æ¡£ï¼ˆå·²è¿‡æ»¤ {original_count - len(valid_docs)} ä¸ªé”™è¯¯æ–‡æ¡£ï¼‰")
        else:
            relevant_docs = relevant_docs[:MAX_CHUNKS]
            if os.getenv("ENV") == "development":
                print(f"âš ï¸ æ£€ç´¢åˆ°çš„æ–‡æ¡£å¯èƒ½åŒ…å«é”™è¯¯ï¼Œä½¿ç”¨åŸå§‹ç»“æœ")
        
        context = "\n\n".join([doc.page_content for doc in relevant_docs])
    
    # 1.2.52: è·å–è¯­è¨€è®¾ç½®ï¼Œé»˜è®¤ä¸ºä¸­æ–‡
    language = state.get('language', 'zh')
    if language not in ['zh', 'en', 'ja']:
        language = 'zh'
    
    # 1.2.52: å¤šè¯­è¨€ç©ºä¸Šä¸‹æ–‡æç¤º
    empty_context_messages = {
        'zh': "æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨çš„é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ã€‚",
        'en': "Sorry, I couldn't find any relevant information in the knowledge base related to your question.",
        'ja': "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã«ã”è³ªå•ã«é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
    }
    
    # 1.2.24: å¦‚æœä¸Šä¸‹æ–‡ä¸ºç©ºï¼Œè¿”å›å‹å¥½æç¤º
    # 1.3.11: æ·»åŠ  citations å­—æ®µ
    if not context or not context.strip() or len(context.strip()) < 50:
        if os.getenv("ENV") == "development":
            print("âš ï¸ è­¦å‘Š: ä¸Šä¸‹æ–‡ä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œå¯èƒ½æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
        yield {
            "answer": empty_context_messages.get(language, empty_context_messages['zh']),
            "done": True,
            "context": "",
            "citations": []  # 1.3.11: ç©ºä¸Šä¸‹æ–‡æ—¶æ— å¼•ç”¨
        }
        return
    
    # 1.2.52: å¤šè¯­è¨€ç³»ç»Ÿæç¤ºè¯
    system_prompts = {
        'zh': "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åº“åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®åœ°è¯´ä½ ä¸çŸ¥é“ã€‚è¯·ä½¿ç”¨ä¸­æ–‡å›å¤ã€‚\n\nä¸Šä¸‹æ–‡:\n{context}",
        'en': "You are a professional knowledge base assistant. Please answer the user's question based on the context provided below. If there is no relevant information in the context, please honestly say you don't know. Please respond in English.\n\nContext:\n{context}",
        'ja': "ã‚ãªãŸã¯ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã«æä¾›ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«é–¢é€£æƒ…å ±ãŒãªã„å ´åˆã¯ã€æ­£ç›´ã«ã‚ã‹ã‚‰ãªã„ã¨è¨€ã£ã¦ãã ã•ã„ã€‚æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚\n\nã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:\n{context}"
    }
    
    # 1.2.24: ç”Ÿæˆæµå¼å›ç­”
    # 1.2.52: ä½¿ç”¨å¤šè¯­è¨€ç³»ç»Ÿæç¤ºè¯
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompts.get(language, system_prompts['zh'])),
        MessagesPlaceholder(variable_name="messages"),
    ])
    
    llm = ChatOpenAI(model="gpt-4o", streaming=True)
    chain = prompt | llm
    
    # 1.2.24: ä½¿ç”¨ astream è¿›è¡Œæµå¼è¾“å‡º
    full_response = ""
    async for chunk in chain.astream({"context": context, "messages": messages}):
        if chunk.content:
            full_response += chunk.content
            # å‘é€æ•°æ®å—
            yield {
                "chunk": chunk.content,
                "done": False
            }
    
    # 1.2.24: å‘é€å®Œæˆæ ‡è®°ï¼ŒåŒ…å«å®Œæ•´ç­”æ¡ˆå’Œä¸Šä¸‹æ–‡
    # 1.3.11: æ·»åŠ  citations å­—æ®µ
    yield {
        "answer": full_response,
        "context": context,
        "done": True,
        "citations": citations  # 1.3.11: å¼•ç”¨æ¥æºåˆ—è¡¨
    }

# 1.1.11: å…¥å£èŠ‚ç‚¹ - æ ¹æ®è¾“å…¥ç±»å‹è·¯ç”±åˆ°ä¸åŒå¤„ç†èŠ‚ç‚¹
def entry_node(state: GraphState):
    """1.1.11: å…¥å£èŠ‚ç‚¹ï¼Œä¸åšä»»ä½•å¤„ç†ï¼Œä»…ç”¨äºè·¯ç”±"""
    return state

# 1.1.11: å…¥å£è·¯ç”±å‡½æ•° - å†³å®šå¤„ç†æ–‡ä»¶ã€URLè¿˜æ˜¯ç›´æ¥èŠå¤©
def route_entry(state: GraphState) -> str:
    """
    1.1.11: æ ¹æ®çŠ¶æ€å†³å®šå…¥å£ç‚¹
    - å¦‚æœæœ‰ file_path â†’ process_file
    - å¦‚æœæœ‰ urls â†’ crawl_url
    - å¦åˆ™ç›´æ¥è·³åˆ°èŠå¤©
    """
    file_path = state.get('file_path', '')
    urls = state.get('urls', [])
    
    if file_path and file_path.strip():
        return "process_file"
    elif urls and len(urls) > 0:
        return "crawl_url"
    else:
        # ç›´æ¥è·³åˆ°èŠå¤©
        return "chat"

# 1.1.10: æ¡ä»¶è·¯ç”±å‡½æ•° - åˆ¤æ–­æ˜¯å¦éœ€è¦å¤„ç†æ–‡ä»¶
def should_process_file(state: GraphState) -> str:
    """
    1.1.10: æ ¹æ®çŠ¶æ€å†³å®šä¸‹ä¸€æ­¥ï¼ˆä»process_fileèŠ‚ç‚¹ï¼‰
    - å¦‚æœæœ‰ file_pathï¼Œè¿›è¡Œæ–‡ä»¶å¤„ç†ååˆ°split_text
    - å¦åˆ™ç›´æ¥è·³åˆ°èŠå¤©
    """
    file_path = state.get('file_path', '')
    
    # å¦‚æœæœ‰æ–‡ä»¶è·¯å¾„ï¼Œè¿›è¡Œæ–‡ä»¶å¤„ç†
    if file_path and file_path.strip():
        return "split_text"
    
    # å¦åˆ™ç›´æ¥è·³åˆ°èŠå¤©
    return "chat"

# æ„å»ºå·¥ä½œæµ
# 1.1.11: æ›´æ–°å·¥ä½œæµä»¥æ”¯æŒURLçˆ¬è™«
def create_workflow():
    workflow = StateGraph(GraphState)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("entry", entry_node)  # 1.1.11: æ·»åŠ å…¥å£èŠ‚ç‚¹
    workflow.add_node("process_file", process_file_node)
    workflow.add_node("crawl_url", crawl_url_node)  # 1.1.11: æ·»åŠ çˆ¬è™«èŠ‚ç‚¹
    workflow.add_node("split_text", split_text_node)
    workflow.add_node("embed_and_store", embed_and_store_node)
    workflow.add_node("chat", chat_node)

    # 1.1.11: è®¾ç½®å…¥å£ç‚¹ - æ ¹æ®è¾“å…¥ç±»å‹è·¯ç”±
    workflow.set_entry_point("entry")
    workflow.add_conditional_edges(
        "entry",
        route_entry,
        {
            "process_file": "process_file",
            "crawl_url": "crawl_url",
            "chat": "chat"
        }
    )
    
    # 1.1.10: æ–‡ä»¶å¤„ç†èŠ‚ç‚¹çš„æ¡ä»¶è·¯ç”±
    workflow.add_conditional_edges(
        "process_file",
        should_process_file,
        {
            "split_text": "split_text",
            "chat": "chat"
        }
    )
    
    # 1.1.11: URLçˆ¬è™«èŠ‚ç‚¹ç›´æ¥åˆ°split_text
    workflow.add_edge("crawl_url", "split_text")
    
    # æ–‡æœ¬åˆ‡ç‰‡ååˆ°å‘é‡å­˜å‚¨
    workflow.add_edge("split_text", "embed_and_store")
    
    # å‘é‡å­˜å‚¨ååˆ°èŠå¤©
    workflow.add_edge("embed_and_store", "chat")
    
    # èŠå¤©èŠ‚ç‚¹ç»“æŸ
    workflow.add_edge("chat", END)

    return workflow.compile()

# å¯¼å‡ºåº”ç”¨
app = create_workflow()
