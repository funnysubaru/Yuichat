# 1.2.24: ç§»é™¤ Chainlit ä¾èµ–ï¼Œä½¿ç”¨ç‹¬ç«‹çš„ FastAPI åº”ç”¨
# import chainlit as cl  # 1.2.23: å·²æ›¿ä»£ï¼Œä½¿ç”¨æµå¼ /api/chat/stream
from langchain_core.messages import HumanMessage, AIMessage
from workflow import app as workflow_app, chat_node_stream  # 1.2.24: å¯¼å…¥æµå¼èŠå¤©å‡½æ•°
from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import JSONResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
import os
import uuid
import re  # 1.1.13: å¯¼å…¥ re ç”¨äº collection_name éªŒè¯
import json  # 1.2.24: ç”¨äº SSE æ•°æ®æ ¼å¼åŒ–
import logging  # 1.2.36: æ·»åŠ æ—¥å¿—æ¨¡å—ï¼Œç”¨äºç”Ÿäº§ç¯å¢ƒé”™è¯¯è®°å½•
from dotenv import load_dotenv
from supabase import create_client, Client
from cachetools import TTLCache  # 1.2.39: é«˜é¢‘é—®é¢˜ç¼“å­˜
import asyncio  # 1.2.39: å¹¶è¡Œå¤„ç†

# 1.2.39: ä¼˜å…ˆåŠ è½½ .env.localï¼Œç„¶ååŠ è½½ .envï¼ˆå¦‚æœå­˜åœ¨ï¼‰
load_dotenv('.env.local')  # æœ¬åœ°å¼€å‘é…ç½®ä¼˜å…ˆ
load_dotenv()  # å›é€€åˆ° .env

# 1.2.36: é…ç½®æ—¥å¿—è®°å½•å™¨ï¼Œç¡®ä¿ç”Ÿäº§ç¯å¢ƒä¹Ÿèƒ½è®°å½•é”™è¯¯
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# 1.2.39: é«˜é¢‘é—®é¢˜ç¼“å­˜ - TTL ä¸º 6 å°æ—¶ï¼Œæœ€å¤šç¼“å­˜ 1000 ä¸ªç»“æœ
# ç¼“å­˜é”®æ ¼å¼: f"{kb_token}:{language}"
frequent_questions_cache = TTLCache(maxsize=1000, ttl=21600)

# 1.1.2: åˆå§‹åŒ– Supabase å®¢æˆ·ç«¯ï¼ˆç”¨äºæŸ¥è¯¢ vector_collectionï¼‰
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_SERVICE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")

supabase: Client = None
if SUPABASE_URL and SUPABASE_SERVICE_KEY:
    supabase = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)

# 1.2.24: åˆ›å»ºç‹¬ç«‹çš„ FastAPI åº”ç”¨ï¼Œæ›¿ä»£ Chainlit
# 1.2.36: æ›´æ–°ç‰ˆæœ¬å·
fastapi_app = FastAPI(
    title="YUIChat API",
    description="YUIChat åç«¯ APIï¼Œæä¾›çŸ¥è¯†åº“ç®¡ç†å’ŒèŠå¤©åŠŸèƒ½",
    version="1.2.39"
)

# 1.2.24: æ·»åŠ  CORS ä¸­é—´ä»¶ï¼Œå…è®¸å‰ç«¯è®¿é—®
fastapi_app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒåº”è¯¥é™åˆ¶å…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# 1.2.35: å¥åº·æ£€æŸ¥ç«¯ç‚¹ï¼ˆç”¨äº Cloud Runï¼‰
# 1.2.36: æ›´æ–°ç‰ˆæœ¬å·
@fastapi_app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹ï¼Œç”¨äº Cloud Run å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "service": "YUIChat API",
        "version": "1.2.39"
    }

@fastapi_app.post("/api/process-file")
async def process_file(request: Request):
    """
    1.1.13: ä¾›ç®¡ç†ç«¯è°ƒç”¨çš„ APIï¼Œç”¨äºè§¦å‘æ–‡ä»¶å¤„ç†æµç¨‹
    collection_name åº”è¯¥æ˜¯é¡¹ç›®çš„ vector_collection
    1.1.13: åŠ å¼ºçŸ¥è¯†åº“éš”ç¦»ï¼ŒéªŒè¯ collection_name æ ¼å¼
    """
    try:
        data = await request.json()
        file_path = data.get("file_path")
        collection_name = data.get("collection_name")
        
        if not file_path or not collection_name:
            raise HTTPException(status_code=400, detail="Missing file_path or collection_name")
        
        # 1.1.13: éªŒè¯ collection_name æ ¼å¼ï¼Œç¡®ä¿çŸ¥è¯†åº“éš”ç¦»
        if not isinstance(collection_name, str) or not re.match(r'^[a-zA-Z0-9_-]+$', collection_name):
            raise HTTPException(status_code=400, detail="Invalid collection_name format")
            
        # 1.1.2: è¿è¡Œå·¥ä½œæµè¿›è¡Œæ–‡ä»¶é¢„å¤„ç†
        initial_state = {
            "file_path": file_path,
            "collection_name": collection_name,
            "messages": [], # 1.1.2: ä¼ å…¥ç©ºæ¶ˆæ¯åˆ—è¡¨ï¼Œworkflow.py ä¼šè·³è¿‡ chat èŠ‚ç‚¹
            "docs": [],
            "splits": [],
            "context": "",
            "answer": ""
        }
        
        # æ‰§è¡Œå·¥ä½œæµ
        # 1.2.24: ç§»é™¤ cl.make_asyncï¼Œä½¿ç”¨ asyncio.to_thread å¤„ç†åŒæ­¥è°ƒç”¨
        import asyncio
        final_state = await asyncio.to_thread(workflow_app.invoke, initial_state)
        
        # 1.1.15: ç»Ÿè®¡æ–‡ä»¶å­—æ•°å¹¶æ›´æ–°æ•°æ®åº“
        docs = final_state.get('docs', [])
        if os.getenv("ENV") == "development":
            print(f"ğŸ” DEBUG: process_file - docs count: {len(docs) if docs else 0}")
        
        if docs and supabase:
            try:
                # è®¡ç®—æ‰€æœ‰æ–‡æ¡£çš„æ€»å­—æ•°
                total_word_count = sum(len(doc.page_content) for doc in docs if doc.page_content)
                if os.getenv("ENV") == "development":
                    print(f"ğŸ” DEBUG: process_file - total_word_count: {total_word_count}")
                
                # é€šè¿‡ collection_name æŸ¥è¯¢ knowledge_base_id
                kb_result = supabase.table("knowledge_bases")\
                    .select("id")\
                    .eq("vector_collection", collection_name)\
                    .single()\
                    .execute()
                
                if kb_result.data:
                    kb_id = kb_result.data.get("id")
                    
                    # é€šè¿‡æ–‡ä»¶è·¯å¾„åŒ¹é…æ–‡æ¡£ï¼ˆéœ€è¦ä»file_pathæå–æ–‡ä»¶åï¼‰
                    # file_path æ ¼å¼å¯èƒ½æ˜¯: https://xxx.supabase.co/storage/v1/object/public/knowledge-base-files/{kb_id}/{filename}
                    import urllib.parse
                    parsed_path = urllib.parse.urlparse(file_path)
                    path_parts = parsed_path.path.split('/')
                    # å°è¯•æå–æ–‡ä»¶åï¼ˆæœ€åä¸€éƒ¨åˆ†ï¼‰
                    if path_parts:
                        # å¯èƒ½æ ¼å¼: /storage/v1/object/public/knowledge-base-files/{kb_id}/{filename}
                        filename = path_parts[-1] if path_parts[-1] else (path_parts[-2] if len(path_parts) > 1 else '')
                        
                        # æ›´æ–°æ–‡æ¡£çš„å­—æ•°ï¼ˆé€šè¿‡ kb_id å’Œ status='processing' åŒ¹é…æœ€è¿‘åˆ›å»ºçš„æ–‡æ¡£ï¼‰
                        # å…ˆæŸ¥è¯¢ç¬¦åˆæ¡ä»¶çš„æ–‡æ¡£
                        docs_result = supabase.table("documents")\
                            .select("id, processing_metadata, created_at")\
                            .eq("knowledge_base_id", kb_id)\
                            .eq("status", "processing")\
                            .execute()
                        
                        # åœ¨Pythonä¸­æ’åºï¼Œå–æœ€æ–°çš„æ–‡æ¡£
                        if docs_result.data and len(docs_result.data) > 0:
                            docs_result.data.sort(key=lambda x: x.get("created_at", ""), reverse=True)
                        
                        if docs_result.data and len(docs_result.data) > 0:
                            # æ›´æ–°ç¬¬ä¸€ä¸ªåŒ¹é…çš„æ–‡æ¡£ï¼ˆé€šå¸¸æ˜¯æœ€æ–°åˆ›å»ºçš„ï¼‰
                            doc_data = docs_result.data[0]
                            doc_id = doc_data.get("id")
                            # åˆå¹¶ç°æœ‰çš„ processing_metadataï¼Œé¿å…è¦†ç›–å…¶ä»–å­—æ®µ
                            existing_metadata = doc_data.get("processing_metadata") or {}
                            existing_metadata["word_count"] = total_word_count
                            
                            update_result = supabase.table("documents")\
                                .update({
                                    "processing_metadata": existing_metadata
                                })\
                                .eq("id", doc_id)\
                                .execute()
                            if os.getenv("ENV") == "development":
                                print(f"ğŸ“Š Updated word_count for file (doc_id: {doc_id}): {total_word_count} (kb_id: {kb_id})")
                        else:
                            # 1.1.15: å¦‚æœæ‰¾ä¸åˆ° processing çŠ¶æ€çš„æ–‡æ¡£ï¼Œå°è¯•é€šè¿‡æ–‡ä»¶åå’Œ kb_id åŒ¹é…æœ€è¿‘çš„æ–‡æ¡£
                            # è¿™å¯èƒ½æ˜¯å‰ç«¯å·²ç»æ›´æ–°äº†çŠ¶æ€
                            if filename:
                                # å°è¯•é€šè¿‡æ–‡ä»¶ååŒ¹é…ï¼ˆå»æ‰æ‰©å±•åååŒ¹é…ï¼Œå› ä¸ºä¸Šä¼ çš„æ–‡ä»¶åå¯èƒ½åŒ…å«éšæœºå‰ç¼€ï¼‰
                                file_ext = filename.split('.')[-1] if '.' in filename else ''
                                docs_result2 = supabase.table("documents")\
                                    .select("id, processing_metadata, created_at")\
                                    .eq("knowledge_base_id", kb_id)\
                                    .ilike("file_type", file_ext)\
                                    .execute()
                                
                                if docs_result2.data and len(docs_result2.data) > 0:
                                    docs_result2.data.sort(key=lambda x: x.get("created_at", ""), reverse=True)
                                    doc_data = docs_result2.data[0]
                                    doc_id = doc_data.get("id")
                                    existing_metadata = doc_data.get("processing_metadata") or {}
                                    existing_metadata["word_count"] = total_word_count
                                    
                                    update_result = supabase.table("documents")\
                                        .update({
                                            "processing_metadata": existing_metadata
                                        })\
                                        .eq("id", doc_id)\
                                        .execute()
                                    if os.getenv("ENV") == "development":
                                        print(f"ğŸ“Š Updated word_count for file (by filename match, doc_id: {doc_id}): {total_word_count}")
                            if os.getenv("ENV") == "development":
                                print(f"âš ï¸ No processing document found for kb_id {kb_id}, trying alternative matching...")
                            
                            # 1.1.15: å¤‡ç”¨æ–¹æ¡ˆ - å°è¯•åŒ¹é…æœ€è¿‘çš„æ–‡æ¡£ï¼ˆä¸ä¾èµ–çŠ¶æ€ï¼‰
                            # æŸ¥è¯¢æœ€è¿‘åˆ›å»ºçš„æ–‡æ¡£ï¼ˆæ— è®ºçŠ¶æ€å¦‚ä½•ï¼‰
                            alt_docs_result = supabase.table("documents")\
                                .select("id, processing_metadata, created_at, filename")\
                                .eq("knowledge_base_id", kb_id)\
                                .execute()
                            
                            if alt_docs_result.data and len(alt_docs_result.data) > 0:
                                # æŒ‰åˆ›å»ºæ—¶é—´æ’åº
                                alt_docs_result.data.sort(key=lambda x: x.get("created_at", ""), reverse=True)
                                # å°è¯•åŒ¹é…æ–‡ä»¶ç±»å‹
                                for alt_doc in alt_docs_result.data[:3]:  # åªæ£€æŸ¥æœ€è¿‘3ä¸ªæ–‡æ¡£
                                    alt_doc_filename = alt_doc.get("filename", "")
                                    if file_ext and alt_doc_filename.endswith(f".{file_ext}"):
                                        doc_id = alt_doc.get("id")
                                        existing_metadata = alt_doc.get("processing_metadata") or {}
                                        existing_metadata["word_count"] = total_word_count
                                        
                                        update_result = supabase.table("documents")\
                                            .update({
                                                "processing_metadata": existing_metadata
                                            })\
                                            .eq("id", doc_id)\
                                            .execute()
                                        if os.getenv("ENV") == "development":
                                            print(f"ğŸ“Š Updated word_count (alternative match, doc_id: {doc_id}): {total_word_count}")
                                        break
            except Exception as e:
                import traceback
                if os.getenv("ENV") == "development":
                    print(f"âš ï¸ Failed to update word_count for file: {e}")
                    print(f"   Traceback: {traceback.format_exc()}")
        else:
            if os.getenv("ENV") == "development":
                if not docs:
                    print(f"âš ï¸ process_file: docs is empty")
                if not supabase:
                    print(f"âš ï¸ process_file: supabase client not available")
        
        return JSONResponse(content={
            "status": "success",
            "collection_name": collection_name,
            "message": "File processed and indexed"
        })
    except Exception as e:
        return JSONResponse(status_code=500, content={"status": "error", "message": str(e)})

# 1.1.13: æ·»åŠ URLçˆ¬è™« API ç«¯ç‚¹ï¼ˆåŠ å¼ºçŸ¥è¯†åº“éš”ç¦»ï¼‰
@fastapi_app.post("/api/process-url")
async def process_url(request: Request):
    """
    1.1.13: ä¾›ç®¡ç†ç«¯è°ƒç”¨çš„ APIï¼Œç”¨äºè§¦å‘URLçˆ¬å–å’Œå¤„ç†æµç¨‹
    collection_name åº”è¯¥æ˜¯é¡¹ç›®çš„ vector_collection
    1.1.13: åŠ å¼ºçŸ¥è¯†åº“éš”ç¦»ï¼ŒéªŒè¯ collection_name æ ¼å¼
    """
    from urllib.parse import urlparse
    
    try:
        data = await request.json()
        urls = data.get("urls")
        collection_name = data.get("collection_name")
        
        # éªŒè¯å‚æ•°
        if not urls:
            raise HTTPException(status_code=400, detail="Missing urls")
        
        if not collection_name:
            raise HTTPException(status_code=400, detail="Missing collection_name")
        
        # 1.1.13: éªŒè¯ collection_name æ ¼å¼ï¼Œç¡®ä¿çŸ¥è¯†åº“éš”ç¦»
        if not isinstance(collection_name, str) or not re.match(r'^[a-zA-Z0-9_-]+$', collection_name):
            raise HTTPException(status_code=400, detail="Invalid collection_name format")
        
        # ç¡®ä¿urlsæ˜¯åˆ—è¡¨
        if isinstance(urls, str):
            urls = [urls]
        
        # éªŒè¯URLæ ¼å¼
        valid_urls = []
        for url in urls:
            url = url.strip()
            if not url:
                continue
            try:
                result = urlparse(url)
                if not all([result.scheme, result.netloc]):
                    if os.getenv("ENV") == "development":
                        print(f"âš ï¸ Invalid URL format: {url}")
                    continue
                # åªæ”¯æŒhttpå’Œhttps
                if result.scheme not in ['http', 'https']:
                    if os.getenv("ENV") == "development":
                        print(f"âš ï¸ URL must start with http:// or https://: {url}")
                    continue
                valid_urls.append(url)
            except Exception as e:
                if os.getenv("ENV") == "development":
                    print(f"âš ï¸ URL validation error: {url} - {str(e)}")
                continue
        
        if not valid_urls:
            raise HTTPException(status_code=400, detail="No valid URLs provided. URLs must start with http:// or https://")
        
        if os.getenv("ENV") == "development":
            print(f"ğŸ•·ï¸ Processing {len(valid_urls)} URL(s) for collection: {collection_name}")
        
        # 1.1.11: è¿è¡Œå·¥ä½œæµè¿›è¡ŒURLçˆ¬å–å’Œå¤„ç†
        initial_state = {
            "urls": valid_urls,  # 1.1.11: ä¼ é€’URLåˆ—è¡¨
            "collection_name": collection_name,
            "file_path": "",  # ä¸ä½¿ç”¨æ–‡ä»¶è·¯å¾„
            "messages": [],  # ä¼ å…¥ç©ºæ¶ˆæ¯åˆ—è¡¨ï¼Œworkflow.py ä¼šè·³è¿‡ chat èŠ‚ç‚¹
            "docs": [],
            "splits": [],
            "context": "",
            "answer": ""
        }
        
        # æ‰§è¡Œå·¥ä½œæµ
        # 1.2.24: ç§»é™¤ cl.make_asyncï¼Œä½¿ç”¨ asyncio.to_thread å¤„ç†åŒæ­¥è°ƒç”¨
        import asyncio
        final_state = await asyncio.to_thread(workflow_app.invoke, initial_state)
        
        # 1.1.12: æŒ‰ç…§ chatmax é€»è¾‘ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰è§£æå¤±è´¥çš„æ–‡æ¡£
        docs = final_state.get('docs', [])
        error_docs = []
        valid_docs = []
        
        # 1.1.15: æ„å»º URL -> å­—æ•°çš„æ˜ å°„
        url_word_counts = {}
        
        for doc in docs:
            is_error = (
                'error' in doc.metadata or 
                'çˆ¬å–å¤±è´¥' in doc.page_content or 
                'è§£æå¤±è´¥' in doc.page_content or
                doc.page_content.strip().startswith('çˆ¬å–å¤±è´¥') or
                doc.page_content.strip().startswith('è§£æå¤±è´¥')
            )
            if is_error:
                error_docs.append(doc)
            else:
                valid_docs.append(doc)
                # 1.1.15: ç»Ÿè®¡å­—æ•°ï¼ˆéé”™è¯¯æ–‡æ¡£ï¼‰
                source_url = doc.metadata.get('source', '')
                if source_url:
                    # è®¡ç®—æ–‡æ¡£å†…å®¹çš„å­—ç¬¦æ•°ï¼ˆä¸­æ–‡å­—ç¬¦å’Œå…¶ä»–å­—ç¬¦éƒ½ç®—1ä¸ªï¼‰
                    word_count = len(doc.page_content)
                    url_word_counts[source_url] = word_count
                    if os.getenv("ENV") == "development":
                        print(f"ğŸ” URLå­—æ•°ç»Ÿè®¡: {source_url} -> {word_count} å­—ç¬¦")
                else:
                    if os.getenv("ENV") == "development":
                        print(f"âš ï¸ æ–‡æ¡£ç¼ºå°‘ source URL: metadata={doc.metadata}")
        
        # 1.1.15: æ›´æ–°æ•°æ®åº“ä¸­çš„å­—æ•°ç»Ÿè®¡
        if os.getenv("ENV") == "development":
            print(f"ğŸ” DEBUG: process_url - url_word_counts: {url_word_counts}")
            print(f"ğŸ” DEBUG: process_url - supabase available: {supabase is not None}")
        
        if supabase and url_word_counts:
            try:
                # é€šè¿‡ collection_name æŸ¥è¯¢ knowledge_base_id
                kb_result = supabase.table("knowledge_bases")\
                    .select("id")\
                    .eq("vector_collection", collection_name)\
                    .single()\
                    .execute()
                
                if kb_result.data:
                    kb_id = kb_result.data.get("id")
                    for url, word_count in url_word_counts.items():
                        if os.getenv("ENV") == "development":
                            print(f"ğŸ” Trying to update word_count for URL: {url}, word_count: {word_count}")
                        
                        # å…ˆå°è¯•é€šè¿‡ storage_path åŒ¹é…
                        doc_result = supabase.table("documents")\
                            .select("id, processing_metadata, filename, storage_path")\
                            .eq("knowledge_base_id", kb_id)\
                            .eq("storage_path", url)\
                            .limit(1)\
                            .execute()
                        
                        # å¦‚æœ storage_path åŒ¹é…å¤±è´¥ï¼Œå°è¯•é€šè¿‡ filename åŒ¹é…
                        if not doc_result.data or len(doc_result.data) == 0:
                            if os.getenv("ENV") == "development":
                                print(f"   âš ï¸ Not found by storage_path, trying filename...")
                            doc_result = supabase.table("documents")\
                                .select("id, processing_metadata, filename, storage_path")\
                                .eq("knowledge_base_id", kb_id)\
                                .eq("filename", url)\
                                .limit(1)\
                                .execute()
                        
                        # å¦‚æœè¿˜æ˜¯åŒ¹é…ä¸åˆ°ï¼Œå°è¯•é€šè¿‡ URL çš„éƒ¨åˆ†åŒ¹é…ï¼ˆå¤„ç† URL å‚æ•°å˜åŒ–çš„æƒ…å†µï¼‰
                        if not doc_result.data or len(doc_result.data) == 0:
                            if os.getenv("ENV") == "development":
                                print(f"   âš ï¸ Not found by filename, trying partial match...")
                            # æå–åŸºç¡€URLï¼ˆå»æ‰å‚æ•°å’Œé”šç‚¹ï¼‰
                            from urllib.parse import urlparse
                            parsed = urlparse(url)
                            base_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
                            # å°è¯•åŒ¹é…åŒ…å«åŸºç¡€URLçš„æ–‡æ¡£
                            all_docs = supabase.table("documents")\
                                .select("id, processing_metadata, filename, storage_path, created_at")\
                                .eq("knowledge_base_id", kb_id)\
                                .eq("file_type", "url")\
                                .execute()
                            
                            if all_docs.data:
                                # æŸ¥æ‰¾æœ€åŒ¹é…çš„æ–‡æ¡£ï¼ˆæŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼ŒåŒ¹é…åŸºç¡€URLï¼‰
                                matching_docs = [d for d in all_docs.data 
                                               if (d.get("filename") or "").startswith(base_url) or 
                                                  (d.get("storage_path") or "").startswith(base_url)]
                                if matching_docs:
                                    matching_docs.sort(key=lambda x: x.get("created_at", ""), reverse=True)
                                    doc_result.data = [matching_docs[0]]
                        
                        if doc_result.data and len(doc_result.data) > 0:
                            doc_id = doc_result.data[0].get("id")
                            # åˆå¹¶ç°æœ‰çš„ processing_metadataï¼Œé¿å…è¦†ç›–å…¶ä»–å­—æ®µ
                            existing_metadata = doc_result.data[0].get("processing_metadata") or {}
                            existing_metadata["word_count"] = word_count
                            
                            update_result = supabase.table("documents")\
                                .update({
                                    "processing_metadata": existing_metadata
                                })\
                                .eq("id", doc_id)\
                                .execute()
                            if os.getenv("ENV") == "development":
                                print(f"âœ… Updated word_count for URL {url}: {word_count} (doc_id: {doc_id}, kb_id: {kb_id})")
                        else:
                            if os.getenv("ENV") == "development":
                                print(f"âŒ Document not found for URL {url} in kb_id {kb_id}")
                                # åˆ—å‡ºæ‰€æœ‰URLæ–‡æ¡£ç”¨äºè°ƒè¯•
                                all_url_docs = supabase.table("documents")\
                                    .select("id, filename, storage_path, created_at")\
                                    .eq("knowledge_base_id", kb_id)\
                                    .eq("file_type", "url")\
                                    .limit(5)\
                                    .execute()
                                if all_url_docs.data:
                                    print(f"   Available URL docs: {[(d.get('id'), d.get('filename'), d.get('storage_path')) for d in all_url_docs.data]}")
                else:
                    if os.getenv("ENV") == "development":
                        print(f"âš ï¸ Knowledge base not found for collection_name: {collection_name}")
            except Exception as e:
                import traceback
                if os.getenv("ENV") == "development":
                    print(f"âš ï¸ Failed to update word_count for URLs: {e}")
                    print(f"   Traceback: {traceback.format_exc()}")
        
        # 1.1.12: å¦‚æœæœ‰é”™è¯¯æ–‡æ¡£ï¼Œè¿”å›éƒ¨åˆ†æˆåŠŸæˆ–å¤±è´¥çŠ¶æ€
        if error_docs:
            if valid_docs:
                # éƒ¨åˆ†æˆåŠŸ
                return JSONResponse(content={
                    "status": "partial_success",
                    "collection_name": collection_name,
                    "urls_processed": len(valid_docs),
                    "urls_failed": len(error_docs),
                    "message": f"æˆåŠŸå¤„ç† {len(valid_docs)} ä¸ªURLï¼Œå¤±è´¥ {len(error_docs)} ä¸ªURL",
                    "errors": [doc.metadata.get('error', 'è§£æå¤±è´¥') for doc in error_docs]
                })
            else:
                # å…¨éƒ¨å¤±è´¥
                return JSONResponse(
                    status_code=400,
                    content={
                        "status": "error",
                        "collection_name": collection_name,
                        "urls_processed": 0,
                        "urls_failed": len(error_docs),
                        "message": f"æ‰€æœ‰URLè§£æå¤±è´¥",
                        "errors": [doc.metadata.get('error', 'è§£æå¤±è´¥') for doc in error_docs]
                    }
                )
        
        # å…¨éƒ¨æˆåŠŸ
        return JSONResponse(content={
            "status": "success",
            "collection_name": collection_name,
            "urls_processed": len(valid_docs),
            "message": f"Successfully processed {len(valid_docs)} URL(s) and indexed"
        })
    except HTTPException:
        raise
    except Exception as e:
        error_msg = str(e)
        if os.getenv("ENV") == "development":
            print(f"âŒ URL processing error: {error_msg}")
        
        # æ ¹æ®é”™è¯¯ç±»å‹è¿”å›ä¸åŒçš„çŠ¶æ€ç 
        status_code = 500
        if "timeout" in error_msg.lower() or "è¶…æ—¶" in error_msg:
            status_code = 504
        elif "æ— æ³•è®¿é—®" in error_msg or "æ— æ³•è¿æ¥" in error_msg or "connection" in error_msg.lower():
            status_code = 502
        
        return JSONResponse(
            status_code=status_code,
            content={"status": "error", "message": error_msg}
        )

# 1.1.13: æ·»åŠ èŠå¤© API ç«¯ç‚¹ï¼ˆåŠ å¼ºçŸ¥è¯†åº“éš”ç¦»ï¼‰
@fastapi_app.post("/api/chat")
async def chat(request: Request):
    """
    1.1.13: ä¾›ç®¡ç†ç«¯è°ƒç”¨çš„èŠå¤© APIï¼Œç”¨äºæµ‹è¯•å¯¹è¯åŠŸèƒ½
    æ”¯æŒåŸºäºçŸ¥è¯†åº“æ–‡æ¡£çš„é—®ç­”
    1.1.13: åŠ å¼ºçŸ¥è¯†åº“éš”ç¦»ï¼Œç¡®ä¿ç”¨æˆ·åªèƒ½è®¿é—®æœ‰æƒé™çš„çŸ¥è¯†åº“
    """
    try:
        data = await request.json()
        query = data.get("query")
        kb_token = data.get("kb_id")  # share_token æˆ– vector_collection
        conversation_history = data.get("conversation_history", [])
        user_id = data.get("user_id")  # 1.1.13: å¯é€‰ï¼Œç”¨äºæƒé™éªŒè¯
        
        if not query:
            raise HTTPException(status_code=400, detail="Missing query")
        
        if not kb_token:
            raise HTTPException(status_code=400, detail="Missing kb_id")
        
        # 1.1.13: ä» Supabase è·å– vector_collection å¹¶éªŒè¯æƒé™
        collection_name = None
        kb_data = None
        
        if supabase:
            try:
                # 1.1.13: å°è¯•é€šè¿‡ share_token æŸ¥è¯¢çŸ¥è¯†åº“ä¿¡æ¯
                result = supabase.table("knowledge_bases")\
                    .select("vector_collection, user_id, id")\
                    .eq("share_token", kb_token)\
                    .single()\
                    .execute()
                
                if result.data:
                    kb_data = result.data
                    vector_collection = kb_data.get("vector_collection")
                    
                    # 1.1.15: éªŒè¯ vector_collection æ˜¯å¦æœ‰æ•ˆï¼ˆä¸ä¸º None ä¸”ä¸ä¸ºç©ºå­—ç¬¦ä¸²ï¼‰
                    if vector_collection and isinstance(vector_collection, str) and vector_collection.strip():
                        collection_name = vector_collection.strip()
                    else:
                        # 1.1.15: å¦‚æœ vector_collection ä¸ºç©ºï¼Œä½¿ç”¨ kb_token ä½œä¸ºåå¤‡
                        if os.getenv("ENV") == "development":
                            print(f"âš ï¸ vector_collection is empty for kb_id {kb_token}, using kb_token as collection_name")
                        collection_name = kb_token
                    
                    # 1.1.13: æƒé™éªŒè¯ - å¦‚æœæä¾›äº† user_idï¼ŒéªŒè¯ç”¨æˆ·æ˜¯å¦æœ‰æƒé™è®¿é—®
                    if user_id:
                        kb_user_id = kb_data.get("user_id")
                        if kb_user_id != user_id:
                            # 1.1.13: ç”¨æˆ·ä¸æ˜¯çŸ¥è¯†åº“æ‰€æœ‰è€…ï¼Œä½†å¯ä»¥é€šè¿‡ share_token è®¿é—®ï¼ˆå…¬å¼€åˆ†äº«ï¼‰
                            if os.getenv("ENV") == "development":
                                print(f"âš ï¸ User {user_id} accessing shared knowledge base {kb_data.get('id')} via share_token")
                    else:
                        # 1.1.13: æœªæä¾› user_idï¼Œå…è®¸é€šè¿‡ share_token è®¿é—®ï¼ˆå…¬å¼€åˆ†äº«æ¨¡å¼ï¼‰
                        if os.getenv("ENV") == "development":
                            print(f"âš ï¸ Accessing knowledge base via share_token without user_id (public share mode)")
                else:
                    # 1.1.13: å¦‚æœé€šè¿‡ share_token æŸ¥è¯¢å¤±è´¥ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨ kb_token ä½œä¸º vector_collection
                    # ä½†éœ€è¦éªŒè¯è¯¥ collection æ˜¯å¦å±äºæŸä¸ªçŸ¥è¯†åº“ï¼ˆå¯é€‰ï¼Œç”¨äºå‘åå…¼å®¹ï¼‰
                    if os.getenv("ENV") == "development":
                        print(f"âš ï¸ Knowledge base not found by share_token, using kb_token as collection_name: {kb_token}")
                    collection_name = kb_token
            except Exception as e:
                # 1.1.13: å¦‚æœæŸ¥è¯¢å¤±è´¥ï¼Œå¯èƒ½ kb_token æœ¬èº«å°±æ˜¯ vector_collectionï¼ˆå‘åå…¼å®¹ï¼‰
                if os.getenv("ENV") == "development":
                    print(f"âš ï¸ Failed to fetch knowledge base info, using kb_token as collection_name: {e}")
                collection_name = kb_token
        else:
            # 1.1.15: å¦‚æœæ²¡æœ‰ Supabase è¿æ¥ï¼Œç›´æ¥ä½¿ç”¨ kb_token
            collection_name = kb_token
        
        # 1.1.15: ç¡®ä¿ collection_name ä¸ä¸ºç©ºä¸”æ˜¯æœ‰æ•ˆå­—ç¬¦ä¸²
        if not collection_name or not isinstance(collection_name, str) or not collection_name.strip():
            raise HTTPException(status_code=404, detail="Knowledge base not found or invalid kb_id: collection_name is empty")
        
        # 1.1.10: æ„å»ºæ¶ˆæ¯å†å²
        from langchain_core.messages import HumanMessage, AIMessage
        messages = []
        for msg in conversation_history:
            if msg.get("role") == "user":
                messages.append(HumanMessage(content=msg.get("content", "")))
            elif msg.get("role") == "assistant":
                messages.append(AIMessage(content=msg.get("content", "")))
        
        # æ·»åŠ å½“å‰ç”¨æˆ·é—®é¢˜
        messages.append(HumanMessage(content=query))
        
        # 1.1.10: å‡†å¤‡çŠ¶æ€å¹¶è°ƒç”¨å·¥ä½œæµ
        state = {
            "messages": messages,
            "collection_name": collection_name,
            "file_path": "",  # ä¸éœ€è¦å¤„ç†æ–‡ä»¶
            "docs": [],
            "splits": [],
            "context": "",
            "answer": ""
        }
        
        # æ‰§è¡Œå·¥ä½œæµï¼ˆåªæ‰§è¡Œ chat èŠ‚ç‚¹ï¼‰
        # 1.2.24: ç§»é™¤ cl.make_asyncï¼Œç›´æ¥ä½¿ç”¨å¼‚æ­¥è°ƒç”¨
        result = workflow_app.invoke(state)
        
        answer = result.get("answer", "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚")
        context = result.get("context", "")
        
        return JSONResponse(content={
            "status": "success",
            "answer": answer,
            "context": context,
            "collection_name": collection_name
        })
    except Exception as e:
        if os.getenv("ENV") == "development":  # 1.1.10: ä»…å¼€å‘ç¯å¢ƒè¾“å‡ºè¯¦ç»†é”™è¯¯
            print(f"Chat API error: {e}")
        return JSONResponse(status_code=500, content={
            "status": "error", 
            "message": str(e)
        })

# 1.2.24: æ–°å¢æµå¼èŠå¤©ç«¯ç‚¹ï¼Œæ”¯æŒ SSE å®æ—¶æ˜¾ç¤º
@fastapi_app.post("/api/chat/stream")
async def chat_stream(request: Request):
    """
    1.2.24: æµå¼èŠå¤© APIï¼Œæ”¯æŒå®æ—¶æ˜¾ç¤ºç­”æ¡ˆ
    ä½¿ç”¨ FastAPI åŸç”Ÿ StreamingResponse å®ç° SSE
    """
    try:
        data = await request.json()
        query = data.get("query")
        kb_token = data.get("kb_id")  # share_token æˆ– vector_collection
        conversation_history = data.get("conversation_history", [])
        user_id = data.get("user_id")  # å¯é€‰ï¼Œç”¨äºæƒé™éªŒè¯
        
        if not query:
            raise HTTPException(status_code=400, detail="Missing query")
        
        if not kb_token:
            raise HTTPException(status_code=400, detail="Missing kb_id")
        
        # 1.2.24: ä» Supabase è·å– vector_collection å¹¶éªŒè¯æƒé™ï¼ˆä¸ /api/chat ç›¸åŒé€»è¾‘ï¼‰
        collection_name = None
        kb_data = None
        
        if supabase:
            try:
                result = supabase.table("knowledge_bases")\
                    .select("vector_collection, user_id, id")\
                    .eq("share_token", kb_token)\
                    .single()\
                    .execute()
                
                if result.data:
                    kb_data = result.data
                    vector_collection = kb_data.get("vector_collection")
                    
                    if vector_collection and isinstance(vector_collection, str) and vector_collection.strip():
                        collection_name = vector_collection.strip()
                    else:
                        if os.getenv("ENV") == "development":
                            print(f"âš ï¸ vector_collection is empty for kb_id {kb_token}, using kb_token as collection_name")
                        collection_name = kb_token
                    
                    # æƒé™éªŒè¯
                    if user_id:
                        kb_user_id = kb_data.get("user_id")
                        if kb_user_id != user_id:
                            if os.getenv("ENV") == "development":
                                print(f"âš ï¸ User {user_id} accessing shared knowledge base {kb_data.get('id')} via share_token")
                    else:
                        if os.getenv("ENV") == "development":
                            print(f"âš ï¸ Accessing knowledge base via share_token without user_id (public share mode)")
                else:
                    if os.getenv("ENV") == "development":
                        print(f"âš ï¸ Knowledge base not found by share_token, using kb_token as collection_name: {kb_token}")
                    collection_name = kb_token
            except Exception as e:
                if os.getenv("ENV") == "development":
                    print(f"âš ï¸ Failed to fetch knowledge base info, using kb_token as collection_name: {e}")
                collection_name = kb_token
        else:
            collection_name = kb_token
        
        # ç¡®ä¿ collection_name æœ‰æ•ˆ
        if not collection_name or not isinstance(collection_name, str) or not collection_name.strip():
            raise HTTPException(status_code=404, detail="Knowledge base not found or invalid kb_id: collection_name is empty")
        
        # æ„å»ºæ¶ˆæ¯å†å²
        messages = []
        for msg in conversation_history:
            if msg.get("role") == "user":
                messages.append(HumanMessage(content=msg.get("content", "")))
            elif msg.get("role") == "assistant":
                messages.append(AIMessage(content=msg.get("content", "")))
        
        # æ·»åŠ å½“å‰ç”¨æˆ·é—®é¢˜
        messages.append(HumanMessage(content=query))
        
        # å‡†å¤‡çŠ¶æ€
        state = {
            "messages": messages,
            "collection_name": collection_name,
            "file_path": "",
            "docs": [],
            "splits": [],
            "context": "",
            "answer": ""
        }
        
        # 1.2.24: å®šä¹‰ SSE æµå¼ç”Ÿæˆå™¨
        async def generate():
            try:
                # è°ƒç”¨æµå¼èŠå¤©å‡½æ•°
                async for chunk_data in chat_node_stream(state):
                    if chunk_data.get("done"):
                        # å‘é€å®Œæˆæ¶ˆæ¯ï¼ŒåŒ…å«å®Œæ•´ç­”æ¡ˆå’Œä¸Šä¸‹æ–‡
                        yield f"data: {json.dumps({'answer': chunk_data.get('answer', ''), 'context': chunk_data.get('context', ''), 'done': True})}\n\n"
                    else:
                        # å‘é€æ•°æ®å—
                        chunk_text = chunk_data.get("chunk", "")
                        if chunk_text:
                            yield f"data: {json.dumps({'chunk': chunk_text})}\n\n"
                
                # å‘é€ç»“æŸæ ‡è®°
                yield "data: [DONE]\n\n"
            except Exception as e:
                # é”™è¯¯å¤„ç†
                error_data = json.dumps({'error': str(e), 'done': True})
                yield f"data: {error_data}\n\n"
                if os.getenv("ENV") == "development":
                    print(f"âŒ Stream chat error: {e}")
        
        # 1.2.24: è¿”å›æµå¼å“åº”
        return StreamingResponse(
            generate(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",  # ç¦ç”¨ Nginx buffering
            }
        )
    except HTTPException:
        raise
    except Exception as e:
        if os.getenv("ENV") == "development":
            print(f"âŒ Stream chat setup error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# 1.2.0: è·å–èŠå¤©é…ç½® API
# 1.2.21: æ”¹ä¸ºPOSTè¯·æ±‚ï¼Œé¿å…Chainlitæ‹¦æˆªGETè¯·æ±‚
@fastapi_app.post("/api/chat-config")
async def get_chat_config(request: Request):
    """
    è·å–é¡¹ç›®çš„èŠå¤©é…ç½®ï¼ˆæ¬¢è¿è¯­ã€æ¨èé—®é¢˜ç­‰ï¼‰
    1.2.21: æ”¹ä¸ºPOSTè¯·æ±‚ï¼Œé¿å…Chainlitæ‹¦æˆªGETè¯·æ±‚
    """
    try:
        # 1.2.21: æ”¯æŒä»bodyæˆ–queryå‚æ•°è·å–å‚æ•°
        body = await request.json() if request.headers.get("content-type") == "application/json" else {}
        kb_token = body.get("kb_id") or request.query_params.get("kb_id")
        language = body.get("language") or request.query_params.get("language", "zh")
        
        if not kb_token:
            return JSONResponse(status_code=400, content={
                "status": "error",
                "message": "Missing kb_id"
            })
        
        # ä» Supabase è·å– chat_config å’Œé¡¹ç›®åç§°
        chat_config = None
        project_name = "YUIChat"  # é»˜è®¤é¡¹ç›®åç§°
        if supabase:
            try:
                result = supabase.table("knowledge_bases")\
                    .select("chat_config, name")\
                    .eq("share_token", kb_token)\
                    .single()\
                    .execute()
                
                if result.data:
                    chat_config = result.data.get("chat_config", {})
                    project_name = result.data.get("name", "YUIChat")
            except Exception as e:
                if os.getenv("ENV") == "development":
                    print(f"âš ï¸ Failed to fetch chat_config: {e}")
                chat_config = {}
        else:
            chat_config = {}
        
        # æ ‡å‡†åŒ–è¯­è¨€ä»£ç 
        if language not in ["zh", "en", "ja"]:
            language = "zh"
        
        # æå–å¯¹åº”è¯­è¨€çš„é…ç½®
        welcome_message = ""
        recommended_questions = []
        
        if chat_config:
            welcome_message = chat_config.get("welcome_message", {}).get(language, "")
            recommended_questions = chat_config.get("recommended_questions", {}).get(language, [])
        
        return JSONResponse(content={
            "status": "success",
            "project_name": project_name,  # 1.2.13: è¿”å›é¡¹ç›®åç§°ç”¨äºæ›¿æ¢ Chainlit logo
            "avatar_url": chat_config.get("avatar_url", "") if chat_config else "",
            "welcome_message": welcome_message,
            "recommended_questions": recommended_questions[:3] if recommended_questions else []
        })
    except Exception as e:
        if os.getenv("ENV") == "development":
            print(f"Chat config API error: {e}")
        return JSONResponse(status_code=500, content={
            "status": "error",
            "message": str(e)
        })

# 1.2.0: è·å–é«˜é¢‘é—®é¢˜ API
# 1.2.11: åŸºäºæ–‡æ¡£ç”Ÿæˆå¸¸è§é—®é¢˜ï¼Œç¡®ä¿æ¯ä¸ªé—®é¢˜éƒ½æœ‰å›å¤
# 1.2.12: æ”¹ä¸ºPOSTè¯·æ±‚ï¼Œé¿å…Chainlitæ‹¦æˆªGETè¯·æ±‚
# 1.2.36: æ”¹è¿›é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•ï¼Œç¡®ä¿ç”Ÿäº§ç¯å¢ƒä¹Ÿèƒ½è¿½è¸ªé—®é¢˜
# 1.2.39: æ€§èƒ½ä¼˜åŒ– - æ·»åŠ ç¼“å­˜ã€å¹¶è¡Œå¤„ç†ã€ä½¿ç”¨æ›´å¿«æ¨¡å‹
@fastapi_app.post("/api/frequent-questions")
async def get_frequent_questions(request: Request):
    """
    è·å–é«˜é¢‘é—®é¢˜ï¼ˆå½“é¡¹ç›®æœªé…ç½®æ¨èé—®é¢˜æ—¶ä½¿ç”¨ï¼‰
    1.2.11: åŸºäºä¸Šä¼ çš„æ–‡æ¡£ç”Ÿæˆé—®é¢˜ï¼Œå¹¶ç¡®ä¿æ¯ä¸ªé—®é¢˜éƒ½æœ‰å›å¤
    1.2.12: æ”¹ä¸ºPOSTè¯·æ±‚ï¼Œé¿å…Chainlitæ‹¦æˆªGETè¯·æ±‚
    1.2.36: æ”¹è¿›é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•ï¼Œç¡®ä¿ç”Ÿäº§ç¯å¢ƒä¹Ÿèƒ½è¿½è¸ªé—®é¢˜
    1.2.39: æ€§èƒ½ä¼˜åŒ– - ç¼“å­˜(6h)ã€å¹¶è¡Œembeddingã€gpt-4o-miniã€å¹¶è¡ŒéªŒè¯
    """
    # 1.2.36: ä½¿ç”¨ logger è®°å½• API è°ƒç”¨ï¼ˆç”Ÿäº§ç¯å¢ƒä¹Ÿä¼šè®°å½•ï¼‰
    logger.info("API called: /api/frequent-questions")
    if os.getenv("ENV") == "development":
        print(f"âœ… DEBUG: /api/frequent-questions API called")
    try:
        # 1.2.12: æ”¯æŒä»bodyæˆ–queryå‚æ•°è·å–å‚æ•°
        try:
            data = await request.json()
            kb_token = data.get("kb_id") or request.query_params.get("kb_id")
            language = data.get("language") or request.query_params.get("language", "zh")
        except:
            # å¦‚æœæ²¡æœ‰bodyï¼Œä»queryå‚æ•°è·å–
            kb_token = request.query_params.get("kb_id")
            language = request.query_params.get("language", "zh")
        
        if not kb_token:
            logger.warning("Missing kb_id parameter in frequent-questions request")
            return JSONResponse(status_code=400, content={
                "status": "error",
                "message": "Missing kb_id"
            })
        
        # æ ‡å‡†åŒ–è¯­è¨€ä»£ç 
        if language not in ["zh", "en", "ja"]:
            language = "zh"
        
        # 1.2.39: æ£€æŸ¥ç¼“å­˜
        cache_key = f"{kb_token}:{language}"
        if cache_key in frequent_questions_cache:
            cached_questions = frequent_questions_cache[cache_key]
            logger.info(f"Cache hit for kb_token: {kb_token}, language: {language}")
            if os.getenv("ENV") == "development":
                print(f"âœ… DEBUG: Cache hit, returning cached questions")
            return JSONResponse(
                status_code=200,
                content={
                    "status": "success",
                    "questions": cached_questions,
                    "cached": True  # 1.2.39: æ ‡è®°ä¸ºç¼“å­˜ç»“æœ
                },
                headers={"Content-Type": "application/json"}
            )
        
        logger.info(f"Processing frequent questions request for kb_token: {kb_token}, language: {language}")
        
        # 1.2.11: ä» Supabase è·å– vector_collection
        # 1.2.36: æ”¹è¿›é”™è¯¯æ—¥å¿—è®°å½•
        # 1.2.40: ç®€åŒ–æŸ¥è¯¢é€»è¾‘ï¼Œæ”¯æŒé€šè¿‡ id æˆ– share_token æŸ¥è¯¢
        collection_name = None
        if supabase:
            try:
                # 1.2.39: æ”¯æŒé€šè¿‡ id æˆ– share_token æŸ¥è¯¢ï¼ˆå…¼å®¹å‰ç«¯ä¼ é€’ project idï¼‰
                # å…ˆå°è¯•é€šè¿‡ share_token æŸ¥è¯¢
                result = supabase.table("knowledge_bases")\
                    .select("vector_collection")\
                    .eq("share_token", kb_token)\
                    .limit(1)\
                    .execute()
                
                # å¦‚æœé€šè¿‡ share_token æœªæ‰¾åˆ°ï¼Œå°è¯•é€šè¿‡ id æŸ¥è¯¢
                if not result.data or len(result.data) == 0:
                    result = supabase.table("knowledge_bases")\
                        .select("vector_collection")\
                        .eq("id", kb_token)\
                        .limit(1)\
                        .execute()
                
                if result.data and len(result.data) > 0:
                    data = result.data[0]  # è·å–ç¬¬ä¸€æ¡è®°å½•
                    vector_collection = data.get("vector_collection")
                    if os.getenv("ENV") == "development":
                        print(f"ğŸ” DEBUG: Found vector_collection: {vector_collection}")
                    if vector_collection and isinstance(vector_collection, str) and vector_collection.strip():
                        collection_name = vector_collection.strip()
                        logger.info(f"Found collection_name: {collection_name} for kb_token: {kb_token}")
                        if os.getenv("ENV") == "development":
                            print(f"âœ… DEBUG: Using collection_name: {collection_name}")
                    else:
                        logger.warning(f"vector_collection is empty or invalid for kb_token: {kb_token}, value: {vector_collection}")
                        if os.getenv("ENV") == "development":
                            print(f"âš ï¸ DEBUG: vector_collection is empty or invalid: {vector_collection}")
                else:
                    logger.warning(f"No data found in knowledge_bases for kb_token: {kb_token}")
                    if os.getenv("ENV") == "development":
                        print(f"âš ï¸ DEBUG: No data found for kb_token: {kb_token}")
            except Exception as e:
                logger.error(f"Failed to fetch vector_collection from Supabase for kb_token {kb_token}: {str(e)}", exc_info=True)
                if os.getenv("ENV") == "development":
                    print(f"âš ï¸ Failed to fetch vector_collection: {e}")
        else:
            logger.error("Supabase client not initialized - missing SUPABASE_URL or SUPABASE_SERVICE_ROLE_KEY")
            if os.getenv("ENV") == "development":
                print(f"âš ï¸ DEBUG: Supabase client not initialized")
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ° collection_nameï¼Œä½¿ç”¨é»˜è®¤é—®é¢˜
        if not collection_name:
            logger.warning(f"collection_name is None for kb_token: {kb_token}, returning default questions")
            if os.getenv("ENV") == "development":
                print(f"âš ï¸ DEBUG: collection_name is None, using default questions")
            default_questions = {
                "zh": [
                    "æ‚¨èƒ½ä»‹ç»ä¸€ä¸‹è¿™ä¸ªé¡¹ç›®å—ï¼Ÿ",
                    "æœ‰å“ªäº›å¸¸è§é—®é¢˜ï¼Ÿ",
                    "å¦‚ä½•ä½¿ç”¨è¿™ä¸ªç³»ç»Ÿï¼Ÿ"
                ],
                "en": [
                    "Can you introduce this project?",
                    "What are the common questions?",
                    "How to use this system?"
                ],
                "ja": [
                    "ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ã¤ã„ã¦ç´¹ä»‹ã—ã¦ã„ãŸã ã‘ã¾ã™ã‹ï¼Ÿ",
                    "ã‚ˆãã‚ã‚‹è³ªå•ã¯ä½•ã§ã™ã‹ï¼Ÿ",
                    "ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã®ä½¿ã„æ–¹ã¯ï¼Ÿ"
                ]
            }
            return JSONResponse(
                status_code=200,
                content={
                    "status": "success",
                    "questions": default_questions.get(language, default_questions["zh"])
                },
                headers={"Content-Type": "application/json"}
            )
        
        # 1.2.11: ä»å‘é‡åº“æ£€ç´¢æ–‡æ¡£ç‰‡æ®µ
        from langchain_openai import OpenAIEmbeddings, ChatOpenAI
        from langchain_community.vectorstores import Chroma
        from langchain_core.prompts import ChatPromptTemplate
        import vecs
        
        # å°è¯•ä»å‘é‡åº“æ£€ç´¢æ–‡æ¡£
        sample_docs = []
        try:
            # 1.2.11: ä½¿ç”¨é€šç”¨æŸ¥è¯¢è¯æ£€ç´¢æ–‡æ¡£
            query_texts = {
                "zh": ["ä»‹ç»", "è¯´æ˜", "æ¦‚è¿°", "åŠŸèƒ½", "ä½¿ç”¨æ–¹æ³•"],
                "en": ["introduction", "overview", "features", "how to use", "description"],
                "ja": ["ç´¹ä»‹", "æ¦‚è¦", "æ©Ÿèƒ½", "ä½¿ã„æ–¹", "èª¬æ˜"]
            }
            
            query_words = query_texts.get(language, query_texts["zh"])
            
            # æ ¹æ®é…ç½®é€‰æ‹©å‘é‡æ•°æ®åº“
            USE_PGVECTOR = os.getenv("USE_PGVECTOR", "false").lower() == "true"
            DATABASE_URL = os.getenv("PGVECTOR_DATABASE_URL") or os.getenv("DATABASE_URL")
            
            if USE_PGVECTOR and DATABASE_URL:
                # ä½¿ç”¨ pgvector
                try:
                    logger.info(f"Attempting to query pgvector collection: {collection_name}")
                    vx = vecs.create_client(DATABASE_URL)
                    collection = vx.get_collection(name=collection_name)
                    embeddings_model = OpenAIEmbeddings()
                    
                    # 1.2.39: å¹¶è¡Œç”Ÿæˆæ‰€æœ‰æŸ¥è¯¢è¯çš„ embeddings
                    query_words_to_use = query_words[:3]  # åªä½¿ç”¨å‰3ä¸ªæŸ¥è¯¢è¯
                    if os.getenv("ENV") == "development":
                        print(f"ğŸ” DEBUG: Parallel embedding {len(query_words_to_use)} query words")
                    
                    # å¹¶è¡Œè°ƒç”¨ embed_query
                    query_vectors = await asyncio.gather(*[
                        asyncio.to_thread(embeddings_model.embed_query, word)
                        for word in query_words_to_use
                    ])
                    
                    # å¯¹æ¯ä¸ªæŸ¥è¯¢è¯æ£€ç´¢æ–‡æ¡£
                    all_results = []
                    for idx, query_word in enumerate(query_words_to_use):
                        query_vector = query_vectors[idx]
                        # 1.2.39: vecs 0.4.5 API: data æ›¿ä»£ query_vector
                        results = collection.query(
                            data=query_vector,
                            limit=2,
                            include_value=False,
                            include_metadata=True
                        )
                        logger.info(f"pgvector query for '{query_word}' returned {len(results)} results")
                        for record in results:
                            # 1.2.39: vecs è¿”å›æ ¼å¼: (id, metadata)
                            if len(record) > 1 and record[1]:  # ç¡®ä¿æœ‰metadata
                                text = record[1].get("text", "")
                                metadata = record[1].get("metadata", {}) if isinstance(record[1], dict) else {}
                                is_error = (
                                    'error' in metadata or 
                                    'çˆ¬å–å¤±è´¥' in text or 
                                    'è§£æå¤±è´¥' in text or
                                    text.strip().startswith('çˆ¬å–å¤±è´¥') or
                                    text.strip().startswith('è§£æå¤±è´¥')
                                )
                                if not is_error and text.strip() and len(text.strip()) > 50:
                                    all_results.append(text)
                    
                    # å»é‡å¹¶é™åˆ¶æ•°é‡
                    sample_docs = list(dict.fromkeys(all_results))[:10]  # æœ€å¤š10ä¸ªæ–‡æ¡£ç‰‡æ®µ
                    logger.info(f"Retrieved {len(sample_docs)} valid documents from pgvector for collection: {collection_name}")
                    if os.getenv("ENV") == "development":
                        print(f"âœ… DEBUG: Retrieved {len(sample_docs)} documents from pgvector (parallel)")
                except Exception as e:
                    logger.error(f"pgvector query failed for collection {collection_name}: {str(e)}", exc_info=True)
                    logger.warning(f"Falling back to Chroma for collection: {collection_name}")
                    if os.getenv("ENV") == "development":
                        print(f"âš ï¸ pgvector query error: {e}, falling back to Chroma")
                    # å›é€€åˆ° Chromaï¼ˆæ³¨æ„ï¼šCloud Run ç¯å¢ƒä¸­å¯èƒ½æ— æ³•è®¿é—®æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿï¼‰
                    logger.warning(f"Attempting Chroma fallback for collection: {collection_name} (may fail in Cloud Run)")
                    try:
                        vectorstore = Chroma(
                            persist_directory=f"./chroma_db/{collection_name}",
                            embedding_function=OpenAIEmbeddings()
                        )
                        retriever = vectorstore.as_retriever(search_kwargs={"k": 10})
                        for query_word in query_words[:3]:
                            docs = retriever.invoke(query_word)
                            logger.info(f"Chroma query for '{query_word}' returned {len(docs)} docs")
                            for doc in docs:
                                if doc.page_content and len(doc.page_content.strip()) > 50:
                                    is_error = (
                                        'error' in doc.metadata or 
                                        'çˆ¬å–å¤±è´¥' in doc.page_content or 
                                        'è§£æå¤±è´¥' in doc.page_content
                                    )
                                    if not is_error:
                                        sample_docs.append(doc.page_content)
                        sample_docs = list(dict.fromkeys(sample_docs))[:10]
                        logger.info(f"Retrieved {len(sample_docs)} documents from Chroma (fallback) for collection: {collection_name}")
                        if os.getenv("ENV") == "development":
                            print(f"âœ… DEBUG: Retrieved {len(sample_docs)} documents from Chroma (fallback)")
                    except Exception as chroma_error:
                        logger.error(f"Chroma fallback also failed for collection {collection_name}: {str(chroma_error)}", exc_info=True)
                        raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©å¤–å±‚å¤„ç†
            else:
                # ä½¿ç”¨ Chroma
                if os.getenv("ENV") == "development":
                    print(f"ğŸ” DEBUG: Using Chroma, collection_name: {collection_name}")
                try:
                    vectorstore = Chroma(
                        persist_directory=f"./chroma_db/{collection_name}",
                        embedding_function=OpenAIEmbeddings()
                    )
                    retriever = vectorstore.as_retriever(search_kwargs={"k": 10})
                    for query_word in query_words[:3]:
                        docs = retriever.invoke(query_word)
                        if os.getenv("ENV") == "development":
                            print(f"ğŸ” DEBUG: Query '{query_word}' returned {len(docs)} docs")
                        for doc in docs:
                            if doc.page_content and len(doc.page_content.strip()) > 50:
                                is_error = (
                                    'error' in doc.metadata or 
                                    'çˆ¬å–å¤±è´¥' in doc.page_content or 
                                    'è§£æå¤±è´¥' in doc.page_content
                                )
                                if not is_error:
                                    sample_docs.append(doc.page_content)
                    sample_docs = list(dict.fromkeys(sample_docs))[:10]
                    if os.getenv("ENV") == "development":
                        print(f"âœ… DEBUG: Retrieved {len(sample_docs)} documents from Chroma")
                except Exception as e:
                    if os.getenv("ENV") == "development":
                        print(f"âŒ DEBUG: Chroma error: {e}")
                    raise
        except Exception as e:
            logger.error(f"Failed to retrieve documents from vector database (collection: {collection_name}): {str(e)}", exc_info=True)
            if os.getenv("ENV") == "development":
                print(f"âš ï¸ Failed to retrieve documents: {e}")
                import traceback
                traceback.print_exc()
            sample_docs = []
        
        # 1.2.11: å¦‚æœæœ‰æ–‡æ¡£ï¼Œä½¿ç”¨LLMç”Ÿæˆé—®é¢˜ï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤é—®é¢˜
        if os.getenv("ENV") == "development":
            print(f"ğŸ” DEBUG: sample_docs count: {len(sample_docs)}")
        if sample_docs and len(sample_docs) > 0:
            # æ„å»ºä¸Šä¸‹æ–‡
            context = "\n\n".join(sample_docs[:5])  # æœ€å¤šä½¿ç”¨5ä¸ªæ–‡æ¡£ç‰‡æ®µ
            
            # 1.2.39: ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹ gpt-4o-mini
            llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)
            
            prompts = {
                "zh": """åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œç”Ÿæˆ3ä¸ªç”¨æˆ·æœ€å¯èƒ½é—®çš„å¸¸è§é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. é—®é¢˜å¿…é¡»åŸºäºæ–‡æ¡£å†…å®¹ï¼Œç¡®ä¿èƒ½ä»æ–‡æ¡£ä¸­æ‰¾åˆ°æ˜ç¡®çš„ç­”æ¡ˆ
2. é—®é¢˜è¦å…·ä½“ã€å®ç”¨ï¼Œä¸è¦è¿‡äºå®½æ³›ï¼ˆé¿å…"ä»‹ç»"ã€"è¯´æ˜"ç­‰è¿‡äºå®½æ³›çš„é—®é¢˜ï¼‰
3. æ¯ä¸ªé—®é¢˜éƒ½è¦ç¡®ä¿æ–‡æ¡£ä¸­æœ‰è¶³å¤Ÿçš„ä¿¡æ¯å¯ä»¥å›ç­”
4. é—®é¢˜åº”è¯¥ä»¥é—®å·ï¼ˆï¼Ÿï¼‰ç»“å°¾
5. åªè¿”å›é—®é¢˜åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªé—®é¢˜ï¼Œä¸è¦æ·»åŠ ç¼–å·ã€å‰ç¼€æˆ–å…¶ä»–è¯´æ˜æ–‡å­—
6. é—®é¢˜åº”è¯¥æ˜¯å®Œæ•´çš„ç–‘é—®å¥ï¼Œå¯ä»¥ç›´æ¥ç”¨äºæé—®

ç¤ºä¾‹æ ¼å¼ï¼š
ä»€ä¹ˆæ˜¯XXXï¼Ÿ
å¦‚ä½•XXXï¼Ÿ
XXXæœ‰å“ªäº›ç‰¹ç‚¹ï¼Ÿ

æ–‡æ¡£å†…å®¹ï¼š
{context}

è¯·ç”Ÿæˆ3ä¸ªå¸¸è§é—®é¢˜ï¼ˆæ¯è¡Œä¸€ä¸ªé—®é¢˜ï¼Œä»¥é—®å·ç»“å°¾ï¼‰ï¼š""",
                "en": """Based on the following document content, generate 3 common questions that users are most likely to ask.

Requirements:
1. Questions must be based on the document content, ensuring clear answers can be found in the documents
2. Questions should be specific and practical, not too broad (avoid questions like "introduce" or "explain" that are too general)
3. Each question must have sufficient information in the documents to answer it
4. Questions should end with a question mark (?)
5. Only return the question list, one question per line, without numbering, prefixes, or other explanatory text
6. Questions should be complete interrogative sentences that can be used directly for asking

Example format:
What is XXX?
How to XXX?
What are the features of XXX?

Document content:
{context}

Please generate 3 common questions (one per line, ending with a question mark):""",
                "ja": """ä»¥ä¸‹ã®æ–‡æ›¸å†…å®¹ã«åŸºã¥ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæœ€ã‚‚å°‹ã­ãã†ãª3ã¤ã®ã‚ˆãã‚ã‚‹è³ªå•ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

è¦ä»¶ï¼š
1. è³ªå•ã¯æ–‡æ›¸å†…å®¹ã«åŸºã¥ãå¿…è¦ãŒã‚ã‚Šã€æ–‡æ›¸ã‹ã‚‰æ˜ç¢ºãªç­”ãˆã‚’è¦‹ã¤ã‘ã‚‰ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„
2. è³ªå•ã¯å…·ä½“çš„ã§å®Ÿç”¨çš„ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã€åºƒã™ãã¦ã¯ã„ã‘ã¾ã›ã‚“ï¼ˆã€Œç´¹ä»‹ã€ã€Œèª¬æ˜ã€ãªã©ã€åºƒã™ãã‚‹è³ªå•ã¯é¿ã‘ã¦ãã ã•ã„ï¼‰
3. å„è³ªå•ã«ã¯ã€æ–‡æ›¸ã«å›ç­”ã§ãã‚‹ååˆ†ãªæƒ…å ±ãŒå¿…è¦ã§ã™
4. è³ªå•ã¯ç–‘å•ç¬¦ï¼ˆï¼Ÿï¼‰ã§çµ‚ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
5. è³ªå•ãƒªã‚¹ãƒˆã®ã¿ã‚’è¿”ã—ã€å„è¡Œã«1ã¤ã®è³ªå•ã‚’ã€ç•ªå·ã€æ¥é ­è¾ã€ãã®ä»–ã®èª¬æ˜ã‚’è¿½åŠ ã›ãšã«è¿”ã—ã¦ãã ã•ã„
6. è³ªå•ã¯å®Œå…¨ãªç–‘å•æ–‡ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã€ç›´æ¥è³ªå•ã«ä½¿ç”¨ã§ãã‚‹å½¢å¼ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™

ä¾‹ã®å½¢å¼ï¼š
XXXã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ
XXXã¯ã©ã®ã‚ˆã†ã«XXXã—ã¾ã™ã‹ï¼Ÿ
XXXã«ã¯ã©ã®ã‚ˆã†ãªç‰¹å¾´ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ

æ–‡æ›¸å†…å®¹ï¼š
{context}

3ã¤ã®ã‚ˆãã‚ã‚‹è³ªå•ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼ˆå„è¡Œã«1ã¤ã®è³ªå•ã€ç–‘å•ç¬¦ã§çµ‚ã‚ã‚‹ï¼‰ï¼š"""
            }
            
            prompt_template = prompts.get(language, prompts["zh"])
            prompt = ChatPromptTemplate.from_template(prompt_template)
            
            try:
                if os.getenv("ENV") == "development":
                    print(f"ğŸ” DEBUG: Generating questions with LLM, context length: {len(context)}")
                # 1.2.24: ç§»é™¤ cl.make_asyncï¼Œä½¿ç”¨ asyncio.to_thread å¤„ç†åŒæ­¥è°ƒç”¨
                # 1.2.39: asyncio å·²åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥ï¼Œæ— éœ€é‡å¤å¯¼å…¥
                response = await asyncio.to_thread(llm.invoke, prompt.format(context=context))
                generated_text = response.content.strip()
                if os.getenv("ENV") == "development":
                    print(f"ğŸ” DEBUG: LLM generated text: {generated_text[:200]}...")
                
                # è§£æç”Ÿæˆçš„é—®é¢˜
                questions = []
                # 1.2.11: æ”¹è¿›é—®é¢˜è§£æé€»è¾‘ï¼Œå¤„ç†å„ç§æ ¼å¼
                lines = generated_text.split('\n')
                for line in lines:
                    line = line.strip()
                    # è·³è¿‡ç©ºè¡Œ
                    if not line:
                        continue
                    # ç§»é™¤å¯èƒ½çš„ç¼–å·ï¼ˆå¦‚ "1. ", "1)", "- ", "â€¢ " ç­‰ï¼‰
                    line = re.sub(r'^[\d\.\)\-\sâ€¢Â·]+', '', line)
                    # ç§»é™¤å¯èƒ½çš„å¼•å·
                    line = line.strip('"\'ã€Œã€ã€ã€')
                    # ç¡®ä¿é—®é¢˜æœ‰è¶³å¤Ÿé•¿åº¦ä¸”ä»¥é—®å·ç»“å°¾ï¼ˆä¸­æ–‡é—®å·æˆ–è‹±æ–‡é—®å·ï¼‰
                    if line and len(line) > 5 and (line.endswith('?') or line.endswith('ï¼Ÿ')):
                        questions.append(line)
                
                # å¦‚æœè§£æåé—®é¢˜ä¸è¶³ï¼Œå°è¯•å…¶ä»–è§£ææ–¹å¼
                if len(questions) < 3:
                    # å°è¯•æŒ‰å¥å·ã€é—®å·åˆ†å‰²
                    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', generated_text)
                    for sentence in sentences:
                        sentence = sentence.strip()
                        if sentence and len(sentence) > 5 and (sentence.endswith('?') or sentence.endswith('ï¼Ÿ')):
                            sentence = re.sub(r'^[\d\.\)\-\sâ€¢Â·]+', '', sentence)
                            sentence = sentence.strip('"\'ã€Œã€ã€ã€')
                            if sentence and sentence not in questions:
                                questions.append(sentence)
                
                if os.getenv("ENV") == "development":
                    print(f"ğŸ” DEBUG: Parsed {len(questions)} questions: {questions}")
                
                # 1.2.11: éªŒè¯æ¯ä¸ªé—®é¢˜æ˜¯å¦æœ‰å›å¤ï¼ˆé€šè¿‡å¿«é€Ÿæ£€ç´¢æµ‹è¯•ï¼‰
                # 1.2.39: ä¼˜åŒ–éªŒè¯ - å¹¶è¡Œå¤„ç† + æ‰¹é‡ embedding + å¤ç”¨è¿æ¥
                embeddings_model = OpenAIEmbeddings()
                questions_to_validate = questions[:5]  # 1.2.39: åªéªŒè¯å‰5ä¸ªé—®é¢˜
                
                if os.getenv("ENV") == "development":
                    print(f"ğŸ” DEBUG: Validating {len(questions_to_validate)} questions in parallel")
                
                # 1.2.39: æ‰¹é‡ç”Ÿæˆæ‰€æœ‰é—®é¢˜çš„ embeddingsï¼ˆä¸€æ¬¡ API è°ƒç”¨ï¼‰
                question_vectors = await asyncio.to_thread(
                    embeddings_model.embed_documents, 
                    questions_to_validate
                )
                
                # 1.2.39: å¤ç”¨æ•°æ®åº“è¿æ¥ï¼ˆåªåˆ›å»ºä¸€æ¬¡ï¼‰
                vx = None
                collection = None
                if USE_PGVECTOR and DATABASE_URL:
                    try:
                        vx = vecs.create_client(DATABASE_URL)
                        collection = vx.get_collection(name=collection_name)
                    except Exception as e:
                        logger.warning(f"Failed to initialize pgvector connection: {e}")
                
                # å®šä¹‰éªŒè¯å•ä¸ªé—®é¢˜çš„å‡½æ•°
                async def validate_single_question(question, query_vector):
                    """éªŒè¯å•ä¸ªé—®é¢˜æ˜¯å¦èƒ½æ‰¾åˆ°æœ‰æ•ˆæ–‡æ¡£"""
                    try:
                        found_doc = False
                        use_chroma_fallback = False
                        
                        if collection:
                            try:
                                results = collection.query(
                                    data=query_vector,
                                    limit=1,
                                    include_value=False,
                                    include_metadata=True
                                )
                                if results and len(results) > 0:
                                    record = results[0]
                                    if len(record) > 1 and record[1]:
                                        text = record[1].get("text", "")
                                        metadata = record[1].get("metadata", {}) if isinstance(record[1], dict) else {}
                                        is_error = (
                                            'error' in metadata or 
                                            'çˆ¬å–å¤±è´¥' in text or 
                                            'è§£æå¤±è´¥' in text or
                                            text.strip().startswith('çˆ¬å–å¤±è´¥') or
                                            text.strip().startswith('è§£æå¤±è´¥')
                                        )
                                        if not is_error and text.strip() and len(text.strip()) > 50:
                                            found_doc = True
                                    else:
                                        use_chroma_fallback = True
                                else:
                                    use_chroma_fallback = True
                            except Exception as e:
                                if os.getenv("ENV") == "development":
                                    print(f"âš ï¸ pgvector validation error for '{question}': {e}")
                                use_chroma_fallback = True
                        else:
                            use_chroma_fallback = True
                        
                        # Chroma å›é€€
                        if use_chroma_fallback and not found_doc:
                            try:
                                vectorstore = Chroma(
                                    persist_directory=f"./chroma_db/{collection_name}",
                                    embedding_function=OpenAIEmbeddings()
                                )
                                retriever = vectorstore.as_retriever(search_kwargs={"k": 1})
                                docs = await asyncio.to_thread(retriever.invoke, question)
                                if docs and len(docs) > 0:
                                    doc = docs[0]
                                    is_error = (
                                        'error' in doc.metadata or 
                                        'çˆ¬å–å¤±è´¥' in doc.page_content or 
                                        'è§£æå¤±è´¥' in doc.page_content
                                    )
                                    if not is_error and doc.page_content and len(doc.page_content.strip()) > 50:
                                        found_doc = True
                            except Exception as e:
                                if os.getenv("ENV") == "development":
                                    print(f"âš ï¸ Chroma validation error for '{question}': {e}")
                        
                        if found_doc:
                            if os.getenv("ENV") == "development":
                                print(f"âœ… DEBUG: Question validated: {question}")
                            return question
                        else:
                            if os.getenv("ENV") == "development":
                                print(f"âš ï¸ DEBUG: Question has no valid reply: {question}")
                            return None
                    except Exception as e:
                        if os.getenv("ENV") == "development":
                            print(f"âš ï¸ Failed to validate question '{question}': {e}")
                        return None
                
                # 1.2.39: å¹¶è¡ŒéªŒè¯æ‰€æœ‰é—®é¢˜
                validation_results = await asyncio.gather(*[
                    validate_single_question(q, qv) 
                    for q, qv in zip(questions_to_validate, question_vectors)
                ])
                
                # è¿‡æ»¤å‡ºæœ‰æ•ˆé—®é¢˜
                valid_questions = [q for q in validation_results if q is not None]
                
                if len(valid_questions) >= 3:
                    # 1.2.39: ä¿å­˜åˆ°ç¼“å­˜
                    result_questions = valid_questions[:3]
                    frequent_questions_cache[cache_key] = result_questions
                    logger.info(f"Cached {len(result_questions)} questions for kb_token: {kb_token}, language: {language}")
                    
                    return JSONResponse(
                        status_code=200,
                        content={
                            "status": "success",
                            "questions": result_questions
                        },
                        headers={"Content-Type": "application/json"}
                    )
                elif len(valid_questions) > 0:
                    # å¦‚æœåªæœ‰éƒ¨åˆ†é—®é¢˜æœ‰æ•ˆï¼Œè¡¥å……é»˜è®¤é—®é¢˜
                    default_questions = {
                        "zh": ["æ‚¨èƒ½ä»‹ç»ä¸€ä¸‹è¿™ä¸ªé¡¹ç›®å—ï¼Ÿ", "æœ‰å“ªäº›å¸¸è§é—®é¢˜ï¼Ÿ", "å¦‚ä½•ä½¿ç”¨è¿™ä¸ªç³»ç»Ÿï¼Ÿ"],
                        "en": ["Can you introduce this project?", "What are the common questions?", "How to use this system?"],
                        "ja": ["ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ã¤ã„ã¦ç´¹ä»‹ã—ã¦ã„ãŸã ã‘ã¾ã™ã‹ï¼Ÿ", "ã‚ˆãã‚ã‚‹è³ªå•ã¯ä½•ã§ã™ã‹ï¼Ÿ", "ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã®ä½¿ã„æ–¹ã¯ï¼Ÿ"]
                    }
                    default_qs = default_questions.get(language, default_questions["zh"])
                    # åˆå¹¶æœ‰æ•ˆé—®é¢˜å’Œé»˜è®¤é—®é¢˜
                    final_questions = valid_questions + [q for q in default_qs if q not in valid_questions]
                    result_questions = final_questions[:3]
                    
                    # 1.2.39: ä¿å­˜åˆ°ç¼“å­˜ï¼ˆå³ä½¿æ˜¯éƒ¨åˆ†ç”Ÿæˆ+é»˜è®¤é—®é¢˜çš„ç»„åˆï¼‰
                    frequent_questions_cache[cache_key] = result_questions
                    logger.info(f"Cached {len(result_questions)} questions (partial) for kb_token: {kb_token}, language: {language}")
                    
                    return JSONResponse(
                        status_code=200,
                        content={
                            "status": "success",
                            "questions": result_questions
                        },
                        headers={"Content-Type": "application/json"}
                    )
            except Exception as e:
                logger.error(f"Failed to generate questions with LLM (collection: {collection_name}, language: {language}): {str(e)}", exc_info=True)
                if os.getenv("ENV") == "development":
                    print(f"âš ï¸ Failed to generate questions: {e}")
                    import traceback
                    traceback.print_exc()
                # ç”Ÿæˆå¤±è´¥æ—¶ï¼Œç»§ç»­æ‰§è¡Œä¸‹é¢çš„ä»£ç è¿”å›é»˜è®¤é—®é¢˜
        
        # å¦‚æœæ²¡æœ‰æ–‡æ¡£æˆ–ç”Ÿæˆå¤±è´¥ï¼Œè¿”å›é»˜è®¤é—®é¢˜
        logger.warning(f"Returning default questions for kb_token: {kb_token}, reason: no documents or generation failed (docs_count: {len(sample_docs) if 'sample_docs' in locals() else 0})")
        if os.getenv("ENV") == "development":
            print(f"âš ï¸ DEBUG: No documents or generation failed, using default questions")
        default_questions = {
            "zh": [
                "æ‚¨èƒ½ä»‹ç»ä¸€ä¸‹è¿™ä¸ªé¡¹ç›®å—ï¼Ÿ",
                "æœ‰å“ªäº›å¸¸è§é—®é¢˜ï¼Ÿ",
                "å¦‚ä½•ä½¿ç”¨è¿™ä¸ªç³»ç»Ÿï¼Ÿ"
            ],
            "en": [
                "Can you introduce this project?",
                "What are the common questions?",
                "How to use this system?"
            ],
            "ja": [
                "ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ã¤ã„ã¦ç´¹ä»‹ã—ã¦ã„ãŸã ã‘ã¾ã™ã‹ï¼Ÿ",
                "ã‚ˆãã‚ã‚‹è³ªå•ã¯ä½•ã§ã™ã‹ï¼Ÿ",
                "ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã®ä½¿ã„æ–¹ã¯ï¼Ÿ"
            ]
        }
        
        return JSONResponse(
            status_code=200,
            content={
                "status": "success",
                "questions": default_questions.get(language, default_questions["zh"])
            },
            headers={"Content-Type": "application/json"}
        )
    except Exception as e:
        # 1.2.36: è®°å½•å®Œæ•´çš„é”™è¯¯ä¿¡æ¯ï¼ŒåŒ…æ‹¬å †æ ˆè·Ÿè¸ªï¼ˆç”Ÿäº§ç¯å¢ƒä¹Ÿä¼šè®°å½•ï¼‰
        logger.error(f"Frequent questions API error: {str(e)}", exc_info=True)
        if os.getenv("ENV") == "development":
            print(f"âŒ Frequent questions API error: {e}")
            import traceback
            traceback.print_exc()
        # å‡ºé”™æ—¶è¿”å›é»˜è®¤é—®é¢˜ï¼Œç¡®ä¿æ€»æ˜¯è¿”å›JSONå“åº”
        try:
            # å°è¯•ä»è¯·æ±‚ä¸­è·å–è¯­è¨€å‚æ•°
            try:
                data = await request.json()
                language = data.get("language", "zh")
            except:
                language = request.query_params.get("language", "zh")
            if language not in ["zh", "en", "ja"]:
                language = "zh"
        except:
            language = "zh"
        
        logger.warning(f"Returning default questions due to exception for language: {language}")
        default_questions = {
            "zh": ["æ‚¨èƒ½ä»‹ç»ä¸€ä¸‹è¿™ä¸ªé¡¹ç›®å—ï¼Ÿ", "æœ‰å“ªäº›å¸¸è§é—®é¢˜ï¼Ÿ", "å¦‚ä½•ä½¿ç”¨è¿™ä¸ªç³»ç»Ÿï¼Ÿ"],
            "en": ["Can you introduce this project?", "What are the common questions?", "How to use this system?"],
            "ja": ["ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ã¤ã„ã¦ç´¹ä»‹ã—ã¦ã„ãŸã ã‘ã¾ã™ã‹ï¼Ÿ", "ã‚ˆãã‚ã‚‹è³ªå•ã¯ä½•ã§ã™ã‹ï¼Ÿ", "ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã®ä½¿ã„æ–¹ã¯ï¼Ÿ"]
        }
        return JSONResponse(
            status_code=200,
            content={
                "status": "success",
                "questions": default_questions.get(language, default_questions["zh"])
            },
            headers={"Content-Type": "application/json"}
        )

# 1.2.24: ä»¥ä¸‹ Chainlit ç›¸å…³ä»£ç å·²è¢«æµå¼ /api/chat/stream æ›¿ä»£
# ä¿ç•™æ³¨é‡Šä»¥ä¾¿å‚è€ƒåŸæœ‰å®ç°
"""
# 1.2.23: å·²æ›¿ä»£ - ä»¥ä¸‹ä»£ç ä½¿ç”¨ Chainlit æ¡†æ¶å®ç°é¢å‘ç”¨æˆ·çš„èŠå¤©
# 1.2.24: ç°åœ¨ä½¿ç”¨ React å‰ç«¯ + /api/chat/stream å®ç°æµå¼èŠå¤©
# 1.1.0: å­˜å‚¨å½“å‰ä¼šè¯çš„çŠ¶æ€
@cl.on_chat_start
async def start():
    # 1.1.2: ä» URL å‚æ•°è·å– kb_id (share_token)
    # æ ¼å¼: http://localhost:8000/?kb_id=xxx
    # 1.1.17: å°è¯•ä» http_referer è·å– kb_id
    from urllib.parse import urlparse, parse_qs

    kb_id = None

    # 1.1.17: ä» http_referer è·å– kb_id
    http_referer = cl.user_session.get("http_referer")
    if os.getenv("ENV") == "development":
        print(f"ğŸ” DEBUG on_chat_start called")
        client_type = cl.user_session.get("client_type")
        session_id = cl.user_session.get("id")
        print(f"ğŸ” DEBUG: http_referer = {http_referer}")
        print(f"ğŸ” DEBUG: client_type = {client_type}")
        print(f"ğŸ” DEBUG: session_id = {session_id}")

    if http_referer:
        try:
            parsed_url = urlparse(http_referer)
            query_params = parse_qs(parsed_url.query)
            kb_id = query_params.get("kb_id", [None])[0]
            if os.getenv("ENV") == "development":
                print(f"ğŸ” DEBUG: parsed kb_id = {kb_id} from http_referer")
        except Exception as e:
            if os.getenv("ENV") == "development":
                print(f"âš ï¸ Failed to parse http_referer: {e}")
            kb_id = None
    
    if kb_id:
        # 1.1.2: å…¬å¼€è®¿é—®æ¨¡å¼ - ä» Supabase è·å–é¡¹ç›®çš„ vector_collection
        collection_name = None  # åˆå§‹åŒ–ä¸º Noneï¼Œç¡®ä¿åç»­éªŒè¯ç”Ÿæ•ˆ
        
        if supabase:
            try:
                # 1.2.0: æŸ¥è¯¢ knowledge_bases è¡¨ï¼Œè·å– vector_collection å’Œ chat_config
                result = supabase.table("knowledge_bases")\
                    .select("vector_collection, name, chat_config")\
                    .eq("share_token", kb_id)\
                    .single()\
                    .execute()
                
                if result.data:
                    vector_collection = result.data.get("vector_collection")
                    # 1.1.15: éªŒè¯ vector_collection æ˜¯å¦æœ‰æ•ˆ
                    if vector_collection and isinstance(vector_collection, str) and vector_collection.strip():
                        collection_name = vector_collection.strip()
                        project_name = result.data.get("name", "é¡¹ç›®")
                        
                        # 1.2.0: è¯»å–èŠå¤©é…ç½®
                        chat_config = result.data.get("chat_config", {})
                        
                        # 1.2.0: æ£€æµ‹è¯­è¨€ï¼ˆä»è¯·æ±‚å¤´æˆ–é»˜è®¤ä¸­æ–‡ï¼‰
                        language = "zh"  # TODO: ä»è¯·æ±‚å¤´æ£€æµ‹è¯­è¨€
                        
                        # 1.2.0: è·å–æ¬¢è¿è¯­å’Œå¤´åƒ
                        welcome_message = ""
                        avatar_url = ""
                        recommended_questions = []
                        
                        if chat_config:
                            welcome_message = chat_config.get("welcome_message", {}).get(language, "")
                            avatar_url = chat_config.get("avatar_url", "")
                            recommended_questions = chat_config.get("recommended_questions", {}).get(language, [])
                        
                        # 1.2.0: å¦‚æœæ²¡æœ‰é…ç½®æ¬¢è¿è¯­ï¼Œä½¿ç”¨é»˜è®¤æ¬¢è¿è¯­
                        if not welcome_message:
                            welcome_message = f"æ¬¢è¿è®¿é—® {project_name} çš„çŸ¥è¯†åº“ï¼æ‚¨å¯ä»¥è¯¢é—®ä¸è¯¥é¡¹ç›®ç›¸å…³çš„ä»»ä½•é—®é¢˜ã€‚"
                        
                        # 1.2.0: å‘é€æ¬¢è¿è¯­ï¼ˆå¸¦å¤´åƒï¼‰
                        if avatar_url:
                            # 1.2.24: è®¾ç½®åŠ¨æ€å¤´åƒ
                            await cl.Avatar(name="Assistant", url=avatar_url).send()
                            # Chainlitæ”¯æŒé€šè¿‡authorå‚æ•°ä¼ é€’å¤´åƒURL
                            await cl.Message(content=welcome_message, author="Assistant").send()
                        else:
                            await cl.Message(content=welcome_message, author="Assistant").send()
                        
                        # 1.2.0: å¦‚æœæœªé…ç½®æ¨èé—®é¢˜ï¼Œè·å–é«˜é¢‘é—®é¢˜
                        if not recommended_questions:
                            # è°ƒç”¨é«˜é¢‘é—®é¢˜APIï¼ˆæˆ–ç›´æ¥ä½¿ç”¨é»˜è®¤é—®é¢˜ï¼‰
                            default_questions = {
                                "zh": [
                                    "æ‚¨èƒ½ä»‹ç»ä¸€ä¸‹è¿™ä¸ªé¡¹ç›®å—ï¼Ÿ",
                                    "æœ‰å“ªäº›å¸¸è§é—®é¢˜ï¼Ÿ",
                                    "å¦‚ä½•ä½¿ç”¨è¿™ä¸ªç³»ç»Ÿï¼Ÿ"
                                ],
                                "en": [
                                    "Can you introduce this project?",
                                    "What are the common questions?",
                                    "How to use this system?"
                                ],
                                "ja": [
                                    "ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ã¤ã„ã¦ç´¹ä»‹ã—ã¦ã„ãŸã ã‘ã¾ã™ã‹ï¼Ÿ",
                                    "ã‚ˆãã‚ã‚‹è³ªå•ã¯ä½•ã§ã™ã‹ï¼Ÿ",
                                    "ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã®ä½¿ã„æ–¹ã¯ï¼Ÿ"
                                ]
                            }
                            recommended_questions = default_questions.get(language, default_questions["zh"])
                        
                        # 1.2.0: å‘é€æ¨èé—®é¢˜ï¼ˆä½¿ç”¨ActionButtonï¼‰
                        if recommended_questions and len(recommended_questions) > 0:
                            actions = []
                            for question in recommended_questions[:3]:
                                actions.append(
                                    cl.Action(name=question, value=question, label=question)
                                )
                            if actions:
                                await cl.Message(content="", actions=actions, author="Assistant").send()
                    else:
                        # 1.2.1: å¦‚æœ vector_collection ä¸ºç©ºï¼Œä½¿ç”¨ kb_id ä½œä¸ºåå¤‡ï¼Œä½†ä»å‘é€æ¬¢è¿è¯­
                        if os.getenv("ENV") == "development":
                            print(f"âš ï¸ vector_collection is empty for kb_id {kb_id}, using kb_id as collection_name")
                        collection_name = kb_id
                        # 1.2.1: å³ä½¿ vector_collection ä¸ºç©ºï¼Œä¹Ÿå‘é€é»˜è®¤æ¬¢è¿è¯­
                        default_welcome = "æ¬¢è¿è®¿é—®çŸ¥è¯†åº“ï¼æ‚¨å¯ä»¥è¯¢é—®ä¸è¯¥é¡¹ç›®ç›¸å…³çš„ä»»ä½•é—®é¢˜ã€‚"
                        await cl.Message(content=default_welcome, author="Assistant").send()
                else:
                    # 1.2.1: æ•°æ®åº“æŸ¥è¯¢è¿”å›ç©ºç»“æœï¼Œå‘é€é»˜è®¤æ¬¢è¿è¯­
                    if os.getenv("ENV") == "development":
                        print(f"âš ï¸ No data found for kb_id {kb_id}, using kb_id as collection_name")
                    collection_name = kb_id
                    # 1.2.1: å³ä½¿æŸ¥è¯¢å¤±è´¥ï¼Œä¹Ÿå‘é€é»˜è®¤æ¬¢è¿è¯­
                    default_welcome = "æ¬¢è¿è®¿é—®çŸ¥è¯†åº“ï¼æ‚¨å¯ä»¥è¯¢é—®ä¸è¯¥é¡¹ç›®ç›¸å…³çš„ä»»ä½•é—®é¢˜ã€‚"
                    await cl.Message(content=default_welcome, author="Assistant").send()
            except Exception as e:
                # 1.2.1: æ•°æ®åº“æŸ¥è¯¢å¤±è´¥ï¼Œå‘é€é»˜è®¤æ¬¢è¿è¯­
                if os.getenv("ENV") == "development":
                    print(f"âš ï¸ Failed to fetch vector_collection: {e}, using kb_id as collection_name")
                collection_name = kb_id
                # 1.2.1: å³ä½¿æŸ¥è¯¢å¤±è´¥ï¼Œä¹Ÿå‘é€é»˜è®¤æ¬¢è¿è¯­
                default_welcome = "æ¬¢è¿è®¿é—®çŸ¥è¯†åº“ï¼æ‚¨å¯ä»¥è¯¢é—®ä¸è¯¥é¡¹ç›®ç›¸å…³çš„ä»»ä½•é—®é¢˜ã€‚"
                await cl.Message(content=default_welcome, author="Assistant").send()
        else:
            # 1.2.1: Supabase å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨ kb_idï¼Œå‘é€é»˜è®¤æ¬¢è¿è¯­
            if os.getenv("ENV") == "development":
                print(f"âš ï¸ Supabase client not available, using kb_id as collection_name")
            collection_name = kb_id
            # 1.2.1: å³ä½¿ Supabase æœªåˆå§‹åŒ–ï¼Œä¹Ÿå‘é€é»˜è®¤æ¬¢è¿è¯­
            default_welcome = "æ¬¢è¿è®¿é—®çŸ¥è¯†åº“ï¼æ‚¨å¯ä»¥è¯¢é—®ä¸è¯¥é¡¹ç›®ç›¸å…³çš„ä»»ä½•é—®é¢˜ã€‚"
            await cl.Message(content=default_welcome, author="Assistant").send()
        
        # 1.1.15: æœ€ç»ˆéªŒè¯ - ç¡®ä¿ collection_name æœ‰æ•ˆï¼ˆå¿…é¡»æ˜¯éç©ºå­—ç¬¦ä¸²ï¼‰
        if not collection_name or not isinstance(collection_name, str):
            collection_name = kb_id  # ä½¿ç”¨ kb_id ä½œä¸ºæœ€åçš„ä¿éšœ
        
        # 1.1.15: æ¸…ç†å¹¶éªŒè¯æ ¼å¼ï¼ˆå»é™¤é¦–å°¾ç©ºæ ¼ï¼‰
        collection_name = collection_name.strip()
        
        if not collection_name:
            # 1.1.15: å¦‚æœ kb_id æœ¬èº«ä¹Ÿæ˜¯ç©ºçš„ï¼Œç”Ÿæˆä¸€ä¸ªä¸´æ—¶å€¼
            collection_name = f"temp_{uuid.uuid4().hex[:8]}"
            if os.getenv("ENV") == "development":
                print(f"âš ï¸ Both kb_id and collection_name are empty, using temporary: {collection_name}")
        
        # 1.1.15: è°ƒè¯•è¾“å‡º
        if os.getenv("ENV") == "development":
            print(f"âœ… Setting collection_name to: {collection_name} for kb_id: {kb_id}")
        
        cl.user_session.set("collection_name", collection_name)
        cl.user_session.set("messages", [])
        cl.user_session.set("is_public", True)
    else:
        # 1.1.1: è®¾ç½®åˆå§‹çŠ¶æ€ï¼ˆç”¨äºæµ‹è¯•æ¨¡å¼ï¼‰
        cl.user_session.set("messages", [])
        cl.user_session.set("collection_name", str(uuid.uuid4()))
        cl.user_session.set("is_public", False)
        
        # 1.1.0: å¼•å¯¼ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶
        files = None
        while files is None:
            files = await cl.AskFileMessage(
                content="æ¬¢è¿ä½¿ç”¨ YUIChatï¼è¯·ä¸Šä¼  PDFã€Word æˆ– Excel æ–‡ä»¶å¼€å§‹åˆ†æã€‚",
                accept=[
                    "application/pdf", 
                    "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                    "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                ],
                max_size_mb=20,
                timeout=180,
            ).send()

        file = files[0]
        
        msg = cl.Message(content=f"æ­£åœ¨å¤„ç†æ–‡ä»¶: {file.name}...")
        await msg.send()

        initial_state = {
            "file_path": file.path,
            "collection_name": cl.user_session.get("collection_name"),
            "messages": [HumanMessage(content="ä½ å¥½")], # å ä½ç¬¦
            "docs": [],
            "splits": [],
            "context": "",
            "answer": ""
        }
        
        result = await cl.make_async(workflow_app.invoke)(initial_state)
        cl.user_session.set("messages", result.get("messages", []))
        await msg.update(content=f"æ–‡ä»¶ {file.name} å¤„ç†å®Œæˆï¼ç°åœ¨ä½ å¯ä»¥å¼€å§‹æé—®äº†ã€‚")

@cl.on_message
async def main(message: cl.Message):
    from urllib.parse import urlparse, parse_qs

    messages = cl.user_session.get("messages", [])
    collection_name = cl.user_session.get("collection_name")

    # 1.1.17: å¦‚æœ collection_name æ— æ•ˆï¼Œå°è¯•ä» http_referer åŠ¨æ€è·å–
    # è¿™æ˜¯ä¸ºäº†è§£å†³ Chainlit 2.3.0 åœ¨ on_chat_start ä¸­çš„ ContextVar é—®é¢˜
    if not collection_name or not isinstance(collection_name, str) or not collection_name.strip():
        http_referer = cl.user_session.get("http_referer")
        if os.getenv("ENV") == "development":
            print(f"ğŸ” on_message: collection_name invalid, trying http_referer: {http_referer}")

        kb_id = None
        if http_referer:
            try:
                parsed_url = urlparse(http_referer)
                query_params = parse_qs(parsed_url.query)
                kb_id = query_params.get("kb_id", [None])[0]
                if os.getenv("ENV") == "development":
                    print(f"ğŸ” on_message: parsed kb_id = {kb_id}")
            except Exception as e:
                if os.getenv("ENV") == "development":
                    print(f"âš ï¸ on_message: Failed to parse http_referer: {e}")

        # 1.1.17: å¦‚æœè·å–åˆ° kb_idï¼Œä» Supabase è·å– collection_name
        if kb_id and supabase:
            try:
                result = supabase.table("knowledge_bases")\
                    .select("vector_collection")\
                    .eq("share_token", kb_id)\
                    .single()\
                    .execute()

                if result.data and result.data.get("vector_collection"):
                    collection_name = result.data.get("vector_collection").strip()
                    cl.user_session.set("collection_name", collection_name)
                    cl.user_session.set("messages", [])
                    if os.getenv("ENV") == "development":
                        print(f"âœ… on_message: Set collection_name = {collection_name}")
            except Exception as e:
                if os.getenv("ENV") == "development":
                    print(f"âš ï¸ on_message: Failed to fetch collection_name: {e}")

    # 1.1.15: å†æ¬¡éªŒè¯ collection_name æ˜¯å¦æœ‰æ•ˆ
    if not collection_name or not isinstance(collection_name, str) or not collection_name.strip():
        error_msg = "çŸ¥è¯†åº“é…ç½®é”™è¯¯ï¼šcollection_name æ— æ•ˆã€‚è¯·é‡æ–°è®¿é—®çŸ¥è¯†åº“é“¾æ¥ã€‚"
        if os.getenv("ENV") == "development":
            print(f"âŒ Chainlit message error: collection_name is invalid: {collection_name}")
        await cl.Message(content=error_msg, author="Assistant").send()
        return
    
    # 1.1.0: æ·»åŠ ç”¨æˆ·æ–°æ¶ˆæ¯
    messages.append(HumanMessage(content=message.content))
    
    # 1.1.0: å‡†å¤‡çŠ¶æ€
    state = {
        "messages": messages,
        "collection_name": collection_name.strip(),
        "file_path": "",
        "docs": [],
        "splits": [],
        "context": "",
        "answer": ""
    }
    
    # 1.1.2: è°ƒç”¨å·¥ä½œæµå¹¶æ˜¾ç¤ºæµå¼è¾“å‡ºï¼ˆChainlit é»˜è®¤ä¸æ”¯æŒ LangGraph åŸç”Ÿæµå¼ï¼Œè¿™é‡Œç®€å•å¤„ç†ï¼‰
    # å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨ astream_events
    try:
        result = await cl.make_async(workflow_app.invoke)(state)
        
        answer = result.get("answer", "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚")
        messages = result.get("messages", [])
        
        cl.user_session.set("messages", messages)
        
        # 1.2.23: æ˜¾å¼è®¾ç½® author ä»¥åŒ¹é… public/avatars/assistant.png
        await cl.Message(content=answer, author="Assistant").send()
    except ValueError as e:
        # 1.1.15: å¤„ç† collection_name éªŒè¯é”™è¯¯
        error_msg = f"çŸ¥è¯†åº“é…ç½®é”™è¯¯ï¼š{str(e)}ã€‚è¯·é‡æ–°è®¿é—®çŸ¥è¯†åº“é“¾æ¥ã€‚"
        if os.getenv("ENV") == "development":
            print(f"âŒ Chainlit workflow error: {e}")
        await cl.Message(content=error_msg, author="Assistant").send()
    except Exception as e:
        error_msg = f"å¯¹è¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"
        if os.getenv("ENV") == "development":
            print(f"âŒ Chainlit unexpected error: {e}")
        await cl.Message(content=error_msg, author="Assistant").send()
"""  # 1.2.24: Chainlit ä»£ç å—ç»“æŸ

# 1.2.24: ä½¿ç”¨ uvicorn å¯åŠ¨ FastAPI åº”ç”¨
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        fastapi_app,
        host="0.0.0.0",
        port=8000,
        log_level="info"
    )
