#!/usr/bin/env python3
"""
1.3.16: RAG å›å¤ç²¾åº¦è¯„ä¼°è„šæœ¬ - é€šè¿‡ API è°ƒç”¨åç«¯

ä½¿ç”¨æ–¹å¼:
1. ç¡®ä¿åç«¯æœåŠ¡è¿è¡Œä¸­ï¼ˆæœ¬åœ°æˆ–ç”Ÿäº§ç¯å¢ƒï¼‰
2. è¿è¡Œ: python ragas_eval_api.py

è¯„ä¼°ç»´åº¦:
- LLM-as-Judge è¯„åˆ†ï¼ˆå‡†ç¡®æ€§ã€å®Œæ•´æ€§ã€ç›¸å…³æ€§ã€å¿ å®åº¦ã€å¯è¯»æ€§ï¼‰
- æ£€ç´¢è´¨é‡ï¼ˆå…³é”®è¯å‘½ä¸­ç‡ï¼‰
- å›ç­”è´¨é‡ï¼ˆä¸æ ‡å‡†ç­”æ¡ˆå¯¹æ¯”ï¼‰
"""

import os
import sys
import json
import asyncio
import aiohttp
import re
from datetime import datetime
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv('.env.local')
load_dotenv()

# å¯¼å…¥ LLM
from langchain_openai import ChatOpenAI

# ==================== é…ç½® ====================

# API é…ç½®
# æœ¬åœ°å¼€å‘ç¯å¢ƒ
LOCAL_API_URL = "http://localhost:8000/api/chat"
# ç”Ÿäº§ç¯å¢ƒ (GCP Cloud Run)
PROD_API_URL = os.getenv("BACKEND_API_URL", "https://yuichat-backend-xxxxx.a.run.app/api/chat")

# ä½¿ç”¨çš„ API (é»˜è®¤æœ¬åœ°)
API_URL = LOCAL_API_URL

# çŸ¥è¯†åº“é…ç½® - å¯é€šè¿‡å‘½ä»¤è¡Œå‚æ•°åˆ‡æ¢
# æ³¨æ„: æœ¬åœ°å¼€å‘ç¯å¢ƒå’Œç”Ÿäº§ç¯å¢ƒçš„ share_token/vector_collection å¯èƒ½ä¸åŒ
KNOWLEDGE_BASE_CONFIGS = {
    # æœ¬åœ°ç¯å¢ƒé…ç½®
    "æ³•äººç¨æ³•_local": {
        "name": "æ³•äººç¨æ³•",
        "share_token": "85d248a2-14fc-421f-b434-20f1f4ec4617",  # æœ¬åœ° Supabase
        "vector_collection": "kb_1769244387425_4x3ivr",  # æœ¬åœ° pgvector
        "kb_id": "2721e73f-81e1-4b61-8b04-739952866d76",
    },
    # ç”Ÿäº§ç¯å¢ƒé…ç½®
    "æ³•äººç¨æ³•_prod": {
        "name": "æ³•äººç¨æ³•",
        "share_token": "deb48327-150b-408d-8b0c-56ce7333a987",  # ç”Ÿäº§ Supabase
        "vector_collection": "kb_1769060431971_13o0t",  # ç”Ÿäº§ pgvector
        "kb_id": "f870ee08-86b7-4911-932e-7b28f2727276",
    },
    "è³‡æ–™æ¦‚è¦ã—ã‚‰ã¹": {
        "name": "è³‡æ–™æ¦‚è¦ã—ã‚‰ã¹",
        "share_token": "06e922e5-519c-4083-a386-7bb23a6cf3aa",
        "vector_collection": "kb_1769417934844_vwfiu3",
        "kb_id": "3a405ec7-488a-4855-848f-e30473966ea9",
    }
}

# é»˜è®¤ä½¿ç”¨æœ¬åœ°æ³•äººç¨æ³•é…ç½®
KNOWLEDGE_BASE_CONFIG = KNOWLEDGE_BASE_CONFIGS["æ³•äººç¨æ³•_local"]

# ==================== æµ‹è¯•æ•°æ®é›† ====================

# æ³•äººç¨æ³•è¯„ä¼°æµ‹è¯•é›† (Golden Dataset)
TEST_CASES = [
    {
        "id": "001",
        "question": "ç‰¹å®šæ”¯é…é–¢ä¿‚ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "ground_truth": "ç‰¹å®šæ”¯é…é–¢ä¿‚ã¨ã¯ã€ä¸€æ–¹ã®æ³•äººãŒä»–æ–¹ã®æ³•äººã®ç™ºè¡Œæ¸ˆæ ªå¼ç­‰ã®50%ã‚’è¶…ãˆã‚‹æ•°ã®æ ªå¼ç­‰ã‚’ç›´æ¥åˆã¯é–“æ¥ã«ä¿æœ‰ã™ã‚‹é–¢ä¿‚ãªã©ã‚’ã„ã„ã¾ã™ã€‚",
        "expected_keywords": ["ç‰¹å®šæ”¯é…é–¢ä¿‚", "50%", "æ ªå¼", "æ³•äºº"],
        "language": "ja",
        "difficulty": "medium"
    },
    {
        "id": "002",
        "question": "é€£çµæ¬ æé‡‘ã®é©ç”¨æ¡ä»¶ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "ground_truth": "é€£çµæ¬ æé‡‘ã‚’ç¹°è¶Šæ§é™¤ã™ã‚‹ãŸã‚ã«ã¯ã€é€£çµæ³•äººãŒç¶™ç¶šã—ã¦é€£çµç¢ºå®šç”³å‘Šæ›¸ã‚’æå‡ºã—ã€æ¬ æé‡‘ã®ç¹°è¶Šæ§é™¤ã‚’é©ç”¨ã™ã‚‹äº‹æ¥­å¹´åº¦ã®ç¢ºå®šç”³å‘Šæ›¸ç­‰ã«ã‚‚æ¬ æé‡‘ã®æ˜ç´°ã‚’è¨˜è¼‰ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚",
        "expected_keywords": ["é€£çµæ¬ æé‡‘", "ç¹°è¶Šæ§é™¤", "ç¢ºå®šç”³å‘Š"],
        "language": "ja",
        "difficulty": "hard"
    },
    {
        "id": "003",
        "question": "åˆä½µå¾Œã®æœªå‡¦ç†æ¬ æé‡‘ã¯ã©ã®ã‚ˆã†ã«è¨ˆç®—ã•ã‚Œã¾ã™ã‹ï¼Ÿ",
        "ground_truth": "åˆä½µå¾Œã®æœªå‡¦ç†æ¬ æé‡‘ã¯ã€è¢«åˆä½µæ³•äººã®é©æ ¼åˆä½µç­‰å‰ã®æ¬ æé‡‘é¡ã¨åˆä½µæ³•äººã®æ¬ æé‡‘é¡ã‚’ä¸€å®šã®è¨ˆç®—å¼ã«åŸºã¥ã„ã¦ç®—å®šã—ã¾ã™ã€‚",
        "expected_keywords": ["åˆä½µ", "æ¬ æé‡‘", "è¢«åˆä½µæ³•äºº", "è¨ˆç®—"],
        "language": "ja",
        "difficulty": "hard"
    },
    {
        "id": "004",
        "question": "æ³•äººç¨ã®ç”³å‘ŠæœŸé™ã¯ã„ã¤ã§ã™ã‹ï¼Ÿ",
        "ground_truth": "æ³•äººç¨ã®ç”³å‘Šã¯ã€å„äº‹æ¥­å¹´åº¦çµ‚äº†ã®æ—¥ã®ç¿Œæ—¥ã‹ã‚‰2æœˆä»¥å†…ã«ç¢ºå®šç”³å‘Šæ›¸ã‚’æå‡ºã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚",
        "expected_keywords": ["ç”³å‘Š", "æœŸé™", "2æœˆ", "äº‹æ¥­å¹´åº¦"],
        "language": "ja",
        "difficulty": "easy"
    },
    {
        "id": "005",
        "question": "ç¨å‹™ç½²é•·ã¯ã©ã®ã‚ˆã†ãªæ±ºå®šã‚’ä¸‹ã™ã“ã¨ãŒã§ãã¾ã™ã‹ï¼Ÿ",
        "ground_truth": "ç¨å‹™ç½²é•·ã¯ã€ç´ç¨ç¾©å‹™è€…ãŒç”³å‘Šæ›¸ã‚’æå‡ºã—ãªã„å ´åˆã‚„è¨˜è¼‰å†…å®¹ã«èª¤ã‚ŠãŒã‚ã‚‹å ´åˆã«ã€æ›´æ­£ãƒ»æ±ºå®šã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚ã¾ãŸã€é’è‰²ç”³å‘Šã®æ‰¿èªå–æ¶ˆã—ãªã©ã®æ±ºå®šã‚‚è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚",
        "expected_keywords": ["ç¨å‹™ç½²é•·", "æ±ºå®š", "æ›´æ­£", "ç”³å‘Š"],
        "language": "ja",
        "difficulty": "medium"
    }
]


class APIEvaluator:
    """é€šè¿‡ API è°ƒç”¨çš„ RAG è¯„ä¼°å™¨"""
    
    def __init__(self, api_url: str, kb_token: str):
        self.api_url = api_url
        self.kb_token = kb_token
        self.judge_llm = ChatOpenAI(model="gpt-4o", temperature=0)
        self.results = []
        
    async def call_rag_api(self, question: str, language: str = "ja") -> Dict:
        """è°ƒç”¨ RAG API è·å–å›ç­”"""
        payload = {
            "query": question,
            "kb_id": self.kb_token,
            "language": language,
            "conversation_history": []
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.api_url,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=60)
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        return {
                            "success": True,
                            "answer": data.get("answer", ""),
                            "context": data.get("context", ""),
                            "citations": data.get("citations", []),
                            "follow_up": data.get("follow_up", [])
                        }
                    else:
                        error_text = await response.text()
                        return {
                            "success": False,
                            "error": f"API é”™è¯¯ {response.status}: {error_text}"
                        }
        except asyncio.TimeoutError:
            return {"success": False, "error": "API è¯·æ±‚è¶…æ—¶"}
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def evaluate_with_llm_judge(
        self, 
        question: str, 
        answer: str, 
        context: str, 
        ground_truth: str
    ) -> Dict:
        """ä½¿ç”¨ LLM ä½œä¸ºè¯„åˆ¤è€…è¿›è¡Œè¯„ä¼°"""
        
        judge_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ RAG ç³»ç»Ÿè¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä»·ä»¥ä¸‹é—®ç­”ç³»ç»Ÿçš„å›ç­”è´¨é‡ã€‚

## ç”¨æˆ·é—®é¢˜
{question}

## ç³»ç»Ÿå›ç­”
{answer}

## æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼ˆéƒ¨åˆ†ï¼‰
{context[:1500] if context else "ï¼ˆæ— ä¸Šä¸‹æ–‡ï¼‰"}

## æ ‡å‡†ç­”æ¡ˆï¼ˆå‚è€ƒï¼‰
{ground_truth}

## è¯„åˆ†è¦æ±‚
è¯·ä»ä»¥ä¸‹ 5 ä¸ªç»´åº¦è¿›è¡Œè¯„åˆ†ï¼ˆæ¯é¡¹ 1-5 åˆ†ï¼‰ï¼š

1. **å‡†ç¡®æ€§ (Accuracy)**: å›ç­”çš„äº‹å®æ˜¯å¦æ­£ç¡®
   - 5åˆ†ï¼šå®Œå…¨æ­£ç¡®
   - 4åˆ†ï¼šåŸºæœ¬æ­£ç¡®ï¼Œæœ‰å°è¯¯å·®
   - 3åˆ†ï¼šéƒ¨åˆ†æ­£ç¡®
   - 2åˆ†ï¼šå¤šå¤„é”™è¯¯
   - 1åˆ†ï¼šå®Œå…¨é”™è¯¯

2. **å®Œæ•´æ€§ (Completeness)**: æ˜¯å¦å›ç­”äº†é—®é¢˜çš„æ‰€æœ‰æ–¹é¢
   - 5åˆ†ï¼šéå¸¸å®Œæ•´
   - 4åˆ†ï¼šè¾ƒå®Œæ•´
   - 3åˆ†ï¼šåŸºæœ¬å®Œæ•´
   - 2åˆ†ï¼šä¸å¤Ÿå®Œæ•´
   - 1åˆ†ï¼šéå¸¸ä¸å®Œæ•´

3. **ç›¸å…³æ€§ (Relevance)**: å›ç­”æ˜¯å¦ç´§æ‰£é—®é¢˜
   - 5åˆ†ï¼šé«˜åº¦ç›¸å…³
   - 4åˆ†ï¼šè¾ƒç›¸å…³
   - 3åˆ†ï¼šéƒ¨åˆ†ç›¸å…³
   - 2åˆ†ï¼šç›¸å…³æ€§ä½
   - 1åˆ†ï¼šå®Œå…¨ä¸ç›¸å…³

4. **å¿ å®åº¦ (Faithfulness)**: å›ç­”æ˜¯å¦åŸºäºæ£€ç´¢å†…å®¹ï¼Œæœ‰æ— å¹»è§‰
   - 5åˆ†ï¼šå®Œå…¨åŸºäºä¸Šä¸‹æ–‡ï¼Œæ— å¹»è§‰
   - 4åˆ†ï¼šåŸºæœ¬åŸºäºä¸Šä¸‹æ–‡
   - 3åˆ†ï¼šéƒ¨åˆ†ä¿¡æ¯æ¥è‡ªä¸Šä¸‹æ–‡
   - 2åˆ†ï¼šè¾ƒå¤šå¹»è§‰
   - 1åˆ†ï¼šå®Œå…¨æ˜¯å¹»è§‰

5. **å¯è¯»æ€§ (Readability)**: å›ç­”æ˜¯å¦æ¸…æ™°æ˜“æ‡‚
   - 5åˆ†ï¼šéå¸¸æ¸…æ™°
   - 4åˆ†ï¼šè¾ƒæ¸…æ™°
   - 3åˆ†ï¼šåŸºæœ¬æ¸…æ™°
   - 2åˆ†ï¼šä¸å¤Ÿæ¸…æ™°
   - 1åˆ†ï¼šéš¾ä»¥ç†è§£

è¯·ä¸¥æ ¼ä»¥ JSON æ ¼å¼è¿”å›è¯„åˆ†ç»“æœï¼š
{{"accuracy": åˆ†æ•°, "completeness": åˆ†æ•°, "relevance": åˆ†æ•°, "faithfulness": åˆ†æ•°, "readability": åˆ†æ•°, "overall_comment": "ä¸€å¥è¯æ€»ç»“"}}"""

        try:
            response = self.judge_llm.invoke(judge_prompt)
            result_text = response.content
            
            # æå– JSON
            json_match = re.search(r'\{[^{}]+\}', result_text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            else:
                return {"error": "æ— æ³•è§£æè¯„åˆ†ç»“æœ"}
        except Exception as e:
            return {"error": str(e)}
    
    def check_keyword_hits(self, answer: str, context: str, expected_keywords: List[str]) -> Dict:
        """æ£€æŸ¥å…³é”®è¯å‘½ä¸­ç‡"""
        combined = answer + " " + context
        hits = sum(1 for kw in expected_keywords if kw in combined)
        
        return {
            "total_keywords": len(expected_keywords),
            "hits": hits,
            "hit_rate": hits / len(expected_keywords) if expected_keywords else 0,
            "missed_keywords": [kw for kw in expected_keywords if kw not in combined]
        }
    
    async def run_evaluation(self, test_cases: List[Dict]) -> Dict:
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        print("\n" + "=" * 60)
        print("ğŸ”„ å¼€å§‹ RAG è¯„ä¼°")
        print(f"   API: {self.api_url}")
        print(f"   çŸ¥è¯†åº“: {KNOWLEDGE_BASE_CONFIG['name']}")
        print(f"   æµ‹è¯•ç”¨ä¾‹æ•°: {len(test_cases)}")
        print("=" * 60)
        
        all_scores = {
            "accuracy": [],
            "completeness": [],
            "relevance": [],
            "faithfulness": [],
            "readability": []
        }
        
        keyword_hits = []
        api_success_count = 0
        
        for i, case in enumerate(test_cases):
            print(f"\n[{i+1}/{len(test_cases)}] è¯„ä¼°é—®é¢˜: {case['question'][:40]}...")
            
            # 1. è°ƒç”¨ API
            api_result = await self.call_rag_api(case["question"], case.get("language", "ja"))
            
            if not api_result.get("success"):
                print(f"  âŒ API è°ƒç”¨å¤±è´¥: {api_result.get('error')}")
                continue
            
            api_success_count += 1
            answer = api_result["answer"]
            context = api_result["context"]
            citations = api_result.get("citations", [])
            
            print(f"  âœ“ è·å–å›ç­”: {answer[:50]}...")
            print(f"  âœ“ å¼•ç”¨æ¥æº: {len(citations)} ä¸ª")
            
            # 2. å…³é”®è¯æ£€æŸ¥
            kw_result = self.check_keyword_hits(
                answer, context, case.get("expected_keywords", [])
            )
            keyword_hits.append(kw_result["hit_rate"])
            print(f"  âœ“ å…³é”®è¯å‘½ä¸­: {kw_result['hits']}/{kw_result['total_keywords']}")
            
            # 3. LLM-as-Judge è¯„åˆ†
            scores = self.evaluate_with_llm_judge(
                question=case["question"],
                answer=answer,
                context=context,
                ground_truth=case["ground_truth"]
            )
            
            if "error" not in scores:
                for key in all_scores.keys():
                    if key in scores:
                        all_scores[key].append(scores[key])
                print(f"  âœ“ è¯„åˆ†: å‡†ç¡®æ€§={scores.get('accuracy')}, å¿ å®åº¦={scores.get('faithfulness')}")
            else:
                print(f"  âš ï¸ è¯„åˆ†å¤±è´¥: {scores.get('error')}")
            
            # ä¿å­˜è¯¦ç»†ç»“æœ
            self.results.append({
                "case_id": case["id"],
                "question": case["question"],
                "answer": answer,
                "context_length": len(context),
                "citations_count": len(citations),
                "keyword_hit_rate": kw_result["hit_rate"],
                "scores": scores
            })
        
        # è®¡ç®—å¹³å‡åˆ†
        avg_scores = {}
        for key, values in all_scores.items():
            if values:
                avg_scores[key] = sum(values) / len(values)
        
        avg_keyword_hit = sum(keyword_hits) / len(keyword_hits) if keyword_hits else 0
        
        return {
            "avg_scores": avg_scores,
            "avg_keyword_hit_rate": avg_keyword_hit,
            "api_success_rate": api_success_count / len(test_cases),
            "total_cases": len(test_cases),
            "successful_cases": api_success_count
        }
    
    def generate_report(self, eval_result: Dict) -> str:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        report = []
        report.append("=" * 60)
        report.append("ğŸ“‹ RAG å›å¤ç²¾åº¦è¯„ä¼°æŠ¥å‘Š")
        report.append("=" * 60)
        report.append(f"è¯„ä¼°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"çŸ¥è¯†åº“: {KNOWLEDGE_BASE_CONFIG['name']}")
        report.append(f"API: {self.api_url}")
        report.append("")
        
        # API è°ƒç”¨ç»Ÿè®¡
        report.append("ğŸ“¡ API è°ƒç”¨ç»Ÿè®¡")
        report.append("-" * 40)
        report.append(f"  æ€»æµ‹è¯•ç”¨ä¾‹: {eval_result['total_cases']}")
        report.append(f"  æˆåŠŸè°ƒç”¨: {eval_result['successful_cases']}")
        report.append(f"  æˆåŠŸç‡: {eval_result['api_success_rate']:.2%}")
        report.append("")
        
        # LLM-as-Judge è¯„åˆ†
        report.append("ğŸ¤– LLM-as-Judge è¯„åˆ† (æ»¡åˆ†5åˆ†)")
        report.append("-" * 40)
        avg_scores = eval_result.get("avg_scores", {})
        for metric, score in avg_scores.items():
            metric_zh = {
                "accuracy": "å‡†ç¡®æ€§",
                "completeness": "å®Œæ•´æ€§",
                "relevance": "ç›¸å…³æ€§",
                "faithfulness": "å¿ å®åº¦",
                "readability": "å¯è¯»æ€§"
            }.get(metric, metric)
            grade = self._score_to_grade_5(score)
            report.append(f"  {metric_zh}: {score:.2f} {grade}")
        
        if avg_scores:
            overall = sum(avg_scores.values()) / len(avg_scores)
            report.append(f"\n  ç»¼åˆè¯„åˆ†: {overall:.2f}/5.0 {self._score_to_grade_5(overall)}")
        report.append("")
        
        # æ£€ç´¢è´¨é‡
        report.append("ğŸ” æ£€ç´¢è´¨é‡")
        report.append("-" * 40)
        report.append(f"  å…³é”®è¯å‘½ä¸­ç‡: {eval_result['avg_keyword_hit_rate']:.2%}")
        report.append("")
        
        # è¯¦ç»†ç»“æœ
        report.append("ğŸ“ å„æµ‹è¯•ç”¨ä¾‹è¯¦æƒ…")
        report.append("-" * 40)
        for result in self.results:
            scores = result.get("scores", {})
            if "error" not in scores:
                avg = sum(scores.get(k, 0) for k in ["accuracy", "completeness", "relevance", "faithfulness", "readability"]) / 5
                report.append(f"  [{result['case_id']}] ç»¼åˆ: {avg:.1f}/5 | å…³é”®è¯: {result['keyword_hit_rate']:.0%}")
            else:
                report.append(f"  [{result['case_id']}] è¯„åˆ†å¤±è´¥")
        report.append("")
        
        # ä¼˜åŒ–å»ºè®®
        report.append("ğŸ’¡ ä¼˜åŒ–å»ºè®®")
        report.append("-" * 40)
        if avg_scores:
            overall = sum(avg_scores.values()) / len(avg_scores)
            if overall < 3:
                report.append("  âš ï¸ ç³»ç»Ÿè¡¨ç°éœ€è¦æ”¹è¿›:")
                report.append("  - æ£€æŸ¥çŸ¥è¯†åº“æ–‡æ¡£çš„å†…å®¹å®Œæ•´æ€§")
                report.append("  - ä¼˜åŒ–æ–‡æ¡£åˆ†å‰²ç­–ç•¥")
                report.append("  - è°ƒæ•´æ£€ç´¢å‚æ•°")
            elif overall < 4:
                report.append("  ğŸ’¡ å¯è€ƒè™‘çš„ä¼˜åŒ–:")
                report.append("  - å¢åŠ æ›´å¤šç›¸å…³æ–‡æ¡£")
                report.append("  - ä¼˜åŒ– Prompt æ¨¡æ¿")
            else:
                report.append("  âœ… ç³»ç»Ÿè¡¨ç°è‰¯å¥½!")
        
        report.append("")
        report.append("=" * 60)
        
        return "\n".join(report)
    
    @staticmethod
    def _score_to_grade_5(score: float) -> str:
        """å°†5åˆ†åˆ¶åˆ†æ•°è½¬æ¢ä¸ºç­‰çº§"""
        if score >= 4.5:
            return "âœ… ä¼˜ç§€"
        elif score >= 4.0:
            return "ğŸŸ¢ è‰¯å¥½"
        elif score >= 3.0:
            return "ğŸŸ¡ åŠæ ¼"
        elif score >= 2.0:
            return "ğŸŸ  éœ€æ”¹è¿›"
        else:
            return "ğŸ”´ è¾ƒå·®"


async def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ¯ æ³•äººç¨æ³•çŸ¥è¯†åº“ RAG ç²¾åº¦è¯„ä¼° (API æ¨¡å¼)")
    print("=" * 60)
    
    # æ£€æŸ¥ API æ˜¯å¦å¯ç”¨
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--prod", action="store_true", help="ä½¿ç”¨ç”Ÿäº§ç¯å¢ƒ API")
    parser.add_argument("--url", type=str, help="è‡ªå®šä¹‰ API URL")
    args = parser.parse_args()
    
    if args.url:
        api_url = args.url
    elif args.prod:
        api_url = PROD_API_URL
    else:
        api_url = LOCAL_API_URL
    
    print(f"ä½¿ç”¨ API: {api_url}")
    print(f"çŸ¥è¯†åº“: {KNOWLEDGE_BASE_CONFIG['name']}")
    print(f"æµ‹è¯•ç”¨ä¾‹æ•°: {len(TEST_CASES)}")
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    # ä½¿ç”¨ share_tokenï¼Œåç«¯ä¼šé€šè¿‡å®ƒæŸ¥è¯¢å¯¹åº”çš„ vector_collection
    kb_token = KNOWLEDGE_BASE_CONFIG["share_token"]
    print(f"ä½¿ç”¨ share_token: {kb_token}")
    print(f"å¯¹åº” vector_collection: {KNOWLEDGE_BASE_CONFIG.get('vector_collection', 'N/A')}")
    
    evaluator = APIEvaluator(
        api_url=api_url,
        kb_token=kb_token
    )
    
    # è¿è¡Œè¯„ä¼°
    eval_result = await evaluator.run_evaluation(TEST_CASES)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = evaluator.generate_report(eval_result)
    print(report)
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = f"ragas_api_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    with open(report_file, "w", encoding="utf-8") as f:
        f.write(report)
    print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    # ä¿å­˜è¯¦ç»†ç»“æœä¸º JSON
    json_file = f"ragas_api_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(json_file, "w", encoding="utf-8") as f:
        json.dump({
            "config": KNOWLEDGE_BASE_CONFIG,
            "summary": eval_result,
            "details": evaluator.results
        }, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“„ è¯¦ç»†ç»“æœ: {json_file}")


if __name__ == "__main__":
    asyncio.run(main())
