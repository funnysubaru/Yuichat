#!/usr/bin/env python3
"""
1.3.16: RAG å›å¤ç²¾åº¦è¯„ä¼°è„šæœ¬ - ä½¿ç”¨ RAGAS æ¡†æ¶

è¯„ä¼°ç»´åº¦:
- faithfulness (å¿ å®åº¦): å›ç­”æ˜¯å¦åŸºäºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
- answer_relevancy (å›ç­”ç›¸å…³æ€§): å›ç­”æ˜¯å¦ä¸é—®é¢˜ç›¸å…³
- context_precision (ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦): æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ˜¯å¦ç›¸å…³
- context_recall (ä¸Šä¸‹æ–‡å¬å›ç‡): æ˜¯å¦æ£€ç´¢åˆ°æ‰€æœ‰å¿…è¦ä¿¡æ¯
"""

import os
import sys
import json
import asyncio
from datetime import datetime
from typing import List, Dict, Any
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv('.env.local')
load_dotenv()

# è®¾ç½®ç¯å¢ƒ
os.environ["ENV"] = "development"

# å¯¼å…¥ RAG ç›¸å…³æ¨¡å—
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# ä½¿ç”¨ Supabase pgvector
import vecs
DATABASE_URL = os.getenv("PGVECTOR_DATABASE_URL") or os.getenv("DATABASE_URL")

# RAGAS è¯„ä¼°æ¡†æ¶
try:
    from ragas import evaluate
    from ragas.metrics._faithfulness import Faithfulness
    from ragas.metrics._answer_relevance import AnswerRelevancy
    from ragas.metrics._context_precision import ContextPrecision
    from ragas.metrics._context_recall import ContextRecall
    from datasets import Dataset
    RAGAS_AVAILABLE = True
    
    # åˆå§‹åŒ–æŒ‡æ ‡
    faithfulness = Faithfulness()
    answer_relevancy = AnswerRelevancy()
    context_precision = ContextPrecision()
    context_recall = ContextRecall()
except ImportError as e:
    print(f"âš ï¸ RAGAS å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·è¿è¡Œ: pip install ragas datasets")
    RAGAS_AVAILABLE = False

# ==================== é…ç½® ====================

# æ³•äººç¨æ³•çŸ¥è¯†åº“é…ç½®
# æ³¨ï¼šæœ¬åœ° Chroma ä½¿ç”¨ share_token ä½œä¸ºé›†åˆåï¼Œpgvector ä½¿ç”¨ vector_collection
KNOWLEDGE_BASE_CONFIG = {
    "name": "æ³•äººç¨æ³•",
    "collection_name": "deb48327-150b-408d-8b0c-56ce7333a987",  # share_token (æœ¬åœ° Chroma)
    "vector_collection": "kb_1769060431971_13o0t",  # pgvector é›†åˆå
    "kb_id": "f870ee08-86b7-4911-932e-7b28f2727276",
}

# æ£€ç´¢é…ç½®
RETRIEVE_K = 4  # æ£€ç´¢æ–‡æ¡£æ•°é‡

# ==================== æµ‹è¯•æ•°æ®é›† ====================

# æ³•äººç¨æ³•è¯„ä¼°æµ‹è¯•é›† (Golden Dataset)
# æ ¼å¼: question, ground_truth (æ ‡å‡†ç­”æ¡ˆ), é¢„æœŸæ£€ç´¢å…³é”®è¯
TEST_CASES = [
    {
        "id": "001",
        "question": "ç‰¹å®šæ”¯é…é–¢ä¿‚ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "ground_truth": "ç‰¹å®šæ”¯é…é–¢ä¿‚ã¨ã¯ã€ä¸€æ–¹ã®æ³•äººãŒä»–æ–¹ã®æ³•äººã®ç™ºè¡Œæ¸ˆæ ªå¼ç­‰ã®50%ã‚’è¶…ãˆã‚‹æ•°ã®æ ªå¼ç­‰ã‚’ç›´æ¥åˆã¯é–“æ¥ã«ä¿æœ‰ã™ã‚‹é–¢ä¿‚ãªã©ã‚’ã„ã„ã¾ã™ã€‚",
        "expected_keywords": ["ç‰¹å®šæ”¯é…é–¢ä¿‚", "50%", "æ ªå¼", "æ³•äºº"],
        "language": "ja",
        "difficulty": "medium"
    },
    {
        "id": "002", 
        "question": "é€£çµæ¬ æé‡‘ã®é©ç”¨æ¡ä»¶ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "ground_truth": "é€£çµæ¬ æé‡‘ã‚’ç¹°è¶Šæ§é™¤ã™ã‚‹ãŸã‚ã«ã¯ã€é€£çµæ³•äººãŒç¶™ç¶šã—ã¦é€£çµç¢ºå®šç”³å‘Šæ›¸ã‚’æå‡ºã—ã€æ¬ æé‡‘ã®ç¹°è¶Šæ§é™¤ã‚’é©ç”¨ã™ã‚‹äº‹æ¥­å¹´åº¦ã®ç¢ºå®šç”³å‘Šæ›¸ç­‰ã«ã‚‚æ¬ æé‡‘ã®æ˜ç´°ã‚’è¨˜è¼‰ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚",
        "expected_keywords": ["é€£çµæ¬ æé‡‘", "ç¹°è¶Šæ§é™¤", "ç¢ºå®šç”³å‘Š"],
        "language": "ja",
        "difficulty": "hard"
    },
    {
        "id": "003",
        "question": "åˆä½µå¾Œã®æœªå‡¦ç†æ¬ æé‡‘ã¯ã©ã®ã‚ˆã†ã«è¨ˆç®—ã•ã‚Œã¾ã™ã‹ï¼Ÿ",
        "ground_truth": "åˆä½µå¾Œã®æœªå‡¦ç†æ¬ æé‡‘ã¯ã€è¢«åˆä½µæ³•äººã®é©æ ¼åˆä½µç­‰å‰ã®æ¬ æé‡‘é¡ã¨åˆä½µæ³•äººã®æ¬ æé‡‘é¡ã‚’ä¸€å®šã®è¨ˆç®—å¼ã«åŸºã¥ã„ã¦ç®—å®šã—ã¾ã™ã€‚",
        "expected_keywords": ["åˆä½µ", "æ¬ æé‡‘", "è¢«åˆä½µæ³•äºº", "è¨ˆç®—"],
        "language": "ja",
        "difficulty": "hard"
    },
    {
        "id": "004",
        "question": "æ³•äººç¨ã®ç”³å‘ŠæœŸé™ã¯ã„ã¤ã§ã™ã‹ï¼Ÿ",
        "ground_truth": "æ³•äººç¨ã®ç”³å‘Šã¯ã€å„äº‹æ¥­å¹´åº¦çµ‚äº†ã®æ—¥ã®ç¿Œæ—¥ã‹ã‚‰2æœˆä»¥å†…ã«ç¢ºå®šç”³å‘Šæ›¸ã‚’æå‡ºã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚",
        "expected_keywords": ["ç”³å‘Š", "æœŸé™", "2æœˆ", "äº‹æ¥­å¹´åº¦"],
        "language": "ja",
        "difficulty": "easy"
    },
    {
        "id": "005",
        "question": "ç¨å‹™ç½²é•·ã¯ã©ã®ã‚ˆã†ãªæ±ºå®šã‚’ä¸‹ã™ã“ã¨ãŒã§ãã¾ã™ã‹ï¼Ÿ",
        "ground_truth": "ç¨å‹™ç½²é•·ã¯ã€ç´ç¨ç¾©å‹™è€…ãŒç”³å‘Šæ›¸ã‚’æå‡ºã—ãªã„å ´åˆã‚„è¨˜è¼‰å†…å®¹ã«èª¤ã‚ŠãŒã‚ã‚‹å ´åˆã«ã€æ›´æ­£ãƒ»æ±ºå®šã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚ã¾ãŸã€é’è‰²ç”³å‘Šã®æ‰¿èªå–æ¶ˆã—ãªã©ã®æ±ºå®šã‚‚è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚",
        "expected_keywords": ["ç¨å‹™ç½²é•·", "æ±ºå®š", "æ›´æ­£", "ç”³å‘Š"],
        "language": "ja",
        "difficulty": "medium"
    }
]


class RAGEvaluator:
    """RAG è¯„ä¼°å™¨ - æ”¯æŒæœ¬åœ° Chroma"""
    
    def __init__(self, collection_name: str):
        self.collection_name = collection_name
        self.embeddings = OpenAIEmbeddings()
        self.llm = ChatOpenAI(model="gpt-4o", temperature=0)
        self.vectorstore = None
        self.retriever = None
        
    def initialize(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“è¿æ¥ (æœ¬åœ° Chroma)"""
        try:
            import chromadb
            from chromadb.config import Settings
            
            # ç›´æ¥ä½¿ç”¨ chromadb åŸç”Ÿ API
            chroma_path = f"./chroma_db/{self.collection_name}"
            
            # ä½¿ç”¨æŒä¹…åŒ– Chroma å®¢æˆ·ç«¯
            self.client = chromadb.PersistentClient(path=chroma_path)
            
            # è·å–æˆ–åˆ›å»ºé›†åˆ
            collections = self.client.list_collections()
            if collections:
                self.collection = collections[0]
                print(f"âœ… è¿æ¥æœ¬åœ° Chroma: {self.collection_name}")
                print(f"   æ–‡æ¡£æ•°é‡: {self.collection.count()}")
                return True
            else:
                print(f"âŒ é›†åˆä¸ºç©º")
                return False
                
        except ImportError:
            print("âŒ chromadb æœªå®‰è£…")
            # ä½¿ç”¨ LLM ç›´æ¥è¯„ä¼°ï¼ˆæ— éœ€å‘é‡æ£€ç´¢ï¼‰
            print("ğŸ”„ å›é€€åˆ° LLM-as-Judge è¯„ä¼°æ¨¡å¼")
            return True
        except Exception as e:
            print(f"âŒ è¿æ¥å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            # å›é€€æ¨¡å¼
            print("ğŸ”„ å›é€€åˆ° LLM-as-Judge è¯„ä¼°æ¨¡å¼")
            return True
    
    def retrieve_context(self, question: str) -> List[str]:
        """æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡"""
        if not hasattr(self, 'collection') or not self.collection:
            # æ— å‘é‡åº“æ—¶è¿”å›ç©ºï¼ˆå°†ä½¿ç”¨çº¯ LLM è¯„ä¼°ï¼‰
            return ["[æ— æ³•æ£€ç´¢ä¸Šä¸‹æ–‡ï¼Œå°†ä½¿ç”¨ LLM-as-Judge è¿›è¡Œè¯„ä¼°]"]
        
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_vector = self.embeddings.embed_query(question)
            
            # ä½¿ç”¨ Chroma è¿›è¡Œç›¸ä¼¼åº¦æœç´¢
            results = self.collection.query(
                query_embeddings=[query_vector],
                n_results=RETRIEVE_K,
                include=["documents", "metadatas"]
            )
            
            # æå–ä¸Šä¸‹æ–‡å†…å®¹
            contexts = []
            if results and "documents" in results and results["documents"]:
                for doc in results["documents"][0]:
                    if doc:
                        contexts.append(doc)
            
            return contexts if contexts else ["[æœªæ£€ç´¢åˆ°ç›¸å…³å†…å®¹]"]
        except Exception as e:
            print(f"æ£€ç´¢é”™è¯¯: {e}")
            return ["[æ£€ç´¢å‡ºé”™]"]
    
    def generate_answer(self, question: str, contexts: List[str]) -> str:
        """åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”"""
        context_text = "\n\n".join(contexts)
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ³•äººç¨æ³•çŸ¥è¯†åº“åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®åœ°è¯´ä½ ä¸çŸ¥é“ã€‚
è¯·ä½¿ç”¨æ—¥è¯­å›ç­”ã€‚

ä¸Šä¸‹æ–‡:
{context}"""),
            ("human", "{question}")
        ])
        
        chain = prompt | self.llm
        response = chain.invoke({"context": context_text, "question": question})
        return response.content
    
    def run_rag_pipeline(self, test_cases: List[Dict]) -> Dict[str, List]:
        """è¿è¡Œ RAG ç®¡é“ï¼Œç”Ÿæˆè¯„ä¼°æ•°æ®"""
        results = {
            "question": [],
            "answer": [],
            "contexts": [],
            "ground_truth": []
        }
        
        print("\n" + "=" * 60)
        print("ğŸ”„ è¿è¡Œ RAG ç®¡é“")
        print("=" * 60)
        
        for i, case in enumerate(test_cases):
            print(f"\n[{i+1}/{len(test_cases)}] å¤„ç†é—®é¢˜: {case['question'][:50]}...")
            
            # 1. æ£€ç´¢ä¸Šä¸‹æ–‡
            contexts = self.retrieve_context(case["question"])
            print(f"  âœ“ æ£€ç´¢åˆ° {len(contexts)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
            
            # 2. ç”Ÿæˆå›ç­”
            answer = self.generate_answer(case["question"], contexts)
            print(f"  âœ“ ç”Ÿæˆå›ç­”: {answer[:50]}...")
            
            # 3. æ”¶é›†ç»“æœ
            results["question"].append(case["question"])
            results["answer"].append(answer)
            results["contexts"].append(contexts)
            results["ground_truth"].append(case["ground_truth"])
        
        return results
    
    def evaluate_with_ragas(self, eval_data: Dict[str, List]) -> Dict:
        """ä½¿ç”¨ RAGAS è¿›è¡Œè¯„ä¼°"""
        if not RAGAS_AVAILABLE:
            print("âŒ RAGAS æœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œè¯„ä¼°")
            return {}
        
        print("\n" + "=" * 60)
        print("ğŸ“Š RAGAS è¯„ä¼°")
        print("=" * 60)
        
        try:
            # åˆ›å»º Dataset
            dataset = Dataset.from_dict(eval_data)
            
            # è¿è¡Œè¯„ä¼°
            result = evaluate(
                dataset,
                metrics=[
                    faithfulness,
                    answer_relevancy,
                    context_precision,
                    context_recall,
                ]
            )
            
            # å°† EvaluationResult è½¬æ¢ä¸ºå­—å…¸
            if hasattr(result, 'to_pandas'):
                df = result.to_pandas()
                scores = {}
                for col in ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']:
                    if col in df.columns:
                        scores[col] = df[col].mean()
                return scores
            elif hasattr(result, '__dict__'):
                return {k: v for k, v in result.__dict__.items() if isinstance(v, (int, float))}
            else:
                return dict(result) if hasattr(result, '__iter__') else {}
        except Exception as e:
            print(f"RAGAS è¯„ä¼°å‡ºé”™: {e}")
            return {}
    
    def evaluate_retrieval_quality(self, test_cases: List[Dict]) -> Dict:
        """è¯„ä¼°æ£€ç´¢è´¨é‡"""
        print("\n" + "=" * 60)
        print("ğŸ” æ£€ç´¢è´¨é‡è¯„ä¼°")
        print("=" * 60)
        
        total_cases = len(test_cases)
        keyword_hits = 0
        retrieved_lengths = []
        
        for case in test_cases:
            contexts = self.retrieve_context(case["question"])
            retrieved_lengths.append(len(contexts))
            
            # æ£€æŸ¥å…³é”®è¯å‘½ä¸­
            combined_context = " ".join(contexts)
            expected = case.get("expected_keywords", [])
            hits = sum(1 for kw in expected if kw in combined_context)
            
            if expected:
                keyword_hit_rate = hits / len(expected)
                if keyword_hit_rate > 0.5:
                    keyword_hits += 1
        
        return {
            "total_cases": total_cases,
            "keyword_hit_rate": keyword_hits / total_cases if total_cases > 0 else 0,
            "avg_retrieved_docs": sum(retrieved_lengths) / len(retrieved_lengths) if retrieved_lengths else 0
        }
    
    def generate_report(self, ragas_result: Dict, retrieval_result: Dict) -> str:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        report = []
        report.append("=" * 60)
        report.append("ğŸ“‹ RAG å›å¤ç²¾åº¦è¯„ä¼°æŠ¥å‘Š")
        report.append("=" * 60)
        report.append(f"è¯„ä¼°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"çŸ¥è¯†åº“: {KNOWLEDGE_BASE_CONFIG['name']}")
        report.append(f"å‘é‡é›†åˆ: {KNOWLEDGE_BASE_CONFIG['collection_name']}")
        report.append("")
        
        # RAGAS è¯„ä¼°ç»“æœ
        if ragas_result:
            report.append("ğŸ“Š RAGAS è¯„ä¼°æŒ‡æ ‡")
            report.append("-" * 40)
            for metric, score in ragas_result.items():
                if isinstance(score, (int, float)):
                    grade = self._score_to_grade(score)
                    report.append(f"  {metric}: {score:.4f} {grade}")
            report.append("")
        
        # æ£€ç´¢è´¨é‡
        report.append("ğŸ” æ£€ç´¢è´¨é‡æŒ‡æ ‡")
        report.append("-" * 40)
        report.append(f"  æµ‹è¯•ç”¨ä¾‹æ•°: {retrieval_result['total_cases']}")
        report.append(f"  å…³é”®è¯å‘½ä¸­ç‡: {retrieval_result['keyword_hit_rate']:.2%}")
        report.append(f"  å¹³å‡æ£€ç´¢æ–‡æ¡£æ•°: {retrieval_result['avg_retrieved_docs']:.1f}")
        report.append("")
        
        # æ€»ç»“
        report.append("ğŸ“ è¯„ä¼°æ€»ç»“")
        report.append("-" * 40)
        if ragas_result:
            avg_score = sum(v for v in ragas_result.values() if isinstance(v, (int, float))) / 4
            overall_grade = self._score_to_grade(avg_score)
            report.append(f"  ç»¼åˆè¯„åˆ†: {avg_score:.4f} {overall_grade}")
            
            # å»ºè®®
            if avg_score < 0.7:
                report.append("\n  âš ï¸ å»ºè®®æ”¹è¿›:")
                report.append("  - ä¼˜åŒ–æ–‡æ¡£åˆ†å‰²ç­–ç•¥ï¼Œç¡®ä¿è¯­ä¹‰å®Œæ•´")
                report.append("  - å¢åŠ ç›¸å…³è®­ç»ƒæ•°æ®")
                report.append("  - è°ƒæ•´æ£€ç´¢å‚æ•° (RETRIEVE_K)")
            elif avg_score < 0.85:
                report.append("\n  ğŸ’¡ å¯é€‰ä¼˜åŒ–:")
                report.append("  - è€ƒè™‘ä½¿ç”¨æ›´å¥½çš„ embedding æ¨¡å‹")
                report.append("  - æ·»åŠ æŸ¥è¯¢æ‰©å±•/é‡å†™")
            else:
                report.append("\n  âœ… ç³»ç»Ÿè¡¨ç°è‰¯å¥½!")
        
        report.append("")
        report.append("=" * 60)
        
        return "\n".join(report)
    
    @staticmethod
    def _score_to_grade(score: float) -> str:
        """å°†åˆ†æ•°è½¬æ¢ä¸ºç­‰çº§"""
        if score >= 0.9:
            return "âœ… ä¼˜ç§€"
        elif score >= 0.8:
            return "ğŸŸ¢ è‰¯å¥½"
        elif score >= 0.7:
            return "ğŸŸ¡ åŠæ ¼"
        elif score >= 0.5:
            return "ğŸŸ  éœ€æ”¹è¿›"
        else:
            return "ğŸ”´ è¾ƒå·®"


def run_llm_judge_evaluation(evaluator: RAGEvaluator, test_cases: List[Dict]) -> Dict:
    """ä½¿ç”¨ LLM ä½œä¸ºè¯„åˆ¤è€…è¿›è¡Œè¯„ä¼°"""
    print("\n" + "=" * 60)
    print("ğŸ¤– LLM-as-Judge è¯„ä¼°")
    print("=" * 60)
    
    judge_llm = ChatOpenAI(model="gpt-4o", temperature=0)
    
    scores = {
        "accuracy": [],
        "completeness": [],
        "relevance": [],
        "faithfulness": [],
        "readability": []
    }
    
    judge_prompt = """è¯·è¯„ä»·ä»¥ä¸‹ RAG é—®ç­”ç³»ç»Ÿçš„å›ç­”è´¨é‡ï¼ˆ1-5åˆ†ï¼‰:

ç”¨æˆ·é—®é¢˜: {question}
æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡: {context}
AIå›ç­”: {answer}
æ ‡å‡†ç­”æ¡ˆï¼ˆå‚è€ƒï¼‰: {ground_truth}

è¯·ä»ä»¥ä¸‹5ä¸ªç»´åº¦è¯„åˆ†ï¼š
1. å‡†ç¡®æ€§ (Accuracy): å›ç­”æ˜¯å¦æ­£ç¡® (1-5åˆ†)
2. å®Œæ•´æ€§ (Completeness): æ˜¯å¦å›ç­”äº†é—®é¢˜çš„æ‰€æœ‰æ–¹é¢ (1-5åˆ†)
3. ç›¸å…³æ€§ (Relevance): å›ç­”æ˜¯å¦ç´§æ‰£é—®é¢˜ (1-5åˆ†)
4. å¿ å®åº¦ (Faithfulness): æ˜¯å¦åŸºäºæ£€ç´¢å†…å®¹ï¼Œæ— å¹»è§‰ (1-5åˆ†)
5. å¯è¯»æ€§ (Readability): å›ç­”æ˜¯å¦æ¸…æ™°æ˜“æ‡‚ (1-5åˆ†)

è¯·ä»¥ JSON æ ¼å¼è¿”å›è¯„åˆ†ç»“æœï¼š
{{"accuracy": åˆ†æ•°, "completeness": åˆ†æ•°, "relevance": åˆ†æ•°, "faithfulness": åˆ†æ•°, "readability": åˆ†æ•°, "comments": "ç®€çŸ­è¯„è¯­"}}"""

    for i, case in enumerate(test_cases):
        print(f"\n[{i+1}/{len(test_cases)}] è¯„ä¼°é—®é¢˜: {case['question'][:40]}...")
        
        # è·å–ä¸Šä¸‹æ–‡å’Œå›ç­”
        contexts = evaluator.retrieve_context(case["question"])
        answer = evaluator.generate_answer(case["question"], contexts)
        context_text = "\n".join(contexts[:2])  # åªå–å‰2ä¸ªé¿å…å¤ªé•¿
        
        # LLM è¯„åˆ†
        try:
            response = judge_llm.invoke(
                judge_prompt.format(
                    question=case["question"],
                    context=context_text[:1000],
                    answer=answer,
                    ground_truth=case["ground_truth"]
                )
            )
            
            # è§£æ JSON ç»“æœ
            result_text = response.content
            # æå– JSON
            import re
            json_match = re.search(r'\{[^{}]+\}', result_text, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                for key in scores.keys():
                    if key in result:
                        scores[key].append(result[key])
                print(f"  è¯„åˆ†: å‡†ç¡®æ€§={result.get('accuracy', 'N/A')}, å¿ å®åº¦={result.get('faithfulness', 'N/A')}")
            else:
                print(f"  âš ï¸ æ— æ³•è§£æè¯„åˆ†ç»“æœ")
        except Exception as e:
            print(f"  âŒ è¯„ä¼°å‡ºé”™: {e}")
    
    # è®¡ç®—å¹³å‡åˆ†
    avg_scores = {}
    for key, values in scores.items():
        if values:
            avg_scores[key] = sum(values) / len(values)
    
    return avg_scores


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ¯ æ³•äººç¨æ³•çŸ¥è¯†åº“ RAG ç²¾åº¦è¯„ä¼°")
    print("=" * 60)
    print(f"çŸ¥è¯†åº“: {KNOWLEDGE_BASE_CONFIG['name']}")
    print(f"æµ‹è¯•ç”¨ä¾‹æ•°: {len(TEST_CASES)}")
    print(f"è¯„ä¼°æ¡†æ¶: RAGAS" if RAGAS_AVAILABLE else "è¯„ä¼°æ¡†æ¶: LLM-as-Judge (RAGASæœªå®‰è£…)")
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = RAGEvaluator(KNOWLEDGE_BASE_CONFIG["collection_name"])
    if not evaluator.initialize():
        print("âŒ åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥å‘é‡æ•°æ®åº“")
        sys.exit(1)
    
    # 1. æ£€ç´¢è´¨é‡è¯„ä¼°
    retrieval_result = evaluator.evaluate_retrieval_quality(TEST_CASES)
    
    # 2. RAGAS è¯„ä¼° æˆ– LLM-as-Judge
    ragas_result = {}
    if RAGAS_AVAILABLE:
        eval_data = evaluator.run_rag_pipeline(TEST_CASES)
        ragas_result = evaluator.evaluate_with_ragas(eval_data)
    else:
        # ä½¿ç”¨ LLM-as-Judge ä½œä¸ºå¤‡é€‰
        llm_scores = run_llm_judge_evaluation(evaluator, TEST_CASES)
        ragas_result = {
            "accuracy": llm_scores.get("accuracy", 0) / 5,
            "faithfulness": llm_scores.get("faithfulness", 0) / 5,
            "relevance": llm_scores.get("relevance", 0) / 5,
            "completeness": llm_scores.get("completeness", 0) / 5,
        }
    
    # 3. ç”ŸæˆæŠ¥å‘Š
    report = evaluator.generate_report(ragas_result, retrieval_result)
    print(report)
    
    # 4. ä¿å­˜æŠ¥å‘Š
    report_file = f"ragas_eval_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    with open(report_file, "w", encoding="utf-8") as f:
        f.write(report)
    print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")


if __name__ == "__main__":
    main()
