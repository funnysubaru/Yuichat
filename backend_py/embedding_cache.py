"""
1.3.36: Embeddingè¯·æ±‚çº§ç¼“å­˜æ¨¡å—
ä½¿ç”¨contextvarså®ç°è¯·æ±‚çº§åˆ«çš„Embeddingç¼“å­˜ï¼Œé¿å…åŒä¸€è¯·æ±‚ä¸­é‡å¤è°ƒç”¨OpenAI Embedding API

ä¼˜åŒ–æ•ˆæœ:
- å•æ¬¡è¯·æ±‚ä»4æ¬¡Embedding APIè°ƒç”¨å‡å°‘åˆ°1æ¬¡
- é¢„è®¡èŠ‚çœ ~1.5ç§’ å“åº”æ—¶é—´
- èŠ‚çœ 75% Embedding APIè´¹ç”¨
"""

import os
import time
import logging
from contextvars import ContextVar
from typing import List, Dict, Optional, Callable
from functools import wraps

logger = logging.getLogger(__name__)

# è¯·æ±‚çº§åˆ«çš„Embeddingç¼“å­˜
# æ¯ä¸ªè¯·æ±‚æœ‰ç‹¬ç«‹çš„ç¼“å­˜ç©ºé—´ï¼Œè¯·æ±‚ç»“æŸåè‡ªåŠ¨æ¸…ç†
_embedding_cache: ContextVar[Dict[str, List[float]]] = ContextVar(
    'embedding_cache', 
    default=None
)

# ç¼“å­˜ç»Ÿè®¡ï¼ˆç”¨äºè°ƒè¯•ï¼‰
_cache_stats: ContextVar[Dict[str, int]] = ContextVar(
    'cache_stats',
    default=None
)


def init_request_cache():
    """
    1.3.36: åˆå§‹åŒ–è¯·æ±‚çº§ç¼“å­˜
    åº”è¯¥åœ¨æ¯ä¸ªè¯·æ±‚å¼€å§‹æ—¶è°ƒç”¨
    """
    _embedding_cache.set({})
    _cache_stats.set({'hits': 0, 'misses': 0})


def clear_request_cache():
    """
    1.3.36: æ¸…ç†è¯·æ±‚çº§ç¼“å­˜
    åº”è¯¥åœ¨æ¯ä¸ªè¯·æ±‚ç»“æŸæ—¶è°ƒç”¨ï¼ˆå¯é€‰ï¼Œcontextvarsä¼šè‡ªåŠ¨éš”ç¦»ï¼‰
    """
    cache = _embedding_cache.get()
    if cache:
        cache.clear()
    _embedding_cache.set(None)
    _cache_stats.set(None)


def get_cache_stats() -> Dict[str, int]:
    """
    1.3.36: è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
    """
    stats = _cache_stats.get()
    return stats if stats else {'hits': 0, 'misses': 0}


def cached_embed_query(
    query: str,
    embed_func: Callable[[str], List[float]],
    cache_key: str = None
) -> List[float]:
    """
    1.3.36: å¸¦ç¼“å­˜çš„EmbeddingæŸ¥è¯¢
    
    Args:
        query: è¦åµŒå…¥çš„æ–‡æœ¬
        embed_func: å®é™…çš„embeddingå‡½æ•°ï¼ˆå¦‚ embeddings.embed_queryï¼‰
        cache_key: å¯é€‰çš„ç¼“å­˜é”®ï¼ˆé»˜è®¤ä½¿ç”¨queryæœ¬èº«ï¼‰
    
    Returns:
        Embeddingå‘é‡ (List[float])
    """
    key = cache_key or query
    
    # è·å–å½“å‰è¯·æ±‚çš„ç¼“å­˜
    cache = _embedding_cache.get()
    stats = _cache_stats.get()
    
    # å¦‚æœç¼“å­˜æœªåˆå§‹åŒ–ï¼Œç›´æ¥è°ƒç”¨åŸå‡½æ•°
    if cache is None:
        if os.getenv("ENV") == "development":
            logger.debug(f"âš ï¸ Embedding cache not initialized, calling embed_func directly")
        return embed_func(query)
    
    # æ£€æŸ¥ç¼“å­˜
    if key in cache:
        if stats:
            stats['hits'] += 1
        if os.getenv("ENV") == "development":
            logger.info(f"âœ… Embedding cache HIT for: {query[:50]}... (total hits: {stats.get('hits', 0)})")
        return cache[key]
    
    # ç¼“å­˜æœªå‘½ä¸­ï¼Œè°ƒç”¨åŸå‡½æ•°
    if stats:
        stats['misses'] += 1
    
    start_time = time.time()
    embedding = embed_func(query)
    elapsed = time.time() - start_time
    
    # å­˜å…¥ç¼“å­˜
    cache[key] = embedding
    
    if os.getenv("ENV") == "development":
        logger.info(f"ğŸ“¥ Embedding generated in {elapsed:.3f}s for: {query[:50]}... (cached for reuse)")
    
    return embedding


class EmbeddingCacheMiddleware:
    """
    1.3.36: FastAPIä¸­é—´ä»¶ï¼Œè‡ªåŠ¨ç®¡ç†è¯·æ±‚çº§Embeddingç¼“å­˜
    """
    
    def __init__(self, app):
        self.app = app
    
    async def __call__(self, scope, receive, send):
        if scope["type"] == "http":
            # åˆå§‹åŒ–è¯·æ±‚çº§ç¼“å­˜
            init_request_cache()
            try:
                await self.app(scope, receive, send)
            finally:
                # è¯·æ±‚ç»“æŸåæ¸…ç†ç¼“å­˜ï¼ˆå¯é€‰ï¼‰
                if os.getenv("ENV") == "development":
                    stats = get_cache_stats()
                    if stats['hits'] > 0 or stats['misses'] > 0:
                        logger.info(f"ğŸ“Š Request embedding stats: {stats['hits']} hits, {stats['misses']} misses")
                clear_request_cache()
        else:
            await self.app(scope, receive, send)


# 1.3.36: å…¨å±€OpenAI Embeddingså®ä¾‹ï¼ˆé¿å…é‡å¤åˆ›å»ºï¼‰
_global_embeddings = None


def get_cached_embeddings():
    """
    1.3.36: è·å–å…¨å±€çš„OpenAI Embeddingså®ä¾‹
    é¿å…æ¯æ¬¡è¯·æ±‚éƒ½åˆ›å»ºæ–°å®ä¾‹
    """
    global _global_embeddings
    if _global_embeddings is None:
        from langchain_openai import OpenAIEmbeddings
        _global_embeddings = OpenAIEmbeddings()
        if os.getenv("ENV") == "development":
            logger.info("ğŸ”§ Created global OpenAI Embeddings instance")
    return _global_embeddings


def embed_query_with_cache(query: str) -> List[float]:
    """
    1.3.36: ä¾¿æ·å‡½æ•°ï¼šä½¿ç”¨ç¼“å­˜çš„EmbeddingæŸ¥è¯¢
    è¿™æ˜¯æœ€å¸¸ç”¨çš„æ¥å£
    
    Args:
        query: è¦åµŒå…¥çš„æ–‡æœ¬
    
    Returns:
        Embeddingå‘é‡ (List[float])
    """
    embeddings = get_cached_embeddings()
    return cached_embed_query(query, embeddings.embed_query)
